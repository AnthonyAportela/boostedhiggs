{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "d6a04263-186d-4c70-a291-f196c9a8da67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "from typing import List, Optional\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "from coffea import processor\n",
    "from coffea.nanoevents.methods import candidate, vector\n",
    "from coffea.analysis_tools import Weights, PackedSelection\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Found duplicate branch \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d10df933-1d90-472f-8334-29f2b032b2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_val(\n",
    "    arr: ak.Array,\n",
    "    value: float,\n",
    "    target: int = None,\n",
    "    axis: int = 0,\n",
    "    to_numpy: bool = False,\n",
    "    clip: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    pads awkward array up to ``target`` index along axis ``axis`` with value ``value``,\n",
    "    optionally converts to numpy array\n",
    "    \"\"\"\n",
    "    if target:\n",
    "        ret = ak.fill_none(ak.pad_none(arr, target, axis=axis, clip=clip), value, axis=None)\n",
    "    else:\n",
    "        ret = ak.fill_none(arr, value, axis=None)\n",
    "    return ret.to_numpy() if to_numpy else ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6e338aa9-c4ff-4765-b66d-585ad1d90e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HwwProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, year=\"2017\", yearmod=\"\", channels=[\"ele\",\"mu\",\"had\"], output_location=\"./\"):\n",
    "        self._year = year\n",
    "        self._yearmod = yearmod\n",
    "        self._channels = channels\n",
    "        self._output_location = output_location\n",
    "        \n",
    "        # define variables to save for each channel\n",
    "        self._skimvars = {\n",
    "            'ele': [\n",
    "                \"lepton_pt\",\n",
    "                \"lep_isolation\"\n",
    "            ],\n",
    "            'mu': [\n",
    "                \"lepton_pt\",\n",
    "                \"lep_isolation\"\n",
    "            ],\n",
    "            'had': [\n",
    "                \"fatjet_pt\",\n",
    "            ],\n",
    "        }\n",
    "        \n",
    "        # trigger paths\n",
    "        self._HLTs = {\n",
    "            2016: {\n",
    "                'ele': [\n",
    "                    \"Ele27_WPTight_Gsf\",\n",
    "                    \"Ele115_CaloIdVT_GsfTrkIdT\",\n",
    "                    \"Photon175\",\n",
    "                    # \"Ele50_CaloIdVT_GsfTrkIdT_PFJet165\", # extra                                                                                                                                                                                                              \n",
    "                    # \"Ele15_IsoVVVL_PFHT600\", # VVL                                                                                                                                                                                                                            \n",
    "                ],\n",
    "                'mu': [\n",
    "                    \"Mu50\",\n",
    "                    \"TkMu50\",\n",
    "                    \"IsoMu24\",\n",
    "                    \"IsoTkMu24\",\n",
    "                    # \"Mu55\",                                                                                                                                                                                                                                                   \n",
    "                    # \"Mu15_IsoVVVL_PFHT600\" # VVL                                                                                                                                                                                                                              \n",
    "                ],\n",
    "                'had': [\n",
    "                    \"PFHT800\",\n",
    "                    \"PFHT900\",\n",
    "                    \"AK8PFJet360_TrimMass30\",\n",
    "                    \"AK8PFHT700_TrimR0p1PT0p03Mass50\",\n",
    "                    \"PFHT650_WideJetMJJ950DEtaJJ1p5\",\n",
    "                    \"PFHT650_WideJetMJJ900DEtaJJ1p5\",\n",
    "                    \"PFJet450\",\n",
    "                ],\n",
    "            },\n",
    "            2017: {\n",
    "                'ele': [\n",
    "                    \"Ele35_WPTight_Gsf\",\n",
    "                    \"Ele115_CaloIdVT_GsfTrkIdT\",\n",
    "                    \"Photon200\",\n",
    "                    # \"Ele50_CaloIdVT_GsfTrkIdT_PFJet165\", # extra                                                                                                                                                                                                              \n",
    "                    # \"Ele15_IsoVVVL_PFHT600\", # VVL                                                                                                                                                                                                                            \n",
    "                ],\n",
    "                'mu': [\n",
    "                    \"Mu50\",\n",
    "                    \"IsoMu27\",\n",
    "                    \"OldMu100\",\n",
    "                    \"TkMu100\",\n",
    "                    # \"Mu15_IsoVVVL_PFHT600\", # VVL                                                                                                                                                                                                                             \n",
    "                ],\n",
    "                'had': [\n",
    "                    \"PFHT1050\",\n",
    "                    \"AK8PFJet400_TrimMass30\",\n",
    "                    \"AK8PFJet420_TrimMass30\",\n",
    "                    \"AK8PFHT800_TrimMass50\",\n",
    "                    \"PFJet500\",\n",
    "                    \"AK8PFJet500\",\n",
    "                ],\n",
    "            },\n",
    "            2018: {\n",
    "                'ele': [\n",
    "                    \"Ele32_WPTight_Gsf\",\n",
    "                    \"Ele115_CaloIdVT_GsfTrkIdT\",\n",
    "                    \"Photon200\",\n",
    "                    # \"Ele50_CaloIdVT_GsfTrkIdT_PFJet165\", # extra                                                                                                                                                                                                              \n",
    "                    # \"Ele15_IsoVVVL_PFHT600\", # VVL                                                                                                                                                                                                                            \n",
    "                ],\n",
    "                'mu': [\n",
    "                    \"Mu50\",\n",
    "                    \"IsoMu24\",\n",
    "                    \"OldMu100\",\n",
    "                    \"TkMu100\",\n",
    "                    # \"Mu15_IsoVVVL_PFHT600\", # VVL                                                                                                                                                                                                                             \n",
    "                ],\n",
    "                'had': [\n",
    "                    \"PFHT1050\",\n",
    "                    \"AK8PFJet400_TrimMass30\",\n",
    "                    \"AK8PFJet420_TrimMass30\",\n",
    "                    \"AK8PFHT800_TrimMass50\",\n",
    "                    \"PFJet500\",\n",
    "                    \"AK8PFJet500\",\n",
    "                ],\n",
    "            }\n",
    "        }[int(self._year)]\n",
    "    \n",
    "        # https://twiki.cern.ch/twiki/bin/view/CMS/MissingETOptionalFiltersRun2                                                                                                                                                                                                 \n",
    "        self._metfilters = {\n",
    "            2016: [\n",
    "                \"goodVertices\",\n",
    "                \"globalSuperTightHalo2016Filter\",\n",
    "                \"HBHENoiseFilter\",\n",
    "                \"HBHENoiseIsoFilter\",\n",
    "                \"EcalDeadCellTriggerPrimitiveFilter\",\n",
    "                \"BadPFMuonFilter\",\n",
    "                \"eeBadScFilter\",\n",
    "            ],\n",
    "            2017: [\n",
    "                \"goodVertices\",\n",
    "                \"globalSuperTightHalo2016Filter\",\n",
    "                \"HBHENoiseFilter\",\n",
    "                \"HBHENoiseIsoFilter\",\n",
    "                \"EcalDeadCellTriggerPrimitiveFilter\",\n",
    "                \"BadPFMuonFilter\",\n",
    "                # \"BadChargedCandidateFilter\",                                                                                                                                                                                                                                  \n",
    "                \"eeBadScFilter\",\n",
    "                \"ecalBadCalibFilter\",\n",
    "            ],\n",
    "            2018:  [\n",
    "                \"goodVertices\",\n",
    "                \"globalSuperTightHalo2016Filter\",\n",
    "                \"HBHENoiseFilter\",\n",
    "                \"HBHENoiseIsoFilter\",\n",
    "                \"EcalDeadCellTriggerPrimitiveFilter\",\n",
    "                \"BadPFMuonFilter\",\n",
    "                # \"BadChargedCandidateFilter\",                                                                                                                                                                                                                                  \n",
    "                \"eeBadScFilter\",\n",
    "                \"ecalBadCalibFilter\",\n",
    "            ],\n",
    "        }[int(self._year)]\n",
    "        \n",
    "        # https://twiki.cern.ch/twiki/bin/viewauth/CMS/BtagRecommendation                                                                                                                                                                                                       \n",
    "        self._btagWPs = {\n",
    "            '2016preVFP': {\n",
    "                'loose': 0.0508,\n",
    "                'medium': 0.2598,\n",
    "                'tight': 0.6502,\n",
    "            },\n",
    "            '2016postVFP': {\n",
    "                'loose': 0.0480,\n",
    "                'medium': 0.2489,\n",
    "                'tight': 0.6377,\n",
    "            },\n",
    "            '2017': {\n",
    "                'loose': 0.0532,\n",
    "                'medium': 0.3040,\n",
    "                'tight': 0.7476,\n",
    "            },\n",
    "            '2018': {\n",
    "                'loose': 0.0490,\n",
    "                'medium': 0.2783,\n",
    "                'tight': 0.7100,\n",
    "            },\n",
    "        }[year+yearmod]\n",
    "        \n",
    "        self.selections = {}\n",
    "        self.cutflows = {}\n",
    "        \n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "    \n",
    "    def h5store(self, store: pd.HDFStore, df: pd.DataFrame, fname: str, gname: str, **kwargs: float) -> None:\n",
    "        store.put(gname, df)\n",
    "        store.get_storer(gname).attrs.metadata = kwargs\n",
    "        \n",
    "    def save_dfs(self, fname, dfs_dict, isMC, dataset, sumgenweight, cutflows):\n",
    "        subdirs =[]\n",
    "        store = pd.HDFStore(fname)\n",
    "        if self._output_location is not None:\n",
    "            for gname, out in dfs_dict.items():\n",
    "                if isMC:\n",
    "                    metadata = dict(sumgenweight=sumgenweight, year=self._year, mc=isMC, dataset=dataset, cutflow=cutflows[gname])  \n",
    "                else:\n",
    "                    metadata = dict(year=self._year, mc=isMC, dataset=dataset, cutflows=cutflows)\n",
    "                store_fin = self.h5store(store, out, fname, gname, **metadata)\n",
    "            store.close()\n",
    "            self.dump_table(fname, self._output_location, subdirs)\n",
    "        else:\n",
    "            print(\"self._output_location is None\")\n",
    "            store.close()\n",
    "    \n",
    "    def dump_table(self, fname: str, location: str, subdirs: Optional[List[str]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Saves locally first, then:\n",
    "        if ``location`` starts with \"root://\", will xrdcp to ``location``/``[subdirs]``/.\n",
    "        else will copy normally to ``location``/``[subdirs]``/.\n",
    "        \"\"\"\n",
    "        \n",
    "        subdirs = subdirs or []\n",
    "        xrd_prefix = \"root://\"\n",
    "        pfx_len = len(xrd_prefix)\n",
    "        xrootd = False\n",
    "        if xrd_prefix in location:\n",
    "            try:\n",
    "                import XRootD\n",
    "                import XRootD.client\n",
    "\n",
    "                xrootd = True\n",
    "            except ImportError:\n",
    "                raise ImportError(\n",
    "                    \"Install XRootD python bindings with: conda install -c conda-forge xroot\"\n",
    "                )\n",
    "        local_file = (\n",
    "            os.path.abspath(os.path.join(\".\", fname))\n",
    "            if xrootd\n",
    "            else os.path.join(\".\", fname)\n",
    "        )\n",
    "        merged_subdirs = \"/\".join(subdirs) if xrootd else os.path.sep.join(subdirs)\n",
    "        destination = (\n",
    "            location + merged_subdirs + f\"/{fname}\"\n",
    "            if xrootd\n",
    "            else os.path.join(location, os.path.join(merged_subdirs, fname))\n",
    "        )\n",
    "        if xrootd:\n",
    "            copyproc = XRootD.client.CopyProcess()\n",
    "            copyproc.add_job(local_file, destination)\n",
    "            copyproc.prepare()\n",
    "            copyproc.run()\n",
    "            client = XRootD.client.FileSystem(\n",
    "                location[: location[pfx_len:].find(\"/\") + pfx_len]\n",
    "            )\n",
    "            status = client.locate(\n",
    "                destination[destination[pfx_len:].find(\"/\") + pfx_len + 1 :],\n",
    "                XRootD.client.flags.OpenFlags.READ,\n",
    "            )\n",
    "            assert status[0].ok\n",
    "            del client\n",
    "            del copyproc\n",
    "        else:\n",
    "            if not os.path.exists('./outfiles/'):     ### putting the files in ./outfiles directory\n",
    "                os.makedirs('./outfiles/')\n",
    "            dirname = os.path.dirname('./outfiles/' + destination)\n",
    "            if not os.path.exists(dirname):\n",
    "                pathlib.Path(dirname).mkdir(parents=True, exist_ok=True)\n",
    "            ## TODO: need to fix this\n",
    "            if not os.path.samefile(local_file, destination):                \n",
    "                shutil.copy2(local_file, destination)\n",
    "            else:\n",
    "                fname =  \"./outfiles/condor_\" + fname    ### putting the files in ./outfiles directory\n",
    "                destination = os.path.join(location, os.path.join(merged_subdirs, fname))\n",
    "                shutil.copy2(local_file, destination)\n",
    "            assert os.path.isfile(destination)\n",
    "        pathlib.Path(local_file).unlink()\n",
    "        \n",
    "    def ak_to_pandas(self, output_collection: ak.Array) -> pd.DataFrame:\n",
    "        output = pd.DataFrame()\n",
    "        for field in ak.fields(output_collection):\n",
    "            output[field] = ak.to_numpy(output_collection[field])\n",
    "        return output\n",
    "                      \n",
    "    def add_selection(self, name: str, sel: np.ndarray, channel: str = None):\n",
    "        \"\"\"Adds selection to PackedSelection object and the cutflow dictionary\"\"\"\n",
    "        channels = [channel] if channel else self._channels\n",
    "        for ch in channels:\n",
    "            self.selections[ch].add(name, sel)\n",
    "            self.cutflows[ch][name] = np.sum(self.selections[ch].all(*self.selections[ch].names))\n",
    "        \n",
    "    def process(self, events: ak.Array):\n",
    "        \"\"\"Returns skimmed events which pass preselection cuts and with the branches listed in self._skimvars\"\"\"\n",
    "        dataset = events.metadata['dataset']\n",
    "        isMC = hasattr(events, \"genWeight\")\n",
    "        sumgenweight = ak.sum(events.genWeight) if isMC else 0\n",
    "        nevents = len(events)\n",
    "                      \n",
    "        # empty selections and cutflows\n",
    "        self.selections = {}\n",
    "        self.cutflows = {}\n",
    "        for ch in self._channels:\n",
    "            self.selections[ch] = PackedSelection()\n",
    "            self.cutflows[ch] = {}\n",
    "            self.cutflows[ch][\"all\"] = nevents\n",
    "\n",
    "        # trigger                                                                                                                                                                                                   \n",
    "        triggers = {}\n",
    "        for ch in self._channels:\n",
    "            if ch==\"had\" and isMC:\n",
    "                trigger = np.ones(nevents, dtype='bool')\n",
    "            else:\n",
    "                # apply trigger to both data and MC (except for hadronic channel)                                                                                                                                   \n",
    "                trigger = np.zeros(len(events), dtype='bool')\n",
    "                for t in self._HLTs[ch]:\n",
    "                    if t in events.HLT.fields:\n",
    "                        trigger = trigger | events.HLT[t]\n",
    "            self.add_selection(\"trigger\", trigger, ch)\n",
    "            del trigger\n",
    "            \n",
    "        # metfilters\n",
    "        metfilters = np.ones(nevents, dtype='bool')\n",
    "        for mf in self._metfilters:\n",
    "            if mf in events.Flag.fields:\n",
    "                metfilters = metfilters & events.Flag[mf]\n",
    "        self.add_selection(\"metfilters\", metfilters)\n",
    "\n",
    "        # muons                                                                                                                                                                                                     \n",
    "        goodmuon = (\n",
    "            (events.Muon.pt > 25)\n",
    "            & (np.abs(events.Muon.eta) < 2.4)\n",
    "            & (np.abs(events.Muon.dz) < 0.5)\n",
    "            & (np.abs(events.Muon.dxy) < 0.2)\n",
    "            & events.Muon.mediumId\n",
    "        )\n",
    "        nmuons = ak.sum(goodmuon, axis=1)\n",
    "        lowptmuon = (\n",
    "            (events.Muon.pt > 10)\n",
    "            & (abs(events.Muon.eta) < 2.4)\n",
    "            & (np.abs(events.Muon.dz) < 0.5)\n",
    "            & (np.abs(events.Muon.dxy) < 0.2)\n",
    "            & events.Muon.looseId\n",
    "        )\n",
    "        nlowptmuons = ak.sum(lowptmuon, axis=1)\n",
    "        \n",
    "        # electrons\n",
    "        goodelectron = (\n",
    "            (events.Electron.pt > 25)\n",
    "            & (abs(events.Electron.eta) < 2.5)\n",
    "            & ((1.44 < np.abs(events.Electron.eta)) | (np.abs(events.Electron.eta) > 1.57))\n",
    "            & (events.Electron.mvaFall17V2noIso_WP90)\n",
    "        )\n",
    "        nelectrons = ak.sum(goodelectron, axis=1)\n",
    "        lowptelectron = (\n",
    "            (events.Electron.pt > 10)\n",
    "            & ((1.44 < np.abs(events.Electron.eta)) | (np.abs(events.Electron.eta) > 1.57))\n",
    "            & (np.abs(events.Electron.eta) < 2.4)\n",
    "            & (events.Electron.cutBased >= events.Electron.LOOSE)\n",
    "        )\n",
    "        nlowptelectrons = ak.sum(lowptelectron, axis=1)\n",
    "        \n",
    "        goodleptons = ak.concatenate([events.Muon[goodmuon], events.Electron[goodelectron]], axis=1)\n",
    "        candidatelep = ak.firsts(goodleptons[ak.argsort(goodleptons.pt), ascending=False)] \n",
    "        candidatelep_p4 = ak.zip(\n",
    "            {\n",
    "                \"pt\": candidatelep.pt,\n",
    "                \"eta\": candidatelep.eta,\n",
    "                \"phi\": candidatelep.phi,\n",
    "                \"mass\": candidatelep.mass,\n",
    "                \"charge\": candidatelep.charge,\n",
    "            },\n",
    "            with_name=\"PtEtaPhiMCandidate\",\n",
    "            behavior=candidate.behavior,\n",
    "        )\n",
    "        \n",
    "         # define isolation\n",
    "        ele_iso = ak.where(candidatelep.pt >= 120., candidatelep.pfRelIso03_all, candidatelep.pfRelIso03_all)       \n",
    "        mu_iso = ak.where(candidatelep.pt >= 55., candidatelep.miniPFRelIso_all, candidatelep.pfRelIso03_all)  \n",
    "        \n",
    "        ### add electron selections\n",
    "        self.add_selection(\n",
    "            name = 'oneelectron', \n",
    "            sel = (nmuons == 0) & (nelectrons == 1), \n",
    "            channel='ele'\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name = 'electronkin', \n",
    "            sel = (candidatelep.pt > 30.) & abs(candidatelep.eta < 2.4), \n",
    "            channel='ele'\n",
    "        )\n",
    "        \n",
    "        ### add muon selections\n",
    "        self.add_selection(\n",
    "            name = 'onemuon', \n",
    "            sel = (nmuons == 1) & (nelectrons == 0), \n",
    "            channel='mu'\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name = 'muonkin', \n",
    "            sel = (candidatelep.pt > 27.) & abs(candidatelep.eta < 2.4), \n",
    "            channel='mu'\n",
    "         )        \n",
    "        \n",
    "        # initialize pandas dataframe\n",
    "        output = {}\n",
    "        for ch in self._channels:\n",
    "            out = {}\n",
    "            for var in self._skimvars[ch]:\n",
    "                if var == \"lepton_pt\": \n",
    "                    value = pad_val(candidatelep.pt,0)\n",
    "                    out[var] = value\n",
    "                if var == \"lep_isolation\": \n",
    "                    if ch == 'ele':\n",
    "                        value = pad_val(ele_iso,0)\n",
    "                    elif ch == 'mu':\n",
    "                        value = pad_val(mu_iso,0)                        \n",
    "                    out[var] = value\n",
    "                else: continue\n",
    "                \n",
    "            # print arrays and selections to debug\n",
    "            # print(out)\n",
    "            # print(selections[ch].all(*selections[ch].names))\n",
    "            \n",
    "            # apply selections\n",
    "            output[ch] = {\n",
    "                key: value[self.selections[ch].all(*self.selections[ch].names)] for (key, value) in out.items()\n",
    "            }\n",
    "\n",
    "            # convert arrays to pandas\n",
    "            if not isinstance(output[ch], pd.DataFrame): output[ch] = self.ak_to_pandas(output[ch])\n",
    "                \n",
    "        # now save pandas dataframes\n",
    "        fname = events.behavior[\"__events_factory__\"]._partition_key.replace(\"/\", \"_\") + \"-out.hdf5\"        \n",
    "        self.save_dfs(fname, output, isMC, dataset, sumgenweight, self.cutflows)\n",
    "        \n",
    "        return {}\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "078d8f8b-31a1-4780-a86d-d3bd6722617a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GluGluHToWWToLNuQQ_M125_TuneCP5_PSweight_13TeV-powheg2-jhugen727-pythia8\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "datasets = {\"GluGluHToWWToLNuQQ_M125_TuneCP5_PSweight_13TeV-powheg2-jhugen727-pythia8\": \"HWW\",\n",
    "           }\n",
    "\n",
    "fileset = {}\n",
    "for dataset_name,dataset in datasets.items():\n",
    "    print(dataset_name)\n",
    "    with open(\"../fileset/v2_2/2017.json\", 'r') as f:\n",
    "        files = json.load(f)[dataset][dataset_name]\n",
    "    \n",
    "    # use all_files False if you want to test\n",
    "    all_files = False\n",
    "    # need to define the fileset but call them with xcache\n",
    "    if all_files:\n",
    "        fileset[dataset_name] = [\"root://xcache/\"+ f for f in files]\n",
    "    else:\n",
    "        fileset[dataset_name] = [[\"root://xcache/\"+ f for f in files][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "cae6868f-e8b3-4bde-9f36-9dbace1f71dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GluGluHToWWToLNuQQ_M125_TuneCP5_PSweight_13TeV-powheg2-jhugen727-pythia8\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23e24e092f5347659bb23285bd0a5dd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/10 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import uproot\n",
    "uproot.open.defaults['xrootd_handler'] = uproot.source.xrootd.MultithreadedXRootDSource\n",
    "\n",
    "from coffea.processor import IterativeExecutor,Runner,DaskExecutor\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "\n",
    "dask_executor = False\n",
    " \n",
    "    \n",
    "# define executor \n",
    "if dask_executor:\n",
    "    executor = DaskExecutor(compression=1, status=True, client=client, treereduction=2)\n",
    "else:\n",
    "    executor = IterativeExecutor(compression=1, status=True)\n",
    "\n",
    "# define the runner (Same as before)\n",
    "run = Runner(executor=executor,savemetrics=True,chunksize=10000,schema=NanoAODSchema)\n",
    "\n",
    "# run\n",
    "for dataset,dataset_files in fileset.items():\n",
    "    new_fileset = {dataset: dataset_files}\n",
    "    print(dataset)\n",
    "    hwwproc = HwwProcessor(year=\"2017\", channels=[\"ele\",\"mu\",\"had\"], output_location=\"./\")\n",
    "    out,metrics = run(new_fileset,'Events',processor_instance=hwwproc)\n",
    "\n",
    "    # still need to add all hdf5s.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "f7beea7c-b84f-4510-9258-00be772b2442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sumgenweight': 284307.16, 'year': '2017', 'mc': True, 'dataset': 'GluGluHToWWToLNuQQ_M125_TuneCP5_PSweight_13TeV-powheg2-jhugen727-pythia8', 'cutflow': {'all': 9840, 'trigger': 585, 'metfilters': 585, 'oneelectron': 141, 'electronkin': 140, 'muonkin': 140}}\n",
      "{'sumgenweight': 284307.16, 'year': '2017', 'mc': True, 'dataset': 'GluGluHToWWToLNuQQ_M125_TuneCP5_PSweight_13TeV-powheg2-jhugen727-pythia8', 'cutflow': {'all': 9840, 'trigger': 1114, 'metfilters': 1113, 'onemuon': 1092}}\n",
      "{'sumgenweight': 284307.16, 'year': '2017', 'mc': True, 'dataset': 'GluGluHToWWToLNuQQ_M125_TuneCP5_PSweight_13TeV-powheg2-jhugen727-pythia8', 'cutflow': {'all': 9840, 'trigger': 9840, 'metfilters': 9835}}\n"
     ]
    }
   ],
   "source": [
    "# debug output\n",
    "import pandas as pd\n",
    "fname = \"./outfiles/condor_96af172a-47cf-11ec-ac53-0f19000abeef_%2FEvents%3B1_0-9840-out.hdf5\"\n",
    "with pd.HDFStore(fname) as store: \n",
    "    data_ele = store[\"ele\"]\n",
    "    data_mu = store[\"mu\"]\n",
    "    data_had = store[\"had\"]\n",
    "   \n",
    "    metadata_ele = store.get_storer(\"ele\").attrs.metadata\n",
    "    metadata_mu = store.get_storer(\"mu\").attrs.metadata\n",
    "    metadata_had = store.get_storer(\"had\").attrs.metadata\n",
    "\n",
    "    print(metadata_ele)\n",
    "    print(metadata_mu)\n",
    "    print(metadata_had)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5621a6c8-54d7-4104-a10d-7e291680d251",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
