{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging inference \n",
    "\n",
    "Using latest version of coffea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pyg-coffea/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "import pyarrow as pa\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "from typing import List, Optional, Dict\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import importlib.resources\n",
    "\n",
    "from coffea import processor\n",
    "from coffea.nanoevents.methods import candidate, vector\n",
    "from coffea.analysis_tools import Weights, PackedSelection\n",
    "\n",
    "from coffea.nanoevents.methods.base import NanoEventsArray\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema, PFNanoAODSchema\n",
    "from coffea.nanoevents.methods import candidate, vector\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Found duplicate branch \")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Missing cross-reference index \")\n",
    "np.seterr(invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NanoAODschema\n",
    "\n",
    "We want to use PFNanoAODSchema since that load PFCands as a candidate particles, i.e. they have 4-vector properties.\n",
    "\n",
    "https://coffeateam.github.io/coffea/api/coffea.nanoevents.PFNanoAODSchema.html\n",
    "https://github.com/CoffeaTeam/coffea/blob/7dd4f863837a6319579f078c9e445c61d9106943/coffea/nanoevents/schemas/nanoaod.py#L282\n",
    "\n",
    "Additionally, PFNanoAODSchema loads SecondaryVertices as SecondaryVertex:\n",
    "https://github.com/CoffeaTeam/coffea/blob/7dd4f863837a6319579f078c9e445c61d9106943/coffea/nanoevents/schemas/nanoaod.py#L68\n",
    "https://github.com/CoffeaTeam/coffea/blob/f2a99631dcf95b46bd0225b242b3ba512a30a89a/coffea/nanoevents/methods/nanoaod.py#L388\n",
    "\n",
    "We do not neccessarily want this since this means we don't have a candidate 4-vector for the SecondaryVertex and we can't do operations like `delta_phi`. \n",
    "We need to modify this to a `mixin` of PFCand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFCand\n",
      "SecondaryVertex\n",
      "PFCand\n"
     ]
    }
   ],
   "source": [
    "print(PFNanoAODSchema.mixins['PFCands'])\n",
    "print(PFNanoAODSchema.mixins['SV'])\n",
    "\n",
    "# interpret SV with PFCand behavior\n",
    "PFNanoAODSchema.mixins[\"SV\"] = \"PFCand\"\n",
    "\n",
    "print(PFNanoAODSchema.mixins['SV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening file w PFNanoAODSchema\n",
    "\n",
    "Here we manually open the file (only 200 entries), and we use `PFNanoAODSchema`.\n",
    "We also open the json file that specifies how many pf candidates/svs are going to be used in the tagger, as well as any normalization that should be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'dy_sample.root'\n",
    "events = NanoEventsFactory.from_root(file, schemaclass=PFNanoAODSchema, entry_stop=200).events()\n",
    "\n",
    "with open(f\"03_31_ak8.json\") as f:\n",
    "    tagger_vars = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the selection such that it mimics the boostedHiggs selection. We use the fatjet closest to the lepton and obtain its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_p4(cand):\n",
    "    return ak.zip(\n",
    "        {\n",
    "            \"pt\": cand.pt,\n",
    "            \"eta\": cand.eta,\n",
    "            \"phi\": cand.phi,\n",
    "            \"mass\": cand.mass,\n",
    "            \"charge\": cand.charge,\n",
    "        },\n",
    "        with_name=\"PtEtaPhiMCandidate\",\n",
    "        behavior=candidate.behavior,\n",
    "    )\n",
    "\n",
    "good_muons = (\n",
    "    (events.Muon.pt > 30)\n",
    "    & (np.abs(events.Muon.eta) < 2.4)\n",
    "    & (np.abs(events.Muon.dz) < 0.1)\n",
    "    & (np.abs(events.Muon.dxy) < 0.05)\n",
    "    & (events.Muon.sip3d <= 4.0)\n",
    "    & events.Muon.mediumId\n",
    ")   \n",
    "good_electrons = (\n",
    "    (events.Electron.pt > 38)\n",
    "    & (np.abs(events.Electron.eta) < 2.4)\n",
    "    & ((np.abs(events.Electron.eta) < 1.44) | (np.abs(events.Electron.eta) > 1.57))\n",
    "    & (np.abs(events.Electron.dz) < 0.1)\n",
    "    & (np.abs(events.Electron.dxy) < 0.05)\n",
    "    & (events.Electron.sip3d <= 4.0)\n",
    "    & (events.Electron.mvaFall17V2noIso_WP90)\n",
    ")\n",
    "\n",
    "# get candidate lepton\n",
    "goodleptons = ak.concatenate([events.Muon[good_muons], events.Electron[good_electrons]], axis=1)    # concat muons and electrons\n",
    "goodleptons = goodleptons[ak.argsort(goodleptons.pt, ascending=False)]      # sort by pt\n",
    "candidatelep = ak.firsts(goodleptons)   # pick highest pt\n",
    "candidatelep_p4 = build_p4(candidatelep) \n",
    "\n",
    "# get candidate fj\n",
    "fatjets = events.FatJet\n",
    "good_fatjets = (\n",
    "    (fatjets.pt > 200)\n",
    "    & (abs(fatjets.eta) < 2.5)\n",
    "    & fatjets.isTight\n",
    ")\n",
    "n_fatjets = ak.sum(good_fatjets, axis=1)\n",
    "good_fatjets = fatjets[good_fatjets]        # select good fatjets\n",
    "good_fatjets = good_fatjets[ak.argsort(good_fatjets.pt, ascending=False)]    # sort by pt\n",
    "lep_in_fj_overlap_bool = ~ak.is_none(ak.firsts(good_fatjets.delta_r(candidatelep_p4) > 0.1))\n",
    "good_fatjets = ak.mask(good_fatjets, lep_in_fj_overlap_bool)\n",
    "\n",
    "# get idx and fj\n",
    "fj_idx_lep = ak.argmin(good_fatjets.delta_r(candidatelep_p4), axis=1, keepdims=True)\n",
    "candidatefj_lep = ak.firsts(good_fatjets[fj_idx_lep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the candidate jet is jet # [0]\n",
      "the candidate jet is jet # [1]\n"
     ]
    }
   ],
   "source": [
    "print(f'the candidate jet is jet # {fj_idx_lep[74]}')\n",
    "print(f'the candidate jet is jet # {fj_idx_lep[92]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where jet pt > 0\n",
      "(array([ 17,  48,  66,  74,  80,  81,  82,  90,  92, 104, 105, 116, 123,\n",
      "       159, 160, 164]),)\n"
     ]
    }
   ],
   "source": [
    "fatjet_label = \"FatJet\"\n",
    "pfcands_label = \"FatJetPFCands\"\n",
    "svs_label = \"FatJetSVs\"\n",
    "\n",
    "# get the jet given the index (take firsts to avoid singletons)\n",
    "jet = ak.firsts(events[fatjet_label][fj_idx_lep])\n",
    "\n",
    "# print indices of events where we actually have a jet\n",
    "print('Indices where jet pt > 0')\n",
    "print(np.where((ak.fill_none(jet.pt,-1)>0).to_numpy()))    # fill \"None\" events by -1, contsruct a bool and get the inidices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug PFCand features and mask\n",
    "\n",
    "The mask is just an array with a subarray of the length of the number of features e.g. (100). \n",
    "\n",
    "For example, if for a given jet we only have 2 PF Candidates and we have 100 PFcandidate points in our network, we would have:\n",
    "```\n",
    "mask = [1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
    " 0. 0. 0. 0.]\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<FatJetArray [None, None, None, ... None, None, None] type='200 * ?fatJet'>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet = ak.firsts(events[fatjet_label][fj_idx_lep])\n",
    "jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk = events[pfcands_label].jetIdx == ak.firsts(fj_idx_lep)\n",
    "ak.sum(msk[48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AssociatedPFCandArray [None, None, None, ... None, None, None] type='200 * opti...'>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet_ak_pfcands = events[pfcands_label][msk]\n",
    "jet_ak_pfcands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AssociatedPFCandArray [AssociatedPFCand, ... AssociatedPFCand] type='8 * associ...'>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet_ak_pfcands[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PFCandArray [PFCand, PFCand, ... PFCand, PFCand] type='8 * pFCand'>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jet_pfcands = (events.PFCands[jet_ak_pfcands.pFCandsIdx])\n",
    "jet_pfcands[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "See how pfcand variable looks for jet 48\n",
      "[1.26, 1.46, 1.45, 1.41, 1.34, 1.28, 1.44, 1.06]\n",
      "------------------------------------------------------------------------\n",
      "Pad until sub-arrays have the same length\n",
      "[1.255126953125 1.46142578125 1.4521484375 1.408935546875 1.336181640625\n",
      " 1.278564453125 1.4365234375 1.05908203125 -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- -- --\n",
      " -- -- -- -- -- -- -- -- -- --]\n",
      "------------------------------------------------------------------------\n",
      "Get a mask... mask will be by default True if None and False if there's a PF candidate so we invert it\n",
      "Inverted mask\n",
      "[ True  True  True  True  True  True  True  True False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False False False False False False False False False\n",
      " False False False False]\n",
      "------------------------------------------------------------------------\n",
      "Convert boolean mask to ones and zeros\n",
      "[1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "jet = ak.firsts(events[fatjet_label][fj_idx_lep])\n",
    "msk = events[pfcands_label].jetIdx == ak.firsts(fj_idx_lep)\n",
    "jet_ak_pfcands = events[pfcands_label][msk]\n",
    "jet_pfcands = (events.PFCands[jet_ak_pfcands.pFCandsIdx])\n",
    "\n",
    "# get any feature of pfcands\n",
    "pfcand_abseta = np.abs(jet_pfcands.eta)\n",
    "\n",
    "# index for jet w pfcands \n",
    "test_idx = 48\n",
    "\n",
    "print(f'See how pfcand variable looks for jet {test_idx}')\n",
    "print(pfcand_abseta[test_idx])\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Pad until sub-arrays have the same length')\n",
    "print(ak.pad_none(pfcand_abseta, tagger_vars[\"pf_points\"][\"var_length\"], axis=1, clip=True).to_numpy()[test_idx])\n",
    "\n",
    "print('------------------------------------------------------------------------')\n",
    "print(\"Get a mask... mask will be by default True if None and False if there's a PF candidate so we invert it\")\n",
    "invert_msk = ~(ak.pad_none(pfcand_abseta, tagger_vars[\"pf_points\"][\"var_length\"], axis=1, clip=True).to_numpy().mask)\n",
    "print('Inverted mask')\n",
    "print(invert_msk[test_idx])\n",
    "print('------------------------------------------------------------------------')\n",
    "print('Convert boolean mask to ones and zeros')\n",
    "print(invert_msk.astype(np.float32)[test_idx])\n",
    "pfcand_mask = invert_msk.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet = ak.firsts(events[fatjet_label][fj_idx_lep])\n",
    "msk = (events[svs_label].jetIdx == ak.firsts(fj_idx_lep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [False, False, True] type='3 * bool'>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msk[92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [0, 0, 1] type='3 * int32[parameters={\"__doc__\": \"Index of the parent jet\"}]'>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[svs_label].jetIdx[92]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_msk = (events[svs_label].sVIdx != -1) * (msk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [False, False] type='2 * bool'>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((events[svs_label].sVIdx != -1)*msk)[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [] type='0 * int32[parameters={\"__doc__\": \"Index in the SV list\"}]'>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[svs_label][sv_msk].sVIdx[48]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Debug SV features and mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      "  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1 -1 -1 -1 -1 -1\n",
      " -1 -1  0 -1 -1 -1 -1 -1  1  0  0 -1 -1 -1 -1 -1 -1 -1  0 -1  0 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1  2  0 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  0 -1 -1 -1\n",
      " -1 -1 -1  1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  2  0 -1 -1 -1  1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1]\n",
      "Index of events where we actually have SVs\n",
      "(array([ 66,  80, 104, 123, 159, 164]),)\n",
      "See how sv variable looks for jet 66\n",
      "[0.0429]\n",
      "Convert mask to ones and zeros\n",
      "[[False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " ...\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]\n",
      " [False False False ... False False False]]\n",
      "See how mask looks for jet 66\n",
      "[ True False False False False False False]\n",
      "Delta phi for jet 66\n",
      "[0.0332]\n",
      "Fields for jet svs\n",
      "['dlen', 'dlenSig', 'dxy', 'dxySig', 'pAngle', 'charge', 'chi2', 'eta', 'mass', 'ndof', 'phi', 'pt', 'x', 'y', 'z', 'ntracks']\n"
     ]
    }
   ],
   "source": [
    "jet = ak.firsts(events[fatjet_label][fj_idx_lep])\n",
    "msk = (events[svs_label].jetIdx == ak.firsts(fj_idx_lep))\n",
    "\n",
    "# find index where msk is true and svIdx is not -1\n",
    "sv_msk = (events[svs_label].sVIdx != -1) * (msk)\n",
    "count_sv = ak.fill_none(ak.sum(sv_msk,axis=1),-1,axis=0).to_numpy()\n",
    "print(count_sv)\n",
    "print('Index of events where we actually have SVs')\n",
    "print(np.where(count_sv>0))\n",
    "\n",
    "# get jet svs\n",
    "jet_svs = events.SV[\n",
    "        events[svs_label][sv_msk].sVIdx\n",
    "        ]\n",
    "\n",
    "# index for jet w svs\n",
    "test_idx = 66\n",
    "\n",
    "# get sv  feature\n",
    "eta_sign = ak.values_astype(jet_svs.eta > 0, int) * 2 - 1\n",
    "sv_etarel = eta_sign * (jet_svs.eta - jet.eta)\n",
    "print(f'See how sv variable looks for jet {test_idx}')\n",
    "print(sv_etarel[test_idx])\n",
    "\n",
    "# get mask\n",
    "sv_mask = ~ak.pad_none(\n",
    "            sv_etarel, tagger_vars[\"sv_points\"][\"var_length\"], axis=1, clip=True\n",
    "        ).to_numpy().mask    # .to_numpy() will make the [] and the None be both padded in the same way\n",
    "print('Convert mask to ones and zeros')\n",
    "sv_mask.astype(np.float32)\n",
    "print(sv_mask)\n",
    "print(f'See how mask looks for jet {test_idx}')\n",
    "print(sv_mask[test_idx])\n",
    "print(f'Delta phi for jet {test_idx}')\n",
    "print(jet_svs.delta_phi(jet)[test_idx])\n",
    "print('Fields for jet svs')\n",
    "print(jet_svs.fields)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now get feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pfcands_features(\n",
    "    tagger_vars: dict,\n",
    "    preselected_events: NanoEventsArray,\n",
    "    fj_idx_lep,\n",
    "    fatjet_label: str = \"FatJetAK15\",\n",
    "    pfcands_label: str = \"FatJetPFCands\",\n",
    "    normalize: bool = True,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts the pf_candidate features specified in the ``tagger_vars`` dict from the\n",
    "    ``preselected_events`` and returns them as a dict of numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "    jet = ak.firsts(preselected_events[fatjet_label][fj_idx_lep])\n",
    "\n",
    "    msk = preselected_events[pfcands_label].jetIdx == ak.firsts(fj_idx_lep)\n",
    "    jet_ak_pfcands = preselected_events[pfcands_label][msk]\n",
    "    jet_pfcands = (preselected_events.PFCands[jet_ak_pfcands.pFCandsIdx])\n",
    "\n",
    "    # negative eta jets have -1 sign, positive eta jets have +1\n",
    "    eta_sign = ak.values_astype(jet_pfcands.eta > 0, int) * 2 - 1\n",
    "    feature_dict[\"pfcand_etarel\"] = eta_sign * (jet_pfcands.eta - jet.eta)\n",
    "    feature_dict[\"pfcand_phirel\"] = jet.delta_phi(jet_pfcands)\n",
    "    feature_dict[\"pfcand_abseta\"] = np.abs(jet_pfcands.eta)\n",
    "\n",
    "    feature_dict[\"pfcand_pt_log_nopuppi\"] = np.log(jet_pfcands.pt)\n",
    "    feature_dict[\"pfcand_e_log_nopuppi\"] = np.log(jet_pfcands.energy)\n",
    "\n",
    "    pdgIds = jet_pfcands.pdgId\n",
    "    feature_dict[\"pfcand_isEl\"] = np.abs(pdgIds) == 11\n",
    "    feature_dict[\"pfcand_isMu\"] = np.abs(pdgIds) == 13\n",
    "    feature_dict[\"pfcand_isChargedHad\"] = np.abs(pdgIds) == 211\n",
    "    feature_dict[\"pfcand_isGamma\"] = np.abs(pdgIds) == 22\n",
    "    feature_dict[\"pfcand_isNeutralHad\"] = np.abs(pdgIds) == 130\n",
    "\n",
    "    feature_dict[\"pfcand_charge\"] = jet_pfcands.charge\n",
    "    feature_dict[\"pfcand_VTX_ass\"] = jet_pfcands.pvAssocQuality\n",
    "    feature_dict[\"pfcand_lostInnerHits\"] = jet_pfcands.lostInnerHits\n",
    "    feature_dict[\"pfcand_quality\"] = jet_pfcands.trkQuality\n",
    "\n",
    "    feature_dict[\"pfcand_normchi2\"] = np.floor(jet_pfcands.trkChi2)\n",
    "\n",
    "    feature_dict[\"pfcand_dz\"] = jet_pfcands.dz\n",
    "    feature_dict[\"pfcand_dxy\"] = jet_pfcands.d0\n",
    "    feature_dict[\"pfcand_dzsig\"] = jet_pfcands.dz / jet_pfcands.dzErr\n",
    "    feature_dict[\"pfcand_dxysig\"] = jet_pfcands.d0 / jet_pfcands.d0Err\n",
    "\n",
    "    # btag vars\n",
    "    for var in tagger_vars[\"pf_features\"][\"var_names\"]:\n",
    "        if \"btag\" in var:\n",
    "            feature_dict[var] = jet_ak_pfcands[var[len(\"pfcand_\"):]]\n",
    "\n",
    "    # pfcand mask\n",
    "    feature_dict[\"pfcand_mask\"] = (~(ak.pad_none(feature_dict[\"pfcand_abseta\"], tagger_vars[\"pf_points\"][\"var_length\"], axis=1, clip=True).to_numpy().mask)).astype(np.float32)\n",
    "\n",
    "    # convert to numpy arrays and normalize features\n",
    "    for var in tagger_vars[\"pf_features\"][\"var_names\"]:\n",
    "        a = (\n",
    "            ak.pad_none(\n",
    "                feature_dict[var], tagger_vars[\"pf_points\"][\"var_length\"], axis=1, clip=True\n",
    "            )\n",
    "            .to_numpy()\n",
    "            .filled(fill_value=0)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        if normalize:\n",
    "            info = tagger_vars[\"pf_features\"][\"var_infos\"][var]\n",
    "            a = (a - info[\"median\"]) * info[\"norm_factor\"]\n",
    "            a = np.clip(a, info.get(\"lower_bound\", -5), info.get(\"upper_bound\", 5))\n",
    "\n",
    "        feature_dict[var] = a\n",
    "\n",
    "    if normalize:\n",
    "        var = \"pfcand_normchi2\"\n",
    "        info = tagger_vars[\"pf_features\"][\"var_infos\"][var]\n",
    "        # finding what -1 transforms to\n",
    "        chi2_min = -1 - info[\"median\"] * info[\"norm_factor\"]\n",
    "        feature_dict[var][feature_dict[var] == chi2_min] = info[\"upper_bound\"]\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svs_features(\n",
    "    tagger_vars: dict,\n",
    "    preselected_events: NanoEventsArray,\n",
    "    fj_idx_lep,\n",
    "    fatjet_label: str = \"FatJetAK15\",\n",
    "    svs_label: str = \"JetSVsAK15\",\n",
    "    normalize: bool = True,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts the sv features specified in the ``tagger_vars`` dict from the\n",
    "    ``preselected_events`` and returns them as a dict of numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "    jet = ak.firsts(preselected_events[fatjet_label][fj_idx_lep])\n",
    "    msk = preselected_events[svs_label].jetIdx == ak.firsts(fj_idx_lep)\n",
    "    jet_svs = preselected_events.SV[\n",
    "        preselected_events[svs_label].sVIdx[\n",
    "            (preselected_events[svs_label].sVIdx != -1)\n",
    "            * (msk)\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # negative eta jets have -1 sign, positive eta jets have +1\n",
    "    eta_sign = ak.values_astype(jet_svs.eta > 0, int) * 2 - 1\n",
    "    feature_dict[\"sv_etarel\"] = eta_sign * (jet_svs.eta - jet.eta)\n",
    "    feature_dict[\"sv_phirel\"] = jet_svs.delta_phi(jet)\n",
    "    feature_dict[\"sv_abseta\"] = np.abs(jet_svs.eta)\n",
    "    feature_dict[\"sv_mass\"] = jet_svs.mass\n",
    "    feature_dict[\"sv_pt_log\"] = np.log(jet_svs.pt)\n",
    "\n",
    "    feature_dict[\"sv_ntracks\"] = jet_svs.ntracks\n",
    "    feature_dict[\"sv_normchi2\"] = jet_svs.chi2\n",
    "    feature_dict[\"sv_dxy\"] = jet_svs.dxy\n",
    "    feature_dict[\"sv_dxysig\"] = jet_svs.dxySig\n",
    "    feature_dict[\"sv_d3d\"] = jet_svs.dlen\n",
    "    feature_dict[\"sv_d3dsig\"] = jet_svs.dlenSig\n",
    "    svpAngle = jet_svs.pAngle\n",
    "    feature_dict[\"sv_costhetasvpv\"] = -np.cos(svpAngle)\n",
    "\n",
    "    feature_dict[\"sv_mask\"] = (~(ak.pad_none(feature_dict[\"sv_etarel\"], tagger_vars[\"sv_points\"][\"var_length\"], axis=1, clip=True).to_numpy().mask)).astype(np.float32)\n",
    "\n",
    "    # convert to numpy arrays and normalize features\n",
    "    for var in tagger_vars[\"sv_features\"][\"var_names\"]:\n",
    "        a = (\n",
    "            ak.pad_none(\n",
    "                feature_dict[var], tagger_vars[\"sv_points\"][\"var_length\"], axis=1, clip=True\n",
    "            )\n",
    "            .to_numpy()\n",
    "            .filled(fill_value=0)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        if normalize:\n",
    "            info = tagger_vars[\"sv_features\"][\"var_infos\"][var]\n",
    "            a = (a - info[\"median\"]) * info[\"norm_factor\"]\n",
    "            a = np.clip(a, info.get(\"lower_bound\", -5), info.get(\"upper_bound\", 5))\n",
    "\n",
    "        feature_dict[var] = a\n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatjet_label = \"FatJet\"\n",
    "pfcands_label = \"FatJetPFCands\"\n",
    "svs_label = \"FatJetSVs\"\n",
    "\n",
    "selection = candidatefj_lep.pt > 200\n",
    "\n",
    "feature_dict = get_pfcands_features(tagger_vars, events[selection], fj_idx_lep[selection], fatjet_label, pfcands_label)\n",
    "feature_dict = {\n",
    "    **get_pfcands_features(tagger_vars, events[selection], fj_idx_lep[selection], fatjet_label, pfcands_label),\n",
    "    **get_svs_features(tagger_vars, events[selection], fj_idx_lep[selection], fatjet_label, svs_label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'pfcand_etarel': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_phirel': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_abseta': array([[-0.96000004, -0.96000004, -0.96000004, ..., -0.96000004,\n",
      "        -0.96000004, -0.96000004],\n",
      "       [-0.96000004, -0.96000004, -0.96000004, ..., -0.96000004,\n",
      "        -0.96000004, -0.96000004],\n",
      "       [-0.96000004, -0.96000004, -0.96000004, ..., -0.96000004,\n",
      "        -0.96000004, -0.96000004],\n",
      "       ...,\n",
      "       [-0.96000004, -0.96000004, -0.96000004, ..., -0.96000004,\n",
      "        -0.96000004, -0.96000004],\n",
      "       [-0.96000004, -0.96000004, -0.96000004, ..., -0.96000004,\n",
      "        -0.96000004, -0.96000004],\n",
      "       [-0.96000004, -0.96000004, -0.96000004, ..., -0.96000004,\n",
      "        -0.96000004, -0.96000004]], dtype=float32), 'pfcand_pt_log_nopuppi': array([[-0.5, -0.5, -0.5, ..., -0.5, -0.5, -0.5],\n",
      "       [-0.5, -0.5, -0.5, ..., -0.5, -0.5, -0.5],\n",
      "       [-0.5, -0.5, -0.5, ..., -0.5, -0.5, -0.5],\n",
      "       ...,\n",
      "       [-0.5, -0.5, -0.5, ..., -0.5, -0.5, -0.5],\n",
      "       [-0.5, -0.5, -0.5, ..., -0.5, -0.5, -0.5],\n",
      "       [-0.5, -0.5, -0.5, ..., -0.5, -0.5, -0.5]], dtype=float32), 'pfcand_e_log_nopuppi': array([[-0.65, -0.65, -0.65, ..., -0.65, -0.65, -0.65],\n",
      "       [-0.65, -0.65, -0.65, ..., -0.65, -0.65, -0.65],\n",
      "       [-0.65, -0.65, -0.65, ..., -0.65, -0.65, -0.65],\n",
      "       ...,\n",
      "       [-0.65, -0.65, -0.65, ..., -0.65, -0.65, -0.65],\n",
      "       [-0.65, -0.65, -0.65, ..., -0.65, -0.65, -0.65],\n",
      "       [-0.65, -0.65, -0.65, ..., -0.65, -0.65, -0.65]], dtype=float32), 'pfcand_isEl': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_isMu': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_isChargedHad': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_isGamma': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_isNeutralHad': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_charge': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_VTX_ass': array([[-1.2, -1.2, -1.2, ..., -1.2, -1.2, -1.2],\n",
      "       [-1.2, -1.2, -1.2, ..., -1.2, -1.2, -1.2],\n",
      "       [-1.2, -1.2, -1.2, ..., -1.2, -1.2, -1.2],\n",
      "       ...,\n",
      "       [-1.2, -1.2, -1.2, ..., -1.2, -1.2, -1.2],\n",
      "       [-1.2, -1.2, -1.2, ..., -1.2, -1.2, -1.2],\n",
      "       [-1.2, -1.2, -1.2, ..., -1.2, -1.2, -1.2]], dtype=float32), 'pfcand_lostInnerHits': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_quality': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_normchi2': array([[-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       ...,\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.],\n",
      "       [-1., -1., -1., ..., -1., -1., -1.]], dtype=float32), 'pfcand_dz': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_dxy': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_dzsig': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_dxysig': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'pfcand_mask': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'sv_etarel': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'sv_phirel': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), 'sv_abseta': array([[-0.8, -0.8, -0.8, ..., -0.8, -0.8, -0.8],\n",
      "       [-0.8, -0.8, -0.8, ..., -0.8, -0.8, -0.8],\n",
      "       [-0.8, -0.8, -0.8, ..., -0.8, -0.8, -0.8],\n",
      "       ...,\n",
      "       [-0.8, -0.8, -0.8, ..., -0.8, -0.8, -0.8],\n",
      "       [-0.8, -0.8, -0.8, ..., -0.8, -0.8, -0.8],\n",
      "       [-0.8, -0.8, -0.8, ..., -0.8, -0.8, -0.8]], dtype=float32), 'sv_mass': array([[-0.36, -0.36, -0.36, ..., -0.36, -0.36, -0.36],\n",
      "       [-0.36, -0.36, -0.36, ..., -0.36, -0.36, -0.36],\n",
      "       [-0.36, -0.36, -0.36, ..., -0.36, -0.36, -0.36],\n",
      "       ...,\n",
      "       [-0.36, -0.36, -0.36, ..., -0.36, -0.36, -0.36],\n",
      "       [-0.36, -0.36, -0.36, ..., -0.36, -0.36, -0.36],\n",
      "       [-0.36, -0.36, -0.36, ..., -0.36, -0.36, -0.36]], dtype=float32), 'sv_pt_log': array([[-2.4, -2.4, -2.4, ..., -2.4, -2.4, -2.4],\n",
      "       [-2.4, -2.4, -2.4, ..., -2.4, -2.4, -2.4],\n",
      "       [-2.4, -2.4, -2.4, ..., -2.4, -2.4, -2.4],\n",
      "       ...,\n",
      "       [-2.4, -2.4, -2.4, ..., -2.4, -2.4, -2.4],\n",
      "       [-2.4, -2.4, -2.4, ..., -2.4, -2.4, -2.4],\n",
      "       [-2.4, -2.4, -2.4, ..., -2.4, -2.4, -2.4]], dtype=float32), 'sv_ntracks': array([[-3., -3., -3., ..., -3., -3., -3.],\n",
      "       [-3., -3., -3., ..., -3., -3., -3.],\n",
      "       [-3., -3., -3., ..., -3., -3., -3.],\n",
      "       ...,\n",
      "       [-3., -3., -3., ..., -3., -3., -3.],\n",
      "       [-3., -3., -3., ..., -3., -3., -3.],\n",
      "       [-3., -3., -3., ..., -3., -3., -3.]], dtype=float32), 'sv_normchi2': array([[-0.90000004, -0.90000004, -0.90000004, ..., -0.90000004,\n",
      "        -0.90000004, -0.90000004],\n",
      "       [-0.90000004, -0.90000004, -0.90000004, ..., -0.90000004,\n",
      "        -0.90000004, -0.90000004],\n",
      "       [-0.90000004, -0.90000004, -0.90000004, ..., -0.90000004,\n",
      "        -0.90000004, -0.90000004],\n",
      "       ...,\n",
      "       [-0.90000004, -0.90000004, -0.90000004, ..., -0.90000004,\n",
      "        -0.90000004, -0.90000004],\n",
      "       [-0.90000004, -0.90000004, -0.90000004, ..., -0.90000004,\n",
      "        -0.90000004, -0.90000004],\n",
      "       [-0.90000004, -0.90000004, -0.90000004, ..., -0.90000004,\n",
      "        -0.90000004, -0.90000004]], dtype=float32), 'sv_dxy': array([[-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1],\n",
      "       [-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1],\n",
      "       [-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1],\n",
      "       ...,\n",
      "       [-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1],\n",
      "       [-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1],\n",
      "       [-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1]], dtype=float32), 'sv_dxysig': array([[-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14],\n",
      "       [-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14],\n",
      "       [-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14],\n",
      "       ...,\n",
      "       [-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14],\n",
      "       [-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14],\n",
      "       [-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14]], dtype=float32), 'sv_d3d': array([[-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1],\n",
      "       [-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1],\n",
      "       [-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1],\n",
      "       ...,\n",
      "       [-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1],\n",
      "       [-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1],\n",
      "       [-0.1, -0.1, -0.1, ..., -0.1, -0.1, -0.1]], dtype=float32), 'sv_d3dsig': array([[-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14],\n",
      "       [-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14],\n",
      "       [-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14],\n",
      "       ...,\n",
      "       [-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14],\n",
      "       [-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14],\n",
      "       [-0.14, -0.14, -0.14, ..., -0.14, -0.14, -0.14]], dtype=float32), 'sv_costhetasvpv': <Array [None, None, None, ... None, None, None] type='200 * option[var * float32]'>, 'sv_mask': array([[0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       ...,\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.],\n",
      "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "print(feature_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import awkward as ak\n",
    "from coffea.nanoevents.methods.base import NanoEventsArray\n",
    "\n",
    "import json\n",
    "\n",
    "# import onnxruntime as ort\n",
    "\n",
    "import time\n",
    "\n",
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as triton_http\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# adapted from https://github.com/lgray/hgg-coffea/blob/triton-bdts/src/hgg_coffea/tools/chained_quantile.py\n",
    "class wrapped_triton:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_url: str,\n",
    "        batch_size: int,\n",
    "        torchscript: bool = True,\n",
    "    ) -> None:\n",
    "        fullprotocol, location = model_url.split(\"://\")\n",
    "        _, protocol = fullprotocol.split(\"+\")\n",
    "        address, model, version = location.split(\"/\")\n",
    "\n",
    "        self._protocol = protocol\n",
    "        self._address = address\n",
    "        self._model = model\n",
    "        self._version = version\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._torchscript = torchscript\n",
    "\n",
    "    def __call__(self, input_dict: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        if self._protocol == \"grpc\":\n",
    "            client = triton_grpc.InferenceServerClient(url=self._address, verbose=False)\n",
    "            triton_protocol = triton_grpc\n",
    "        elif self._protocol == \"http\":\n",
    "            client = triton_http.InferenceServerClient(\n",
    "                url=self._address,\n",
    "                verbose=False,\n",
    "                concurrency=12,\n",
    "            )\n",
    "            triton_protocol = triton_http\n",
    "        else:\n",
    "            raise ValueError(f\"{self._protocol} does not encode a valid protocol (grpc or http)\")\n",
    "\n",
    "        # manually split into batches for gpu inference\n",
    "        input_size = input_dict[list(input_dict.keys())[0]].shape[0]\n",
    "        print(f\"size of input = {input_size}\")\n",
    "\n",
    "        outs = [\n",
    "            self._do_inference(\n",
    "                {key: input_dict[key][batch: batch + self._batch_size] for key in input_dict},\n",
    "                triton_protocol,\n",
    "                client,\n",
    "            )\n",
    "            for batch in tqdm(\n",
    "                range(0, input_dict[list(input_dict.keys())[0]].shape[0], self._batch_size)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return np.concatenate(outs) if input_size > 0 else outs\n",
    "\n",
    "    def _do_inference(\n",
    "        self, input_dict: Dict[str, np.ndarray], triton_protocol, client\n",
    "    ) -> np.ndarray:\n",
    "        # Infer\n",
    "        inputs = []\n",
    "\n",
    "        for key in input_dict:\n",
    "            input = triton_protocol.InferInput(key, input_dict[key].shape, \"FP32\")\n",
    "            input.set_data_from_numpy(input_dict[key])\n",
    "            inputs.append(input)\n",
    "\n",
    "        out_name = \"softmax__0\" if self._torchscript else \"softmax\"\n",
    "\n",
    "        output = triton_protocol.InferRequestedOutput(out_name)\n",
    "\n",
    "        request = client.infer(\n",
    "            self._model,\n",
    "            model_version=self._version,\n",
    "            inputs=inputs,\n",
    "            outputs=[output],\n",
    "        )\n",
    "\n",
    "        return request.as_numpy(out_name)\n",
    "\n",
    "\n",
    "def runInferenceTriton(\n",
    "    tagger_resources_path: str, events: NanoEventsArray, fj_idx_lep\n",
    ") -> dict:\n",
    "    total_start = time.time()\n",
    "\n",
    "    with open(f\"{tagger_resources_path}/triton_config.json\") as f:\n",
    "        triton_config = json.load(f)\n",
    "\n",
    "    with open(f\"{tagger_resources_path}/{triton_config['model_name']}.json\") as f:\n",
    "        tagger_vars = json.load(f)\n",
    "\n",
    "    triton_model = wrapped_triton(\n",
    "        triton_config[\"model_url\"], triton_config[\"batch_size\"], torchscript=True\n",
    "    )\n",
    "\n",
    "    fatjet_label = \"FatJet\"\n",
    "    pfcands_label = \"FatJetPFCands\"\n",
    "    svs_label = \"FatJetSVs\"\n",
    "    jet_label = \"ak8\"\n",
    "\n",
    "    # prepare inputs for both fat jets\n",
    "    tagger_inputs = []\n",
    "\n",
    "    feature_dict = {\n",
    "        **get_pfcands_features(tagger_vars, events, fj_idx_lep, fatjet_label, pfcands_label),\n",
    "        **get_svs_features(tagger_vars, events, fj_idx_lep, fatjet_label, svs_label),\n",
    "    }\n",
    "\n",
    "    for input_name in tagger_vars[\"input_names\"]:\n",
    "        for key in tagger_vars[input_name][\"var_names\"]:\n",
    "            np.expand_dims(feature_dict[key], 1)\n",
    "\n",
    "    tagger_inputs = {\n",
    "        f\"{input_name}__{i}\": np.concatenate(\n",
    "            [\n",
    "                np.expand_dims(feature_dict[key], 1)\n",
    "                for key in tagger_vars[input_name][\"var_names\"]\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        for i, input_name in enumerate(tagger_vars[\"input_names\"])\n",
    "    }\n",
    "\n",
    "    # run inference for both fat jets\n",
    "    tagger_outputs = []\n",
    "    print(f\"Running inference for candidate Jet\")\n",
    "    start = time.time()\n",
    "    tagger_outputs = triton_model(tagger_inputs)\n",
    "    time_taken = time.time() - start\n",
    "    print(f\"Inference took {time_taken:.1f}s\")\n",
    "    return tagger_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference for candidate Jet\n",
      "size of input = 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 2.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tagger_resources_path = 'binder_tagger_resources'\n",
    "tagger_outputs = runInferenceTriton(\n",
    "    tagger_resources_path, events, fj_idx_lep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16640975,  0.16107106,  0.17442611,  0.159028  ,  0.16716573,\n",
       "         0.17189933],\n",
       "       [ 0.16640975,  0.16107106,  0.17442611,  0.159028  ,  0.16716573,\n",
       "         0.17189933],\n",
       "       [ 0.16640975,  0.16107106,  0.17442611,  0.159028  ,  0.16716573,\n",
       "         0.17189933],\n",
       "       ...,\n",
       "       [-0.00124054, -0.05535316, -0.23376551, -0.13689004, -0.1681944 ,\n",
       "        -0.20851247],\n",
       "       [-0.21039732, -0.12158038, -0.09274215, -0.06227493, -0.10275763,\n",
       "        -0.16624969],\n",
       "       [-0.20933329, -0.04159177, -0.14418834, -0.1099261 , -0.18894503,\n",
       "        -0.05684729]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagger_outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
