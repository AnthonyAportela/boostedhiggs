{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging inference \n",
    "\n",
    "Using latest version of coffea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/pyg-coffea/lib/python3.8/site-packages/setuptools/distutils_patch.py:25: UserWarning: Distutils was imported before Setuptools. This usage is discouraged and may exhibit undesirable behaviors or errors. Please use Setuptools' objects directly or at least import Setuptools first.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "import pyarrow as pa\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "from typing import List, Optional, Dict\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import importlib.resources\n",
    "\n",
    "from coffea import processor\n",
    "from coffea.nanoevents.methods import candidate, vector\n",
    "from coffea.analysis_tools import Weights, PackedSelection\n",
    "\n",
    "from coffea.nanoevents.methods.base import NanoEventsArray\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema, PFNanoAODSchema\n",
    "from coffea.nanoevents.methods import candidate, vector\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Found duplicate branch \")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", message=\"Missing cross-reference index \")\n",
    "np.seterr(invalid='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NanoAODschema\n",
    "\n",
    "We want to use PFNanoAODSchema since that load PFCands as a candidate particles, i.e. they have 4-vector properties.\n",
    "\n",
    "https://coffeateam.github.io/coffea/api/coffea.nanoevents.PFNanoAODSchema.html\n",
    "https://github.com/CoffeaTeam/coffea/blob/7dd4f863837a6319579f078c9e445c61d9106943/coffea/nanoevents/schemas/nanoaod.py#L282\n",
    "\n",
    "Additionally, PFNanoAODSchema loads SecondaryVertices as SecondaryVertex:\n",
    "https://github.com/CoffeaTeam/coffea/blob/7dd4f863837a6319579f078c9e445c61d9106943/coffea/nanoevents/schemas/nanoaod.py#L68\n",
    "https://github.com/CoffeaTeam/coffea/blob/f2a99631dcf95b46bd0225b242b3ba512a30a89a/coffea/nanoevents/methods/nanoaod.py#L388\n",
    "\n",
    "We do not neccessarily want this since this means we don't have a candidate 4-vector for the SecondaryVertex and we can't do operations like `delta_phi`. \n",
    "We need to modify this to a `mixin` of PFCand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PFCand\n",
      "SecondaryVertex\n",
      "PFCand\n"
     ]
    }
   ],
   "source": [
    "print(PFNanoAODSchema.mixins['PFCands'])\n",
    "print(PFNanoAODSchema.mixins['SV'])\n",
    "\n",
    "# interpret SV with PFCand behavior\n",
    "PFNanoAODSchema.mixins[\"SV\"] = \"PFCand\"\n",
    "\n",
    "print(PFNanoAODSchema.mixins['SV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening file w PFNanoAODSchema\n",
    "\n",
    "Here we manually open the file (only 200 entries), and we use `PFNanoAODSchema`.\n",
    "We also open the json file that specifies how many pf candidates/svs are going to be used in the tagger, as well as any normalization that should be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'dy_sample.root'\n",
    "events = NanoEventsFactory.from_root(file, schemaclass=PFNanoAODSchema, entry_stop=200).events()\n",
    "\n",
    "with open(f\"03_31_ak8.json\") as f:\n",
    "    tagger_vars = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the selection such that it mimics the boostedHiggs selection. We use the fatjet closest to the lepton and obtain its index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_p4(cand):\n",
    "    return ak.zip(\n",
    "        {\n",
    "            \"pt\": cand.pt,\n",
    "            \"eta\": cand.eta,\n",
    "            \"phi\": cand.phi,\n",
    "            \"mass\": cand.mass,\n",
    "            \"charge\": cand.charge,\n",
    "        },\n",
    "        with_name=\"PtEtaPhiMCandidate\",\n",
    "        behavior=candidate.behavior,\n",
    "    )\n",
    "\n",
    "good_muons = (\n",
    "    (events.Muon.pt > 30)\n",
    "    & (np.abs(events.Muon.eta) < 2.4)\n",
    "    & (np.abs(events.Muon.dz) < 0.1)\n",
    "    & (np.abs(events.Muon.dxy) < 0.05)\n",
    "    & (events.Muon.sip3d <= 4.0)\n",
    "    & events.Muon.mediumId\n",
    ")   \n",
    "good_electrons = (\n",
    "    (events.Electron.pt > 38)\n",
    "    & (np.abs(events.Electron.eta) < 2.4)\n",
    "    & ((np.abs(events.Electron.eta) < 1.44) | (np.abs(events.Electron.eta) > 1.57))\n",
    "    & (np.abs(events.Electron.dz) < 0.1)\n",
    "    & (np.abs(events.Electron.dxy) < 0.05)\n",
    "    & (events.Electron.sip3d <= 4.0)\n",
    "    & (events.Electron.mvaFall17V2noIso_WP90)\n",
    ")\n",
    "\n",
    "# get candidate lepton\n",
    "goodleptons = ak.concatenate([events.Muon[good_muons], events.Electron[good_electrons]], axis=1)    # concat muons and electrons\n",
    "goodleptons = goodleptons[ak.argsort(goodleptons.pt, ascending=False)]      # sort by pt\n",
    "candidatelep = ak.firsts(goodleptons)   # pick highest pt\n",
    "candidatelep_p4 = build_p4(candidatelep) \n",
    "\n",
    "# get candidate fj\n",
    "fatjets = events.FatJet\n",
    "good_fatjets = (\n",
    "    (fatjets.pt > 200)\n",
    "    & (abs(fatjets.eta) < 2.5)\n",
    "    & fatjets.isTight\n",
    ")\n",
    "n_fatjets = ak.sum(good_fatjets, axis=1)\n",
    "good_fatjets = fatjets[good_fatjets]        # select good fatjets\n",
    "good_fatjets = good_fatjets[ak.argsort(good_fatjets.pt, ascending=False)]    # sort by pt\n",
    "lep_in_fj_overlap_bool = ~ak.is_none(ak.firsts(good_fatjets.delta_r(candidatelep_p4) > 0.1))\n",
    "good_fatjets = ak.mask(good_fatjets, lep_in_fj_overlap_bool)\n",
    "\n",
    "# get idx and fj\n",
    "fj_idx_lep = ak.argmin(good_fatjets.delta_r(candidatelep_p4), axis=1, keepdims=True)\n",
    "candidatefj_lep = ak.firsts(good_fatjets[fj_idx_lep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the candidate jet is jet # [0]\n",
      "the candidate jet is jet # [1]\n"
     ]
    }
   ],
   "source": [
    "print(f'the candidate jet is jet # {fj_idx_lep[74]}')\n",
    "print(f'the candidate jet is jet # {fj_idx_lep[92]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indices where jet pt > 0\n",
      "(array([ 17,  48,  66,  74,  80,  81,  82,  90,  92, 104, 105, 116, 123,\n",
      "       159, 160, 164]),)\n"
     ]
    }
   ],
   "source": [
    "fatjet_label = \"FatJet\"\n",
    "pfcands_label = \"FatJetPFCands\"\n",
    "svs_label = \"FatJetSVs\"\n",
    "\n",
    "# get the jet given the index (take firsts to avoid singletons)\n",
    "jet = ak.firsts(events[fatjet_label][fj_idx_lep])\n",
    "\n",
    "# print indices of events where we actually have a jet\n",
    "print('Indices where jet pt > 0')\n",
    "print(np.where((ak.fill_none(jet.pt,-1)>0).to_numpy()))    # fill \"None\" events by -1, contsruct a bool and get the inidices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now get feature functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pfcands_features(\n",
    "    tagger_vars: dict,\n",
    "    preselected_events: NanoEventsArray,\n",
    "    fj_idx_lep,\n",
    "    fatjet_label: str = \"FatJetAK15\",\n",
    "    pfcands_label: str = \"FatJetPFCands\",\n",
    "    normalize: bool = True,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts the pf_candidate features specified in the ``tagger_vars`` dict from the\n",
    "    ``preselected_events`` and returns them as a dict of numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "    jet = ak.firsts(preselected_events[fatjet_label][fj_idx_lep])\n",
    "\n",
    "    msk = preselected_events[pfcands_label].jetIdx == ak.firsts(fj_idx_lep)\n",
    "    jet_ak_pfcands = preselected_events[pfcands_label][msk]\n",
    "    jet_pfcands = (preselected_events.PFCands[jet_ak_pfcands.pFCandsIdx])\n",
    "\n",
    "    # negative eta jets have -1 sign, positive eta jets have +1\n",
    "    eta_sign = ak.values_astype(jet_pfcands.eta > 0, int) * 2 - 1\n",
    "    feature_dict[\"pfcand_etarel\"] = eta_sign * (jet_pfcands.eta - jet.eta)\n",
    "    feature_dict[\"pfcand_phirel\"] = jet.delta_phi(jet_pfcands)\n",
    "    feature_dict[\"pfcand_abseta\"] = np.abs(jet_pfcands.eta)\n",
    "\n",
    "    feature_dict[\"pfcand_pt_log_nopuppi\"] = np.log(jet_pfcands.pt)\n",
    "    feature_dict[\"pfcand_e_log_nopuppi\"] = np.log(jet_pfcands.energy)\n",
    "\n",
    "    pdgIds = jet_pfcands.pdgId\n",
    "    feature_dict[\"pfcand_isEl\"] = np.abs(pdgIds) == 11\n",
    "    feature_dict[\"pfcand_isMu\"] = np.abs(pdgIds) == 13\n",
    "    feature_dict[\"pfcand_isChargedHad\"] = np.abs(pdgIds) == 211\n",
    "    feature_dict[\"pfcand_isGamma\"] = np.abs(pdgIds) == 22\n",
    "    feature_dict[\"pfcand_isNeutralHad\"] = np.abs(pdgIds) == 130\n",
    "\n",
    "    feature_dict[\"pfcand_charge\"] = jet_pfcands.charge\n",
    "    feature_dict[\"pfcand_VTX_ass\"] = jet_pfcands.pvAssocQuality\n",
    "    feature_dict[\"pfcand_lostInnerHits\"] = jet_pfcands.lostInnerHits\n",
    "    feature_dict[\"pfcand_quality\"] = jet_pfcands.trkQuality\n",
    "\n",
    "    feature_dict[\"pfcand_normchi2\"] = np.floor(jet_pfcands.trkChi2)\n",
    "\n",
    "    feature_dict[\"pfcand_dz\"] = jet_pfcands.dz\n",
    "    feature_dict[\"pfcand_dxy\"] = jet_pfcands.d0\n",
    "    feature_dict[\"pfcand_dzsig\"] = jet_pfcands.dz / jet_pfcands.dzErr\n",
    "    feature_dict[\"pfcand_dxysig\"] = jet_pfcands.d0 / jet_pfcands.d0Err\n",
    "\n",
    "    # btag vars\n",
    "    for var in tagger_vars[\"pf_features\"][\"var_names\"]:\n",
    "        if \"btag\" in var:\n",
    "            feature_dict[var] = jet_ak_pfcands[var[len(\"pfcand_\"):]]\n",
    "\n",
    "    # pfcand mask\n",
    "    feature_dict[\"pfcand_mask\"] = (~(ak.pad_none(feature_dict[\"pfcand_abseta\"], tagger_vars[\"pf_points\"][\"var_length\"], axis=1, clip=True).to_numpy().mask)).astype(np.float32)\n",
    "\n",
    "    # convert to numpy arrays and normalize features\n",
    "    for var in tagger_vars[\"pf_features\"][\"var_names\"]:\n",
    "        a = (\n",
    "            ak.pad_none(\n",
    "                feature_dict[var], tagger_vars[\"pf_points\"][\"var_length\"], axis=1, clip=True\n",
    "            )\n",
    "            .to_numpy()\n",
    "            .filled(fill_value=0)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        if normalize:\n",
    "            info = tagger_vars[\"pf_features\"][\"var_infos\"][var]\n",
    "            a = (a - info[\"median\"]) * info[\"norm_factor\"]\n",
    "            a = np.clip(a, info.get(\"lower_bound\", -5), info.get(\"upper_bound\", 5))\n",
    "\n",
    "        feature_dict[var] = a\n",
    "\n",
    "    if normalize:\n",
    "        var = \"pfcand_normchi2\"\n",
    "        info = tagger_vars[\"pf_features\"][\"var_infos\"][var]\n",
    "        # finding what -1 transforms to\n",
    "        chi2_min = -1 - info[\"median\"] * info[\"norm_factor\"]\n",
    "        feature_dict[var][feature_dict[var] == chi2_min] = info[\"upper_bound\"]\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svs_features(\n",
    "    tagger_vars: dict,\n",
    "    preselected_events: NanoEventsArray,\n",
    "    fj_idx_lep,\n",
    "    fatjet_label: str = \"FatJetAK15\",\n",
    "    svs_label: str = \"JetSVsAK15\",\n",
    "    normalize: bool = True,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts the sv features specified in the ``tagger_vars`` dict from the\n",
    "    ``preselected_events`` and returns them as a dict of numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "    jet = ak.firsts(preselected_events[fatjet_label][fj_idx_lep])\n",
    "    msk = preselected_events[svs_label].jetIdx == ak.firsts(fj_idx_lep)\n",
    "    jet_svs = preselected_events.SV[\n",
    "        preselected_events[svs_label].sVIdx[\n",
    "            (preselected_events[svs_label].sVIdx != -1)\n",
    "            * (msk)\n",
    "        ]\n",
    "    ]\n",
    "\n",
    "    # negative eta jets have -1 sign, positive eta jets have +1\n",
    "    eta_sign = ak.values_astype(jet_svs.eta > 0, int) * 2 - 1\n",
    "    feature_dict[\"sv_etarel\"] = eta_sign * (jet_svs.eta - jet.eta)\n",
    "    feature_dict[\"sv_phirel\"] = jet_svs.delta_phi(jet)\n",
    "    feature_dict[\"sv_abseta\"] = np.abs(jet_svs.eta)\n",
    "    feature_dict[\"sv_mass\"] = jet_svs.mass\n",
    "    feature_dict[\"sv_pt_log\"] = np.log(jet_svs.pt)\n",
    "\n",
    "    feature_dict[\"sv_ntracks\"] = jet_svs.ntracks\n",
    "    feature_dict[\"sv_normchi2\"] = jet_svs.chi2\n",
    "    feature_dict[\"sv_dxy\"] = jet_svs.dxy\n",
    "    feature_dict[\"sv_dxysig\"] = jet_svs.dxySig\n",
    "    feature_dict[\"sv_d3d\"] = jet_svs.dlen\n",
    "    feature_dict[\"sv_d3dsig\"] = jet_svs.dlenSig\n",
    "    svpAngle = jet_svs.pAngle\n",
    "    feature_dict[\"sv_costhetasvpv\"] = -np.cos(svpAngle)\n",
    "\n",
    "    feature_dict[\"sv_mask\"] = (~(ak.pad_none(feature_dict[\"sv_etarel\"], tagger_vars[\"sv_points\"][\"var_length\"], axis=1, clip=True).to_numpy().mask)).astype(np.float32)\n",
    "\n",
    "    # convert to numpy arrays and normalize features\n",
    "    for var in tagger_vars[\"sv_features\"][\"var_names\"]:\n",
    "        a = (\n",
    "            ak.pad_none(\n",
    "                feature_dict[var], tagger_vars[\"sv_points\"][\"var_length\"], axis=1, clip=True\n",
    "            )\n",
    "            .to_numpy()\n",
    "            .filled(fill_value=0)\n",
    "        ).astype(np.float32)\n",
    "\n",
    "        if normalize:\n",
    "            info = tagger_vars[\"sv_features\"][\"var_infos\"][var]\n",
    "            a = (a - info[\"median\"]) * info[\"norm_factor\"]\n",
    "            a = np.clip(a, info.get(\"lower_bound\", -5), info.get(\"upper_bound\", 5))\n",
    "\n",
    "        feature_dict[var] = a\n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "fatjet_label = \"FatJet\"\n",
    "pfcands_label = \"FatJetPFCands\"\n",
    "svs_label = \"FatJetSVs\"\n",
    "\n",
    "selection = candidatefj_lep.pt > 450\n",
    "\n",
    "feature_dict = {\n",
    "    **get_pfcands_features(tagger_vars, events[selection], fj_idx_lep[selection], fatjet_label, pfcands_label),\n",
    "    **get_svs_features(tagger_vars, events[selection], fj_idx_lep[selection], fatjet_label, svs_label)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['pfcand_etarel', 'pfcand_phirel', 'pfcand_abseta', 'pfcand_pt_log_nopuppi', 'pfcand_e_log_nopuppi', 'pfcand_isEl', 'pfcand_isMu', 'pfcand_isChargedHad', 'pfcand_isGamma', 'pfcand_isNeutralHad', 'pfcand_charge', 'pfcand_VTX_ass', 'pfcand_lostInnerHits', 'pfcand_quality', 'pfcand_normchi2', 'pfcand_dz', 'pfcand_dxy', 'pfcand_dzsig', 'pfcand_dxysig', 'pfcand_mask', 'sv_etarel', 'sv_phirel', 'sv_abseta', 'sv_mass', 'sv_pt_log', 'sv_ntracks', 'sv_normchi2', 'sv_dxy', 'sv_dxysig', 'sv_d3d', 'sv_d3dsig', 'sv_costhetasvpv', 'sv_mask'])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import softmax\n",
    "import awkward as ak\n",
    "from coffea.nanoevents.methods.base import NanoEventsArray\n",
    "\n",
    "import json\n",
    "\n",
    "# import onnxruntime as ort\n",
    "\n",
    "import time\n",
    "\n",
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as triton_http\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# adapted from https://github.com/lgray/hgg-coffea/blob/triton-bdts/src/hgg_coffea/tools/chained_quantile.py\n",
    "class wrapped_triton:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_url: str,\n",
    "        batch_size: int,\n",
    "        torchscript: bool = True,\n",
    "    ) -> None:\n",
    "        fullprotocol, location = model_url.split(\"://\")\n",
    "        _, protocol = fullprotocol.split(\"+\")\n",
    "        address, model, version = location.split(\"/\")\n",
    "\n",
    "        self._protocol = protocol\n",
    "        self._address = address\n",
    "        self._model = model\n",
    "        self._version = version\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._torchscript = torchscript\n",
    "\n",
    "    def __call__(self, input_dict: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        if self._protocol == \"grpc\":\n",
    "            client = triton_grpc.InferenceServerClient(url=self._address, verbose=False)\n",
    "            triton_protocol = triton_grpc\n",
    "        elif self._protocol == \"http\":\n",
    "            client = triton_http.InferenceServerClient(\n",
    "                url=self._address,\n",
    "                verbose=False,\n",
    "                concurrency=12,\n",
    "            )\n",
    "            triton_protocol = triton_http\n",
    "        else:\n",
    "            raise ValueError(f\"{self._protocol} does not encode a valid protocol (grpc or http)\")\n",
    "\n",
    "        # manually split into batches for gpu inference\n",
    "        input_size = input_dict[list(input_dict.keys())[0]].shape[0]\n",
    "        print(f\"size of input = {input_size}\")\n",
    "\n",
    "        outs = [\n",
    "            self._do_inference(\n",
    "                {key: input_dict[key][batch: batch + self._batch_size] for key in input_dict},\n",
    "                triton_protocol,\n",
    "                client,\n",
    "            )\n",
    "            for batch in tqdm(\n",
    "                range(0, input_dict[list(input_dict.keys())[0]].shape[0], self._batch_size)\n",
    "            )\n",
    "        ]\n",
    "\n",
    "        return np.concatenate(outs) if input_size > 0 else outs\n",
    "\n",
    "    def _do_inference(\n",
    "        self, input_dict: Dict[str, np.ndarray], triton_protocol, client\n",
    "    ) -> np.ndarray:\n",
    "        # Infer\n",
    "        inputs = []\n",
    "\n",
    "        for key in input_dict:\n",
    "            input = triton_protocol.InferInput(key, input_dict[key].shape, \"FP32\")\n",
    "            input.set_data_from_numpy(input_dict[key])\n",
    "            inputs.append(input)\n",
    "\n",
    "        out_name = \"softmax__0\" if self._torchscript else \"softmax\"\n",
    "\n",
    "        output = triton_protocol.InferRequestedOutput(out_name)\n",
    "\n",
    "        request = client.infer(\n",
    "            self._model,\n",
    "            model_version=self._version,\n",
    "            inputs=inputs,\n",
    "            outputs=[output],\n",
    "        )\n",
    "\n",
    "        return request.as_numpy(out_name)\n",
    "\n",
    "\n",
    "def runInferenceTriton(\n",
    "    tagger_resources_path: str, events: NanoEventsArray, fj_idx_lep\n",
    ") -> dict:\n",
    "    total_start = time.time()\n",
    "\n",
    "    with open(f\"{tagger_resources_path}/triton_config.json\") as f:\n",
    "        triton_config = json.load(f)\n",
    "\n",
    "    with open(f\"{tagger_resources_path}/{triton_config['model_name']}.json\") as f:\n",
    "        tagger_vars = json.load(f)\n",
    "\n",
    "    triton_model = wrapped_triton(\n",
    "        triton_config[\"model_url\"], triton_config[\"batch_size\"], torchscript=True\n",
    "    )\n",
    "\n",
    "    fatjet_label = \"FatJet\"\n",
    "    pfcands_label = \"FatJetPFCands\"\n",
    "    svs_label = \"FatJetSVs\"\n",
    "    jet_label = \"ak8\"\n",
    "\n",
    "    # prepare inputs for both fat jets\n",
    "    tagger_inputs = []\n",
    "\n",
    "    feature_dict = {\n",
    "        **get_pfcands_features(tagger_vars, events, fj_idx_lep, fatjet_label, pfcands_label),\n",
    "        **get_svs_features(tagger_vars, events, fj_idx_lep, fatjet_label, svs_label),\n",
    "    }\n",
    "\n",
    "    for input_name in tagger_vars[\"input_names\"]:\n",
    "        for key in tagger_vars[input_name][\"var_names\"]:\n",
    "            np.expand_dims(feature_dict[key], 1)\n",
    "\n",
    "    tagger_inputs = {\n",
    "        f\"{input_name}__{i}\": np.concatenate(\n",
    "            [\n",
    "                np.expand_dims(feature_dict[key], 1)\n",
    "                for key in tagger_vars[input_name][\"var_names\"]\n",
    "            ],\n",
    "            axis=1,\n",
    "        )\n",
    "        for i, input_name in enumerate(tagger_vars[\"input_names\"])\n",
    "    }\n",
    "    print(tagger_inputs['pf_points__0'].shape)\n",
    "    # run inference for both fat jets\n",
    "    tagger_outputs = []\n",
    "    print(f\"Running inference for candidate Jet\")\n",
    "    start = time.time()\n",
    "    tagger_outputs = triton_model(tagger_inputs)\n",
    "    time_taken = time.time() - start\n",
    "    print(f\"Inference took {time_taken:.1f}s\")\n",
    "\n",
    "    print('tagger_outputs shape', tagger_outputs.shape)\n",
    "    pnet_vars_list = []\n",
    "    if len(tagger_outputs):\n",
    "        pnet_vars = {\n",
    "            f\"{jet_label}FatJetParticleNetHWWMD_probQCD\": np.sum(tagger_outputs[:, :5], axis=1),\n",
    "            f\"{jet_label}FatJetParticleNetHWWMD_probHWW3q\": tagger_outputs[:, -2],\n",
    "            f\"{jet_label}FatJetParticleNetHWWMD_probHWW4q\": tagger_outputs[:, -1],\n",
    "            f\"{jet_label}FatJetParticleNetHWWMD_THWW4q\": (tagger_outputs[:, -2] + tagger_outputs[:, -1]) / np.sum(tagger_outputs, axis=1),\n",
    "        }\n",
    "    else:\n",
    "        pnet_vars = {\n",
    "            f\"{jet_label}FatJetParticleNetHWWMD_probQCD\": np.array([]),\n",
    "            f\"{jet_label}FatJetParticleNetHWWMD_probHWW3q\": np.array([]),\n",
    "            f\"{jet_label}FatJetParticleNetHWWMD_probHWW4q\": np.array([]),\n",
    "            f\"{jet_label}FatJetParticleNetHWWMD_THWW4q\": np.array([]),\n",
    "        }\n",
    "    \n",
    "    print(f\"Total time taken: {time.time() - total_start:.1f}s\")\n",
    "    return pnet_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 2, 100)\n",
      "Running inference for candidate Jet\n",
      "size of input = 200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:04<00:00,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference took 4.5s\n",
      "tagger_outputs shape (200, 6)\n",
      "Total time taken: 4.7s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tagger_resources_path = 'binder_tagger_resources'\n",
    "pfnet_vars = runInferenceTriton(\n",
    "    tagger_resources_path, events, fj_idx_lep\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['ak8FatJetParticleNetHWWMD_probQCD', 'ak8FatJetParticleNetHWWMD_probHWW3q', 'ak8FatJetParticleNetHWWMD_probHWW4q', 'ak8FatJetParticleNetHWWMD_THWW4q'])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfnet_vars.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.9994266 ,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.99979746,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.7367499 ,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.9055151 ,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.99704957,  0.99873286,  0.9994309 ,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.99997056,  0.82810056,  0.9049269 ,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.9654163 ,\n",
       "        0.9999026 ,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.9995822 ,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.7646489 ,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.82810056,\n",
       "        0.82810056,  0.82810056,  0.82810056,  0.82810056,  0.972575  ,\n",
       "        0.8512026 ,  0.82810056,  0.82810056,  0.82810056,  0.9879631 ,\n",
       "        8.842584  ,  3.4315207 , 15.417066  , 38.162437  ,  3.1410995 ,\n",
       "        1.2782662 ,  6.226357  ,  6.497283  ,  1.2703223 ,  0.8576882 ,\n",
       "        0.49726963,  1.4873798 ,  2.3600407 ,  4.1209393 ,  1.430755  ,\n",
       "        3.2001662 ,  2.8399596 , 39.803223  ,  5.0531282 ,  3.4982035 ,\n",
       "        2.289204  ,  3.9711423 ,  2.8437886 ,  6.0781956 ,  2.155056  ,\n",
       "        1.7639475 , 15.201671  ,  3.9512105 ,  3.287754  ,  2.9996367 ,\n",
       "        0.46786088,  2.591516  ,  1.490477  ,  1.6507082 , 33.495274  ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pfnet_vars['ak8FatJetParticleNetHWWMD_probQCD']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import awkward as ak\n",
    "import gzip\n",
    "import pickle\n",
    "import importlib.resources\n",
    "import correctionlib\n",
    "\n",
    "with importlib.resources.path(\"binder_data\", \"ULvjets_corrections.json\") as filename:\n",
    "    vjets_kfactors = correctionlib.CorrectionSet.from_file(str(filename))\n",
    "\n",
    "def add_VJets_kFactors(weights, genpart, dataset):\n",
    "    \"\"\"Revised version of add_VJets_NLOkFactor, for both NLO EW and ~NNLO QCD\"\"\"\n",
    "    def get_vpt(check_offshell=False):\n",
    "        \"\"\"Only the leptonic samples have no resonance in the decay tree, and only\n",
    "        when M is beyond the configured Breit-Wigner cutoff (usually 15*width)\n",
    "        \"\"\"\n",
    "        boson = ak.firsts(genpart[\n",
    "            ((genpart.pdgId == 23)|(abs(genpart.pdgId) == 24))\n",
    "            & genpart.hasFlags([\"fromHardProcess\", \"isLastCopy\"])\n",
    "        ])\n",
    "        if check_offshell:\n",
    "            offshell = genpart[\n",
    "                genpart.hasFlags([\"fromHardProcess\", \"isLastCopy\"])\n",
    "                & ak.is_none(boson)\n",
    "                & (abs(genpart.pdgId) >= 11) & (abs(genpart.pdgId) <= 16)\n",
    "            ].sum()\n",
    "            return ak.where(ak.is_none(boson.pt), offshell.pt, boson.pt)\n",
    "        return np.array(ak.fill_none(boson.pt, 0.))\n",
    "\n",
    "    common_systs = [\n",
    "        \"d1K_NLO\",\n",
    "        \"d2K_NLO\",\n",
    "        \"d3K_NLO\",\n",
    "        \"d1kappa_EW\",\n",
    "    ]\n",
    "    zsysts = common_systs + [\n",
    "        \"Z_d2kappa_EW\",\n",
    "        \"Z_d3kappa_EW\",\n",
    "    ]\n",
    "    wsysts = common_systs + [\n",
    "        \"W_d2kappa_EW\",\n",
    "        \"W_d3kappa_EW\",\n",
    "    ]\n",
    "\n",
    "    if \"ZJetsToQQ_HT\" in dataset or \"DYJetsToLL\" in dataset:\n",
    "        vpt = get_vpt()\n",
    "        qcdcorr = vjets_kfactors[\"ULZ_MLMtoFXFX\"].evaluate(vpt)\n",
    "        ewkcorr = vjets_kfactors[\"Z_FixedOrderComponent\"]\n",
    "    elif \"WJetsToQQ_HT\" in dataset or \"WJetsToLNu\" in dataset:\n",
    "        vpt = get_vpt()\n",
    "        qcdcorr = vjets_kfactors[\"ULW_MLMtoFXFX\"].evaluate(vpt)\n",
    "        ewkcorr = vjets_kfactors[\"W_FixedOrderComponent\"]\n",
    "\n",
    "with importlib.resources.path(\"binder_data\", \"fatjet_triggerSF_Hbb.json\") as filename:\n",
    "    jet_triggerSF = correctionlib.CorrectionSet.from_file(str(filename))\n",
    "\n",
    "def add_jetTriggerSF(weights, leadingjet, year, selection):\n",
    "    def mask(w):\n",
    "        return np.where(selection.all('oneFatjet'), w, 1.)\n",
    "    jet_pt = np.array(ak.fill_none(leadingjet.pt, 0.))\n",
    "    jet_msd = np.array(ak.fill_none(leadingjet.msoftdrop, 0.))  # note: uncorrected\n",
    "    nom = mask(jet_triggerSF[f'fatjet_triggerSF{year}'].evaluate(\"nominal\", jet_pt, jet_msd))\n",
    "    up = mask(jet_triggerSF[f'fatjet_triggerSF{year}'].evaluate(\"stat_up\", jet_pt, jet_msd))\n",
    "    down = mask(jet_triggerSF[f'fatjet_triggerSF{year}'].evaluate(\"stat_dn\", jet_pt, jet_msd))\n",
    "    weights.add('trigger_had', nom, up, down)\n",
    "\n",
    "def add_pdf_weight(weights, pdf_weights):\n",
    "    nweights = len(weights.weight())\n",
    "    nom = np.ones(nweights)\n",
    "    up = np.ones(nweights)\n",
    "    down = np.ones(nweights)\n",
    "    docstring = pdf_weights.__doc__\n",
    "\n",
    "    # NNPDF31_nnlo_hessian_pdfas\n",
    "    # https://lhapdfsets.web.cern.ch/current/NNPDF31_nnlo_hessian_pdfas/NNPDF31_nnlo_hessian_pdfas.info\n",
    "    if True:\n",
    "        # Hessian PDF weights\n",
    "        # Eq. 21 of https://arxiv.org/pdf/1510.03865v1.pdf\n",
    "        arg = pdf_weights[:, 1:-2] - np.ones((nweights, 100))\n",
    "        summed = ak.sum(np.square(arg), axis=1)\n",
    "        pdf_unc = np.sqrt((1. / 99.) * summed)\n",
    "        weights.add('PDF_weight', nom, pdf_unc + nom)\n",
    "\n",
    "        # alpha_S weights\n",
    "        # Eq. 27 of same ref\n",
    "        as_unc = 0.5 * (pdf_weights[:, 102] - pdf_weights[:, 101])\n",
    "        weights.add('aS_weight', nom, as_unc + nom)\n",
    "\n",
    "        # PDF + alpha_S weights\n",
    "        # Eq. 28 of same ref\n",
    "        pdfas_unc = np.sqrt(np.square(pdf_unc) + np.square(as_unc))\n",
    "        weights.add('PDFaS_weight', nom, pdfas_unc + nom)\n",
    "\n",
    "    else:\n",
    "        weights.add('aS_weight', nom, up, down)\n",
    "        weights.add('PDF_weight', nom, up, down)\n",
    "        weights.add('PDFaS_weight', nom, up, down)\n",
    "\n",
    "# 7-point scale variations\n",
    "def add_scalevar_7pt(weights,var_weights):\n",
    "    docstring = var_weights.__doc__\n",
    "    nweights = len(weights.weight())\n",
    "\n",
    "    nom   = np.ones(nweights)\n",
    "    up    = np.ones(nweights)\n",
    "    down  = np.ones(nweights)\n",
    " \n",
    "    if len(var_weights) > 0:\n",
    "        if len(var_weights[0]) == 9: \n",
    "            up = np.maximum.reduce([var_weights[:,0],var_weights[:,1],var_weights[:,3],var_weights[:,5],var_weights[:,7],var_weights[:,8]])\n",
    "            down = np.minimum.reduce([var_weights[:,0],var_weights[:,1],var_weights[:,3],var_weights[:,5],var_weights[:,7],var_weights[:,8]])\n",
    "        elif len(var_weights[0]) > 1:\n",
    "            print(\"Scale variation vector has length \", len(var_weights[0]))\n",
    "    weights.add('scalevar_7pt', nom, up, down)\n",
    "\n",
    "# 3-point scale variations\n",
    "def add_scalevar_3pt(weights,var_weights):\n",
    "    docstring = var_weights.__doc__\n",
    "    \n",
    "    nweights = len(weights.weight())\n",
    "\n",
    "    nom   = np.ones(nweights)\n",
    "    up    = np.ones(nweights)\n",
    "    down  = np.ones(nweights)\n",
    "\n",
    "    if len(var_weights) > 0:\n",
    "        if len(var_weights[0]) == 9:\n",
    "            up = np.maximum(var_weights[:,0], var_weights[:,8])\n",
    "            down = np.minimum(var_weights[:,0], var_weights[:,8])\n",
    "        elif len(var_weights[0]) > 1:\n",
    "            print(\"Scale variation vector has length \", len(var_weights[0]))\n",
    "\n",
    "    weights.add('scalevar_3pt', nom, up, down)\n",
    "\n",
    "def add_ps_weight(weights, ps_weights):\n",
    "    nweights = len(weights.weight())\n",
    "    nom = np.ones(nweights)\n",
    "    up_isr = np.ones(nweights)\n",
    "    down_isr = np.ones(nweights)\n",
    "    up_fsr = np.ones(nweights)\n",
    "    down_fsr = np.ones(nweights)\n",
    "\n",
    "    if ps_weights is not None:\n",
    "        if len(ps_weights[0]) == 4:\n",
    "            up_isr = ps_weights[:, 0]\n",
    "            down_isr = ps_weights[:, 2]\n",
    "            up_fsr = ps_weights[:, 1]\n",
    "            down_fsr = ps_weights[:, 3]\n",
    "        else:\n",
    "            warnings.warn(f\"PS weight vector has length {len(ps_weights[0])}\")\n",
    "    weights.add('UEPS_ISR', nom, up_isr, down_isr)\n",
    "    weights.add('UEPS_FSR', nom, up_fsr, down_fsr)\n",
    "\n",
    "def build_lumimask(filename):\n",
    "    from coffea.lumi_tools import LumiMask\n",
    "    with importlib.resources.path(\"binder_data\", filename) as path:\n",
    "        return LumiMask(path)\n",
    "lumi_masks = {\n",
    "    \"2016\": build_lumimask(\"Cert_271036-284044_13TeV_Legacy2016_Collisions16_JSON.txt\"),\n",
    "    \"2017\": build_lumimask(\"Cert_294927-306462_13TeV_UL2017_Collisions17_GoldenJSON.txt\"),\n",
    "    \"2018\": build_lumimask(\"Cert_314472-325175_13TeV_Legacy2018_Collisions18_JSON.txt\"),\n",
    "}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "CorrectionLib files are available from: /cvmfs/cms.cern.ch/rsync/cms-nanoAOD/jsonpog-integration - synced daily\n",
    "\"\"\"\n",
    "pog_correction_path = \"/cvmfs/cms.cern.ch/rsync/cms-nanoAOD/jsonpog-integration/\"\n",
    "pog_jsons = {\n",
    "    \"muon\": [\"MUO\", \"muon_Z.json.gz\"],\n",
    "    \"electron\": [\"EGM\", \"electron.json.gz\"],\n",
    "    \"pileup\": [\"LUM\", \"puWeights.json.gz\"],\n",
    "}\n",
    "\n",
    "\n",
    "def get_UL_year(year):\n",
    "    if year == \"2016\":\n",
    "        year = \"2016postVFP\"\n",
    "    elif year == \"2016APV\":\n",
    "        year = \"2016preVFP\"\n",
    "    return f\"{year}_UL\"\n",
    "\n",
    "\n",
    "def get_pog_json(obj, year):\n",
    "    try:\n",
    "        pog_json = pog_jsons[obj]\n",
    "    except:\n",
    "        print(f'No json for {obj}')\n",
    "    year = get_UL_year(year)\n",
    "    return f\"{pog_correction_path}POG/{pog_json[0]}/{year}/{pog_json[1]}\"\n",
    "    # os.system(f\"cp {pog_correction_path}POG/{pog_json[0]}/{year}/{pog_json[1]} boostedhiggs/data/POG_{pog_json[0]}_{year}_{pog_json[1]}\")\n",
    "    # fname = \"\"\n",
    "    # with importlib.resources.path(\"boostedhiggs.data\", f\"POG_{pog_json[0]}_{year}_{pog_json[1]}\") as filename:\n",
    "    #     fname = str(filename)\n",
    "    # print(fname)\n",
    "    # return fname\n",
    "\n",
    "\"\"\"\n",
    "Lepton Scale Factors\n",
    "----\n",
    "\n",
    "Muons:\n",
    "https://twiki.cern.ch/twiki/bin/view/CMS/MuonUL2016\n",
    "https://twiki.cern.ch/twiki/bin/view/CMS/MuonUL2017\n",
    "https://twiki.cern.ch/twiki/bin/view/CMS/MuonUL2018\n",
    "\n",
    "- UL CorrectionLib html files:\n",
    "  https://cms-nanoaod-integration.web.cern.ch/commonJSONSFs/MUO_muon_Z_Run2_UL/\n",
    "  e.g. one example of the correction json files can be found here:\n",
    "  https://gitlab.cern.ch/cms-muonPOG/muonefficiencies/-/raw/master/Run2/UL/2017/2017_trigger/Efficiencies_muon_generalTracks_Z_Run2017_UL_SingleMuonTriggers_schemaV2.json\n",
    "  - Trigger iso and non-iso\n",
    "  - Isolation: We use RelIso<0.25 (LooseRelIso) with medium prompt ID\n",
    "  - Reconstruction ID: We use medium prompt ID\n",
    "\n",
    "Electrons:\n",
    "- UL CorrectionLib htmlfiles:\n",
    "  https://cms-nanoaod-integration.web.cern.ch/commonJSONSFs/EGM_electron_Run2_UL/\n",
    "  - ID: wp90noiso\n",
    "  - Reconstruction: RecoAbove20\n",
    "  - Trigger:\n",
    "    Looks like the EGM group does not provide them but applying these for now.\n",
    "    https://twiki.cern.ch/twiki/bin/viewauth/CMS/EgHLTScaleFactorMeasurements (derived by Siqi Yuan)\n",
    "    These include 2017: (HLT_ELE35 OR HLT_ELE115 OR HLT_Photon200)\n",
    "    and 2018: (HLT_Ele32 OR HLT_ELE115 OR HLT_Photon200)\n",
    "  - Isolation:\n",
    "    No SFs for RelIso?\n",
    "\"\"\"\n",
    "\n",
    "lepton_corrections = {\n",
    "    \"trigger_iso\": {\n",
    "        \"muon\": {  # For IsoMu24 (| IsoTkMu24 )\n",
    "            \"2016APV\": \"NUM_IsoMu24_or_IsoTkMu24_DEN_CutBasedIdTight_and_PFIsoTight\",  # preVBP\n",
    "            \"2016\": \"NUM_IsoMu24_or_IsoTkMu24_DEN_CutBasedIdTight_and_PFIsoTight\",  # postVBF\n",
    "            \"2017\": \"NUM_IsoMu27_DEN_CutBasedIdTight_and_PFIsoTight\",\n",
    "            \"2018\": \"NUM_IsoMu24_DEN_CutBasedIdTight_and_PFIsoTight\",\n",
    "        },\n",
    "    },\n",
    "    \"trigger_noniso\": {\n",
    "        \"muon\": {  # For Mu50 (| TkMu50 )\n",
    "            \"2016APV\": \"NUM_Mu50_or_TkMu50_DEN_CutBasedIdGlobalHighPt_and_TkIsoLoose\",\n",
    "            \"2016\": \"NUM_Mu50_or_TkMu50_DEN_CutBasedIdGlobalHighPt_and_TkIsoLoose\",\n",
    "            \"2017\": \"NUM_Mu50_or_OldMu100_or_TkMu100_DEN_CutBasedIdGlobalHighPt_and_TkIsoLoose\",\n",
    "            \"2018\": \"NUM_Mu50_or_OldMu100_or_TkMu100_DEN_CutBasedIdGlobalHighPt_and_TkIsoLoose\",\n",
    "        },\n",
    "    },\n",
    "    \"isolation\": {\n",
    "        \"muon\": {\n",
    "            \"2016APV\": \"NUM_LooseRelIso_DEN_MediumPromptID\",\n",
    "            \"2016\": \"NUM_LooseRelIso_DEN_MediumPromptID\",\n",
    "            \"2017\": \"NUM_LooseRelIso_DEN_MediumPromptID\",\n",
    "            \"2018\": \"NUM_LooseRelIso_DEN_MediumPromptID\",\n",
    "        },\n",
    "        # \"electron\": {\n",
    "        # },\n",
    "    },\n",
    "    # NOTE: We do not have SFs for mini-isolation yet\n",
    "    \"id\": {\n",
    "        \"muon\": {\n",
    "            \"2016APV\": \"NUM_MediumPromptID_DEN_TrackerMuons\",\n",
    "            \"2016\": \"NUM_MediumPromptID_DEN_TrackerMuons\",\n",
    "            \"2017\": \"NUM_MediumPromptID_DEN_TrackerMuons\",\n",
    "            \"2018\": \"NUM_MediumPromptID_DEN_TrackerMuons\",\n",
    "        },\n",
    "        # NOTE: should check that we do not have electrons w pT>500 GeV (I do not think we do)\n",
    "        \"electron\": {\n",
    "            \"2016APV\": \"wp90noiso\",\n",
    "            \"2016\": \"wp90noiso\",\n",
    "            \"2017\": \"wp90noiso\",\n",
    "            \"2018\": \"wp90noiso\",\n",
    "        },\n",
    "    },\n",
    "    \"reco\": {\n",
    "        \"electron\": {\n",
    "            \"2016APV\": \"RecoAbove20\",\n",
    "            \"2016\": \"RecoAbove20\",\n",
    "            \"2017\": \"RecoAbove20\",\n",
    "            \"2018\": \"RecoAbove20\",\n",
    "        },\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "def add_lepton_weight(weights, lepton, year, lepton_type=\"muon\"):\n",
    "    ul_year = get_UL_year(year)\n",
    "    if lepton_type == \"electron\":\n",
    "        ul_year = ul_year.replace('_UL', '')\n",
    "    cset = correctionlib.CorrectionSet.from_file(get_pog_json(lepton_type, year))\n",
    "\n",
    "    def set_isothreshold(corr, value, lepton_pt, lepton_type):\n",
    "        iso_threshold = {\n",
    "            \"muon\": 55.,\n",
    "            \"electron\": 120.,\n",
    "        }[lepton_type]\n",
    "        if corr == \"trigger_iso\":\n",
    "            value[lepton_pt > iso_threshold] = 1.\n",
    "        elif corr == \"trigger_noniso\":\n",
    "            value[lepton_pt < iso_threshold] = 1.\n",
    "        elif corr == \"isolation\":\n",
    "            value[lepton_pt > iso_threshold] = 1.\n",
    "        return value\n",
    "\n",
    "    def get_clip(lep_pt, lep_eta, lepton_type,corr=None):\n",
    "        clip_pt = [0., 2000]\n",
    "        clip_eta = [-2.4999, 2.4999]\n",
    "        if lepton_type == \"electron\":\n",
    "            clip_pt = [10.0, 499.999]\n",
    "            if corr == \"reco\":\n",
    "                clip_pt = [20.1, 499.999]\n",
    "        elif lepton_type == \"muon\":\n",
    "            clip_pt = [30., 1000.]\n",
    "            clip_eta = [0., 2.3999]\n",
    "            if corr == \"trigger_noniso\":\n",
    "                clip_pt = [52., 1000.]\n",
    "        lepton_pt = np.clip(lep_pt, clip_pt[0], clip_pt[1])\n",
    "        lepton_eta = np.clip(lep_eta, clip_eta[0], clip_eta[1])\n",
    "        return lepton_pt,lepton_eta\n",
    "\n",
    "    lep_pt = np.array(ak.fill_none(lepton.pt, 0.))\n",
    "    lep_eta = np.array(ak.fill_none(lepton.eta, 0.))\n",
    "    if lepton_type==\"muon\": lep_eta = np.abs(lep_eta)\n",
    "\n",
    "    for corr,corrDict in lepton_corrections.items():\n",
    "        if lepton_type not in corrDict.keys():\n",
    "            continue\n",
    "        if year not in corrDict[lepton_type].keys():\n",
    "            continue\n",
    "        json_map_name = corrDict[lepton_type][year]\n",
    "\n",
    "        lepton_pt,lepton_eta = get_clip(lep_pt, lep_eta, lepton_type, corr)\n",
    "\n",
    "        values = {}\n",
    "        if lepton_type == \"muon\":\n",
    "            values[\"nominal\"] = cset[json_map_name].evaluate(ul_year, lepton_eta, lepton_pt, \"sf\")\n",
    "        else:\n",
    "            values[\"nominal\"] = cset[\"UL-Electron-ID-SF\"].evaluate(ul_year, \"sf\", json_map_name, lepton_eta, lepton_pt)\n",
    "\n",
    "        if lepton_type == \"muon\":\n",
    "            values[\"up\"] = cset[json_map_name].evaluate(ul_year, lepton_eta, lepton_pt, \"systup\")\n",
    "            values[\"down\"] = cset[json_map_name].evaluate(ul_year, lepton_eta, lepton_pt, \"systdown\")\n",
    "        else:\n",
    "            values[\"up\"] = cset[\"UL-Electron-ID-SF\"].evaluate(ul_year, \"sfup\", json_map_name, lepton_eta, lepton_pt)\n",
    "            values[\"down\"] = cset[\"UL-Electron-ID-SF\"].evaluate(ul_year, \"sfdown\", json_map_name, lepton_eta, lepton_pt)\n",
    "\n",
    "        for key, val in values.items():\n",
    "            # restrict values to 1 for some SFs if we are above/below the ISO threshold\n",
    "            values[key] = set_isothreshold(corr, val, np.array(ak.fill_none(lepton.pt, 0.)), lepton_type)\n",
    "\n",
    "        # add weights (for now only the nominal weight)\n",
    "        weights.add(f\"{corr}_{lepton_type}\", values[\"nominal\"], values[\"up\"], values[\"down\"])\n",
    "        \n",
    "    # quick hack to add electron trigger SFs\n",
    "    if lepton_type==\"electron\":\n",
    "        corr=\"trigger\"\n",
    "        with importlib.resources.path(\"binder_data\", f\"electron_trigger_{ul_year}_UL.json\") as filename:\n",
    "            cset = correctionlib.CorrectionSet.from_file(str(filename))\n",
    "            lepton_pt,lepton_eta = get_clip(lep_pt, lep_eta, lepton_type, corr)\n",
    "            # stil need to add uncertanties..\n",
    "            values[\"nominal\"] = cset[\"UL-Electron-Trigger-SF\"].evaluate( lepton_eta, lepton_pt )   \n",
    "            #print(values[\"nominal\"][lep_pt>30])\n",
    "            weights.add(f\"{corr}_{lepton_type}\", values[\"nominal\"])\n",
    "\n",
    "\n",
    "def add_pileup_weight(weights, year, mod, nPU):\n",
    "    \"\"\"\n",
    "    Should be able to do something similar to lepton weight but w pileup\n",
    "    e.g. see here: https://cms-nanoaod-integration.web.cern.ch/commonJSONSFs/LUMI_puWeights_Run2_UL/\n",
    "    \"\"\"\n",
    "    cset = correctionlib.CorrectionSet.from_file(get_pog_json(\"pileup\", year + mod))\n",
    "\n",
    "    year_to_corr = {'2016': 'Collisions16_UltraLegacy_goldenJSON',\n",
    "                    '2017': 'Collisions17_UltraLegacy_goldenJSON',\n",
    "                    '2018': 'Collisions18_UltraLegacy_goldenJSON',\n",
    "                    }\n",
    "\n",
    "    values = {}\n",
    "\n",
    "    values[\"nominal\"] = cset[year_to_corr[year]].evaluate(nPU, \"nominal\")\n",
    "    values[\"up\"] = cset[year_to_corr[year]].evaluate(nPU, \"up\")\n",
    "    values[\"down\"] = cset[year_to_corr[year]].evaluate(nPU, \"down\")\n",
    "\n",
    "    # add weights (for now only the nominal weight)\n",
    "    weights.add(\"pileup\", values[\"nominal\"], values[\"up\"], values[\"down\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from coffea.analysis_tools import PackedSelection\n",
    "import awkward as ak\n",
    "\n",
    "\n",
    "def getParticles(genparticles, lowid=22, highid=25, flags=['fromHardProcess', 'isLastCopy']):\n",
    "    \"\"\"\n",
    "    returns the particle objects that satisfy a low id,\n",
    "    high id condition and have certain flags\n",
    "    \"\"\"\n",
    "    absid = abs(genparticles.pdgId)\n",
    "    return genparticles[\n",
    "        ((absid >= lowid) & (absid <= highid))\n",
    "        & genparticles.hasFlags(flags)\n",
    "    ]\n",
    "\n",
    "\n",
    "def match_HWW(genparticles, candidatefj):\n",
    "    \"\"\"\n",
    "    return the number of matched objects (hWW*),daughters,\n",
    "    and gen flavor (enuqq, munuqq, taunuqq)\n",
    "    \"\"\"\n",
    "    higgs = getParticles(genparticles, 25)   # genparticles is the full set... this function selects Higgs particles\n",
    "    is_hWW = ak.all(abs(higgs.children.pdgId) == 24, axis=2)    # W~24 so we get H->WW (limitation: only picking one W and assumes the other will be there)\n",
    "\n",
    "    higgs = higgs[is_hWW]\n",
    "    higgs_wstar = higgs.children[ak.argmin(higgs.children.mass, axis=2, keepdims=True)]\n",
    "    higgs_w = higgs.children[ak.argmax(higgs.children.mass, axis=2, keepdims=True)]\n",
    "\n",
    "    prompt_electron = getParticles(genparticles, 11, 11, ['isPrompt', 'isLastCopy'])    # isPrompt avoids displaced leptons\n",
    "    prompt_muon = getParticles(genparticles, 13, 13, ['isPrompt', 'isLastCopy'])\n",
    "    prompt_tau = getParticles(genparticles, 15, 15, ['isPrompt', 'isLastCopy'])\n",
    "    prompt_q = getParticles(genparticles, 0, 5, ['fromHardProcess', 'isLastCopy'])      # 0-5 not 0-6 to avoid top quark\n",
    "    prompt_q = prompt_q[abs(prompt_q.distinctParent.pdgId) == 24]       # parent W\n",
    "\n",
    "    dr_fj_quarks = candidatefj.delta_r(prompt_q)\n",
    "    dr_fj_electrons = candidatefj.delta_r(prompt_electron)\n",
    "    dr_fj_muons = candidatefj.delta_r(prompt_muon)\n",
    "    dr_fj_taus = candidatefj.delta_r(prompt_tau)\n",
    "    dr_daughters = ak.concatenate([dr_fj_quarks, dr_fj_electrons, dr_fj_muons, dr_fj_taus], axis=1)\n",
    "    hWW_nprongs = ak.sum(dr_daughters < 0.8, axis=1)   # impose that something must be inside the cone... tells you # of particles from Higgs matched to the jet\n",
    "\n",
    "    n_electrons = ak.sum(prompt_electron.pt > 0, axis=1)\n",
    "    n_muons = ak.sum(prompt_muon.pt > 0, axis=1)\n",
    "    n_taus = ak.sum(prompt_tau.pt > 0, axis=1)\n",
    "    n_quarks = ak.sum(prompt_q.pt > 0, axis=1)\n",
    "\n",
    "    # 4(elenuqq),6(munuqq),8(taunuqq)\n",
    "    hWW_flavor = (n_quarks == 2) * 1 + (n_electrons == 1) * 3 + (n_muons == 1) * 5 + (n_taus == 1) * 7 + (n_quarks == 4) * 11\n",
    "\n",
    "    matchedH = candidatefj.nearest(higgs, axis=1, threshold=0.8)    # choose higgs closest to fj\n",
    "    matchedW = candidatefj.nearest(higgs_w, axis=1, threshold=0.8)  # choose W closest to fj\n",
    "    matchedWstar = candidatefj.nearest(higgs_wstar, axis=1, threshold=0.8)  # choose Wstar closest to fj\n",
    "\n",
    "    # 1 (H only), 4(W), 6(W star), 9(H, W and Wstar)\n",
    "    hWW_matched = (\n",
    "        (ak.sum(matchedH.pt > 0, axis=1) == 1) * 1\n",
    "        + (ak.sum(ak.flatten(matchedW.pt > 0, axis=2), axis=1) == 1) * 3\n",
    "        + (ak.sum(ak.flatten(matchedWstar.pt > 0, axis=2), axis=1) == 1) * 5\n",
    "    )\n",
    "\n",
    "    # leptons matched\n",
    "    dr_fj_leptons = ak.concatenate([dr_fj_electrons, dr_fj_muons], axis=1)\n",
    "\n",
    "    leptons = ak.concatenate([prompt_electron, prompt_muon], axis=1)\n",
    "    leptons = leptons[dr_fj_leptons < 0.8]\n",
    "\n",
    "    # leptons coming from W or W*\n",
    "    leptons_mass = ak.firsts(leptons.distinctParent.mass)   # # TODO: why need firsts\n",
    "    higgs_w_mass = ak.firsts(ak.flatten(higgs_w.mass))[ak.firsts(leptons.pt > 0)]\n",
    "    higgs_wstar_mass = ak.firsts(ak.flatten(higgs_wstar.mass))[ak.firsts(leptons.pt > 0)]\n",
    "\n",
    "    iswlepton = (leptons_mass == higgs_w_mass)\n",
    "    iswstarlepton = (leptons_mass == higgs_wstar_mass)\n",
    "\n",
    "    ret = {\"hWW_flavor\": hWW_flavor,\n",
    "           \"hWW_matched\": hWW_matched,\n",
    "           \"hWW_nprongs\": hWW_nprongs,\n",
    "           \"matchedH\": matchedH,\n",
    "           \"iswlepton\": iswlepton,  # truth info, higher mass is normally onshell\n",
    "           \"iswstarlepton\": iswstarlepton}  # truth info, lower mass is normally offshell\n",
    "\n",
    "    return ret\n",
    "\n",
    "\n",
    "def match_Htt(genparticles, candidatefj):\n",
    "    higgs = getParticles(genparticles, 25)\n",
    "    is_htt = ak.all(abs(higgs.children.pdgId) == 15, axis=2)\n",
    "\n",
    "    higgs = higgs[is_htt]\n",
    "\n",
    "    fromtau_electron = getParticles(events.GenPart, 11, 11, ['isDirectTauDecayProduct'])\n",
    "    fromtau_muon = getParticles(events.GenPart, 13, 13, ['isDirectTauDecayProduct'])\n",
    "    tau_visible = events.GenVisTau\n",
    "\n",
    "    n_visibletaus = ak.sum(tau_visible.pt > 0, axis=1)\n",
    "    n_electrons_fromtaus = ak.sum(fromtau_electron.pt > 0, axis=1)\n",
    "    n_muons_fromtaus = ak.sum(fromtau_muon.pt > 0, axis=1)\n",
    "    # 3(elenuqq),6(munuqq),8(taunuqq)\n",
    "    htt_flavor = (n_quarks == 2) * 1 + (n_electrons == 1) * 3 + (n_muons == 1) * 5 + (n_taus == 1) * 7\n",
    "\n",
    "    matchedH = candidatefj.nearest(higgs, axis=1, threshold=0.8)\n",
    "    dr_fj_visibletaus = candidatefj.delta_r(tau_visible)\n",
    "    dr_fj_electrons = candidatefj.delta_r(fromtau_electron)\n",
    "    dr_fj_muons = candidatefj.delta_r(fromtau_muon)\n",
    "    dr_daughters = ak.concatenate([dr_fj_visibletaus, dr_fj_electrons, dr_fj_muons], axis=1)\n",
    "    # 1 (H only), 4 (H and one tau/electron or muon from tau), 5 (H and 2 taus/ele)\n",
    "    htt_matched = (ak.sum(matchedH.pt > 0, axis=1) == 1) * 1 + (ak.sum(dr_daughters < 0.8, axis=1) == 1) * 3 + (ak.sum(dr_daughters < 0.8, axis=1) == 2) * 5\n",
    "\n",
    "    return htt_flavor, htt_matched, matchedH, higgs\n",
    "\n",
    "\n",
    "def pad_val(\n",
    "    arr: ak.Array,\n",
    "    target: int,\n",
    "    value: float,\n",
    "    axis: int = 0,\n",
    "    to_numpy: bool = True,\n",
    "    clip: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    pads awkward array up to ``target`` index along axis ``axis`` with value ``value``,\n",
    "    optionally converts to numpy array\n",
    "    \"\"\"\n",
    "    ret = ak.fill_none(ak.pad_none(arr, target, axis=axis, clip=clip), value, axis=None)\n",
    "    return ret.to_numpy() if to_numpy else ret\n",
    "\n",
    "\n",
    "def add_selection(\n",
    "    name: str,\n",
    "    sel: np.ndarray,\n",
    "    selection: PackedSelection,\n",
    "    cutflow: dict,\n",
    "    isData: bool,\n",
    "    signGenWeights: ak.Array,\n",
    "):\n",
    "    \"\"\"adds selection to PackedSelection object and the cutflow dictionary\"\"\"\n",
    "    selection.add(name, sel)\n",
    "    cutflow[name] = (\n",
    "        np.sum(selection.all(*selection.names))\n",
    "        if isData\n",
    "        # add up sign of genWeights for MC\n",
    "        else np.sum(signGenWeights[selection.all(*selection.names)])\n",
    "    )\n",
    "\n",
    "\n",
    "def add_selection_no_cutflow(\n",
    "    name: str,\n",
    "    sel: np.ndarray,\n",
    "    selection: PackedSelection,\n",
    "):\n",
    "    \"\"\"adds selection to PackedSelection object\"\"\"\n",
    "    selection.add(name, ak.fill_none(sel, False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import correctionlib\n",
    "import awkward as ak\n",
    "from coffea import processor, hist, util\n",
    "import numpy as np\n",
    "import pickle as pkl \n",
    "import importlib.resources\n",
    "\n",
    "from coffea.lookup_tools.correctionlib_wrapper import correctionlib_wrapper\n",
    "from coffea.lookup_tools.dense_lookup import dense_lookup\n",
    "\n",
    "# https://twiki.cern.ch/twiki/bin/viewauth/CMS/BtagRecommendation\n",
    "btagWPs = {\n",
    "    \"deepJet\": {\n",
    "        '2016APV': {\n",
    "            'L': 0.0508,\n",
    "            'M': 0.2598,\n",
    "            'T': 0.6502,\n",
    "        },\n",
    "        '2016': {\n",
    "            'L': 0.0480,\n",
    "            'M': 0.2489,\n",
    "            'T': 0.6377,\n",
    "        },\n",
    "        '2017': {\n",
    "            'L': 0.0532,\n",
    "            'M': 0.3040,\n",
    "            'T': 0.7476,\n",
    "        },\n",
    "        '2018': {\n",
    "            'L': 0.0490,\n",
    "            'M': 0.2783,\n",
    "            'T': 0.7100,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "taggerBranch = {\n",
    "    \"deepJet\": \"btagDeepFlavB\",\n",
    "#    \"deepCSV\": \"btagDeep\"\n",
    "}\n",
    "\n",
    "class BTagEfficiency(processor.ProcessorABC):\n",
    "    def __init__(self, year='2017'):\n",
    "        self._year = year\n",
    "        self._accumulator = hist.Hist(\n",
    "            'Events',\n",
    "            hist.Cat('tagger', 'Tagger'),\n",
    "            hist.Bin('passWP', 'passWP',2,0,2),\n",
    "            hist.Bin('flavor', 'Jet hadronFlavour', [0, 4, 5]),\n",
    "            hist.Bin('pt', 'Jet pT', 20, 40, 300),\n",
    "            hist.Bin('abseta', 'Jet abseta', 4, 0, 2.5)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def process(self, events):\n",
    "        jets = events.Jet[\n",
    "            (events.Jet.pt > 30.)\n",
    "            & (abs(events.Jet.eta) < 2.5)\n",
    "            & events.Jet.isTight\n",
    "            & (events.Jet.puId > 0)\n",
    "        ]\n",
    "\n",
    "        out = self.accumulator.identity()\n",
    "        tags = [\n",
    "#            ('deepcsv', 'btagDeepB', 'M'),\n",
    "            ('deepJet', 'btagDeepFlavB', 'M'),\n",
    "        ]\n",
    "\n",
    "        for tagger, branch, wp in tags:\n",
    "            passbtag = jets[branch] > btagWPs[tagger][self._year][wp]\n",
    "\n",
    "            out.fill(tagger=tagger,\n",
    "                     pt=ak.flatten(jets.pt),\n",
    "                     abseta=ak.flatten(abs(jets.eta)),\n",
    "                     flavor=ak.flatten(jets.hadronFlavour),\n",
    "                     passWP=ak.flatten(passbtag)\n",
    "                 )\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def postprocess(self, a):\n",
    "        return a\n",
    "\n",
    "class BTagCorrector:\n",
    "    def __init__(self, wp, tagger=\"deepJet\", year=\"2017\", mod=\"\"):\n",
    "        self._year = year+mod\n",
    "        self._tagger = tagger\n",
    "        self._wp = wp\n",
    "        self._btagwp = btagWPs[tagger][year+mod][wp]\n",
    "        self._branch = taggerBranch[tagger]\n",
    "\n",
    "        # more docs at https://cms-nanoaod-integration.web.cern.ch/commonJSONSFs/BTV_btagging_Run2_UL/BTV_btagging_201*_UL.html   \n",
    "        if year == '2016':\n",
    "            self._cset = correctionlib.CorrectionSet.from_file(\"/cvmfs/cms.cern.ch/rsync/cms-nanoAOD/jsonpog-integration/POG/BTV/2016postVFP_UL/btagging.json.gz\")\n",
    "        elif year == '2016APV':\n",
    "            self._cset = correctionlib.CorrectionSet.from_file(\"/cvmfs/cms.cern.ch/rsync/cms-nanoAOD/jsonpog-integration/POG/BTV/2016preVFP_UL/btagging.json.gz\")\n",
    "        else:\n",
    "            self._cset = correctionlib.CorrectionSet.from_file(f\"/cvmfs/cms.cern.ch/rsync/cms-nanoAOD/jsonpog-integration/POG/BTV/{year}_UL/btagging.json.gz\")\n",
    "\n",
    "        # efficiency lookup\n",
    "        with importlib.resources.path(\"boostedhiggs.data\", f\"btageff_{tagger}_{wp}_{year}.coffea\") as filename:\n",
    "            self.efflookup = util.load(str(filename))\n",
    "\n",
    "    def lighttagSF(self, j, syst=\"central\"):\n",
    "        # syst: central, down, down_correlated, down_uncorrelated, up, up_correlated\n",
    "        # until correctionlib handles jagged data natively we have to flatten and unflatten\n",
    "        j, nj = ak.flatten(j), ak.num(j)\n",
    "        sf = self._cset[\"%s_incl\"%self._tagger].evaluate(syst, self._wp, np.array(j.hadronFlavour), np.array(abs(j.eta)), np.array(j.pt))\n",
    "        return ak.unflatten(sf, nj)\n",
    "        \n",
    "    def btagSF(self, j, syst=\"central\"):\n",
    "        # syst: central, down, down_correlated, down_uncorrelated, up, up_correlated\n",
    "        # until correctionlib handles jagged data natively we have to flatten and unflatten\n",
    "        j, nj = ak.flatten(j), ak.num(j)\n",
    "        sf = self._cset[\"%s_comb\"%self._tagger].evaluate(syst, self._wp, np.array(j.hadronFlavour), np.array(abs(j.eta)), np.array(j.pt))\n",
    "        return ak.unflatten(sf, nj)\n",
    "\n",
    "    def addBtagWeight(self, jets, weights, label=\"\"):\n",
    "        \"\"\"\n",
    "        Adding one common multiplicative SF (including bcjets + lightjets)\n",
    "        weights: weights class from coffea\n",
    "        jets: jets selected in your analysis\n",
    "        \"\"\"\n",
    "        \n",
    "        lightJets = jets[(jets.hadronFlavour == 0) & (abs(jets.eta)<2.5)]\n",
    "        bcJets = jets[(jets.hadronFlavour > 0) & (abs(jets.eta)<2.5)]\n",
    "\n",
    "        lightEff = self.efflookup(lightJets.hadronFlavour, lightJets.pt, abs(lightJets.eta))\n",
    "        bcEff = self.efflookup(bcJets.hadronFlavour, bcJets.pt, abs(bcJets.eta))\n",
    "\n",
    "        lightPass = lightJets[self._branch] > self._btagwp\n",
    "        bcPass = bcJets[self._branch] > self._btagwp\n",
    "\n",
    "        def combine(eff, sf, passbtag):\n",
    "            # tagged SF = SF*eff / eff = SF\n",
    "            tagged_sf = ak.prod(sf[passbtag], axis=-1)\n",
    "            # untagged SF = (1 - SF*eff) / (1 - eff)\n",
    "            untagged_sf = ak.prod(((1 - sf*eff) / (1 - eff))[~passbtag], axis=-1)\n",
    "\n",
    "            return ak.fill_none(tagged_sf * untagged_sf, 1.)\n",
    "            \n",
    "        lightweight = combine(\n",
    "            lightEff,\n",
    "            self.lighttagSF(lightJets, \"central\"),\n",
    "            lightPass\n",
    "        )\n",
    "        bcweight = combine(\n",
    "            bcEff,\n",
    "            self.btagSF(bcJets, \"central\"),\n",
    "            bcPass\n",
    "        )\n",
    "        \n",
    "        # nominal weight = btagSF (btagSFbc*btagSFlight)\n",
    "        nominal = lightweight * bcweight\n",
    "        weights.add('btagSF'+label, nominal )\n",
    "\n",
    "        # systematics:\n",
    "        # btagSFlight_{year}: btagSFlight_up/down\n",
    "        # btagSFbc_{year}: btagSFbc_up/down\n",
    "        # btagSFlight_correlated: btagSFlight_up/down_correlated\n",
    "        # btagSFbc_correlated:  btagSFbc_up/down_correlated\n",
    "        weights.add(\n",
    "            'btagSFlight_%s%s'%(label,self._year),\n",
    "            np.ones(len(nominal)),\n",
    "            weightUp=combine(\n",
    "                lightEff,\n",
    "                self.lighttagSF(lightJets, \"up\"),\n",
    "                lightPass\n",
    "            ),\n",
    "            weightDown=combine(\n",
    "                lightEff,\n",
    "                self.lighttagSF(lightJets, \"down\"),\n",
    "                lightPass\n",
    "            )\n",
    "        )\n",
    "        weights.add(\n",
    "            'btagSFbc_%s'%(label,self._year), \n",
    "            np.ones(len(nominal)),\n",
    "            weightUp=combine(\n",
    "                bcEff,\n",
    "                self.btagSF(bcJets, \"up\"),\n",
    "                bcPass\n",
    "            ),\n",
    "            weightDown=combine(\n",
    "                bcEff,\n",
    "                self.btagSF(bcJets, \"down\"),\n",
    "                bcPass\n",
    "            )\n",
    "        )\n",
    "        weights.add(\n",
    "            'btagSFlight_correlated_%s'%label,\n",
    "            np.ones(len(nominal)),\n",
    "            weightUp=combine(\n",
    "                lightEff,\n",
    "                self.lighttagSF(lightJets, \"up_correlated\"),\n",
    "                lightPass\n",
    "            ),\n",
    "            weightDown=combine(\n",
    "                lightEff,\n",
    "                self.lighttagSF(lightJets, \"down_correlated\"),\n",
    "                lightPass\n",
    "            )\n",
    "        )\n",
    "        weights.add(\n",
    "            'btagSFbc_correlated_%s'%label,\n",
    "            np.ones(len(nominal)),\n",
    "            weightUp=combine(\n",
    "                bcEff,\n",
    "                self.btagSF(bcJets, \"up_correlated\"),\n",
    "                bcPass\n",
    "            ),\n",
    "            weightDown=combine(\n",
    "                bcEff,\n",
    "                self.btagSF(bcJets, \"down_correlated\"),\n",
    "                bcPass\n",
    "            )\n",
    "        )\n",
    "        return nominal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pickle as pkl\n",
    "import pyarrow as pa\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import pathlib\n",
    "from typing import List, Optional\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "import importlib.resources\n",
    "\n",
    "from coffea import processor\n",
    "from coffea.nanoevents.methods import candidate, vector\n",
    "from coffea.analysis_tools import Weights, PackedSelection\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Found duplicate branch \")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "np.seterr(invalid='ignore')\n",
    "\n",
    "\n",
    "def dsum(*dicts):\n",
    "    ret = defaultdict(int)\n",
    "    for d in dicts:\n",
    "        for k, v in d.items():\n",
    "            ret[k] += v\n",
    "    return dict(ret)\n",
    "\n",
    "\n",
    "def pad_val(\n",
    "    arr: ak.Array,\n",
    "    value: float,\n",
    "    target: int = None,\n",
    "    axis: int = 0,\n",
    "    to_numpy: bool = False,\n",
    "    clip: bool = True,\n",
    "):\n",
    "    \"\"\"\n",
    "    pads awkward array up to ``target`` index along axis ``axis`` with value ``value``,\n",
    "    optionally converts to numpy array\n",
    "    \"\"\"\n",
    "    if target:\n",
    "        ret = ak.fill_none(ak.pad_none(arr, target, axis=axis, clip=clip), value, axis=None)\n",
    "    else:\n",
    "        ret = ak.fill_none(arr, value, axis=None)\n",
    "    return ret.to_numpy() if to_numpy else ret\n",
    "\n",
    "\n",
    "def build_p4(cand):\n",
    "    return ak.zip(\n",
    "        {\n",
    "            \"pt\": cand.pt,\n",
    "            \"eta\": cand.eta,\n",
    "            \"phi\": cand.phi,\n",
    "            \"mass\": cand.mass,\n",
    "            \"charge\": cand.charge,\n",
    "        },\n",
    "        with_name=\"PtEtaPhiMCandidate\",\n",
    "        behavior=candidate.behavior,\n",
    "    )\n",
    "\n",
    "\n",
    "class HwwProcessor(processor.ProcessorABC):\n",
    "    def __init__(self, year=\"2017\", yearmod=\"\", channels=[\"ele\", \"mu\", \"had\"], output_location=\"./outfiles/\", apply_trigger=True):\n",
    "        self._year = year\n",
    "        self._yearmod = yearmod\n",
    "        self._channels = channels\n",
    "        self._output_location = output_location\n",
    "\n",
    "        # trigger paths\n",
    "        with importlib.resources.path(\"binder_data\", \"triggers.json\") as path:\n",
    "            with open(path, 'r') as f:\n",
    "                self._HLTs = json.load(f)[self._year]\n",
    "        # apply trigger in selection?\n",
    "        self.apply_trigger = apply_trigger\n",
    "\n",
    "        # https://twiki.cern.ch/twiki/bin/view/CMS/MissingETOptionalFiltersRun2\n",
    "        with importlib.resources.path(\"binder_data\", \"metfilters.json\") as path:\n",
    "            with open(path, 'r') as f:\n",
    "                self._metfilters = json.load(f)[self._year]\n",
    "\n",
    "        # b-tagging corrector\n",
    "        self._btagWPs = btagWPs[\"deepJet\"][year + yearmod]\n",
    "        #self._btagSF = BTagCorrector(\"M\", \"deepJet\", year, yearmod)\n",
    "\n",
    "        self.selections = {}\n",
    "        self.cutflows = {}\n",
    "\n",
    "        if year == '2018':\n",
    "            self.dataset_per_ch = {\n",
    "                \"ele\": \"EGamma\",\n",
    "                \"mu\": \"SingleMuon\",\n",
    "                \"had\": \"JetHT\",\n",
    "            }\n",
    "        else:\n",
    "            self.dataset_per_ch = {\n",
    "                \"ele\": \"SingleElectron\",\n",
    "                \"mu\": \"SingleMuon\",\n",
    "                \"had\": \"JetHT\",\n",
    "            }\n",
    "\n",
    "        # for tagger model and preprocessing dict\n",
    "        self.tagger_resources_path = \"binder_tagger_resources/\"\n",
    "        \n",
    "\n",
    "    @property\n",
    "    def accumulator(self):\n",
    "        return self._accumulator\n",
    "\n",
    "    def save_dfs_parquet(self, fname, dfs_dict, ch):\n",
    "        if self._output_location is not None:\n",
    "            table = pa.Table.from_pandas(dfs_dict)\n",
    "            if len(table) != 0:     # skip dataframes with empty entries\n",
    "                pq.write_table(table, self._output_location + ch + '/parquet/' + fname + '.parquet')\n",
    "\n",
    "    def ak_to_pandas(self, output_collection: ak.Array) -> pd.DataFrame:\n",
    "        output = pd.DataFrame()\n",
    "        for field in ak.fields(output_collection):\n",
    "            output[field] = ak.to_numpy(output_collection[field])\n",
    "        return output\n",
    "\n",
    "    def add_selection(self, name: str, sel: np.ndarray, channel: list = None):\n",
    "        \"\"\"Adds selection to PackedSelection object and the cutflow dictionary\"\"\"\n",
    "        channels = channel if channel else self._channels\n",
    "        for ch in channels:\n",
    "            self.selections[ch].add(name, sel)\n",
    "            self.cutflows[ch][name] = np.sum(self.selections[ch].all(*self.selections[ch].names))\n",
    "\n",
    "    def process(self, events: ak.Array):\n",
    "        \"\"\"Returns skimmed events which pass preselection cuts and with the branches listed in self._skimvars\"\"\"\n",
    "        print('attempt')\n",
    "        dataset = events.metadata['dataset']\n",
    "        isMC = hasattr(events, \"genWeight\")\n",
    "        sumgenweight = ak.sum(events.genWeight) if isMC else 0\n",
    "        nevents = len(events)\n",
    "\n",
    "        # empty selections and cutflows\n",
    "        self.selections = {}\n",
    "        self.cutflows = {}\n",
    "        for ch in self._channels:\n",
    "            self.selections[ch] = PackedSelection()\n",
    "            self.cutflows[ch] = {}\n",
    "            self.cutflows[ch][\"all\"] = nevents\n",
    "\n",
    "#         # trigger\n",
    "#         trigger_noiso = {}\n",
    "#         trigger_iso = {}\n",
    "#         for ch in self._channels:\n",
    "#             if ch == \"had\" and isMC:\n",
    "#                 trigger = np.ones(nevents, dtype='bool')\n",
    "#                 trigger_noiso[ch] = np.zeros(nevents, dtype='bool')\n",
    "#                 trigger_iso[ch] = np.zeros(nevents, dtype='bool')\n",
    "#             else:\n",
    "#                 # apply trigger to both data and MC (except for hadronic channel)\n",
    "#                 trigger = np.zeros(nevents, dtype='bool')\n",
    "#                 trigger_noiso[ch] = np.zeros(nevents, dtype='bool')\n",
    "#                 trigger_iso[ch] = np.zeros(nevents, dtype='bool')\n",
    "#                 for t in self._HLTs[ch]:\n",
    "#                     if t in events.HLT.fields:\n",
    "#                         if \"Iso\" in t or \"WPTight_Gsf\" in t:\n",
    "#                             trigger_iso[ch] = trigger_iso[ch] | events.HLT[t]\n",
    "#                         else:\n",
    "#                             trigger_noiso[ch] = trigger_noiso[ch] | events.HLT[t]\n",
    "#                         trigger = trigger | events.HLT[t]\n",
    "#             if self.apply_trigger:\n",
    "#                 # apply trigger selection\n",
    "#                 self.add_selection(\"trigger\", trigger, [ch])\n",
    "#             del trigger\n",
    "#         print('attempt succeeded')        \n",
    "\n",
    "        # metfilters\n",
    "        metfilters = np.ones(nevents, dtype='bool')\n",
    "        metfilterkey = \"mc\" if isMC else \"data\"\n",
    "        for mf in self._metfilters[metfilterkey]:\n",
    "            if mf in events.Flag.fields:\n",
    "                metfilters = metfilters & events.Flag[mf]\n",
    "        self.add_selection(\"metfilters\", metfilters)\n",
    "\n",
    "        # define tau objects for starters (will be needed in the end to avoid picking taus)\n",
    "        loose_taus_mu = (\n",
    "            (events.Tau.pt > 20)\n",
    "            & (abs(events.Tau.eta) < 2.3)\n",
    "            & (events.Tau.idAntiMu >= 1)  # loose antiMu ID\n",
    "        )\n",
    "        loose_taus_ele = (\n",
    "            (events.Tau.pt > 20)\n",
    "            & (abs(events.Tau.eta) < 2.3)\n",
    "            & (events.Tau.idAntiEleDeadECal >= 2)  # loose Anti-electron MVA discriminator V6 (2018) ?\n",
    "        )\n",
    "        n_loose_taus_mu = ak.sum(loose_taus_mu, axis=1)\n",
    "        n_loose_taus_ele = ak.sum(loose_taus_ele, axis=1)\n",
    "\n",
    "        # Object definitions:\n",
    "        # define muon objects\n",
    "        loose_muons = (\n",
    "            (((events.Muon.pt > 30) & (events.Muon.pfRelIso04_all < 0.25)) |\n",
    "             (events.Muon.pt > 55))\n",
    "            & (np.abs(events.Muon.eta) < 2.4)\n",
    "            & (events.Muon.looseId)\n",
    "        )\n",
    "        n_loose_muons = ak.sum(loose_muons, axis=1)\n",
    "\n",
    "        good_muons = (\n",
    "            (events.Muon.pt > 30)\n",
    "            & (np.abs(events.Muon.eta) < 2.4)\n",
    "            & (np.abs(events.Muon.dz) < 0.1)\n",
    "            & (np.abs(events.Muon.dxy) < 0.05)\n",
    "            & (events.Muon.sip3d <= 4.0)\n",
    "            & events.Muon.mediumId\n",
    "        )\n",
    "        n_good_muons = ak.sum(good_muons, axis=1)\n",
    "\n",
    "        # define electron objects\n",
    "        loose_electrons = (\n",
    "            (((events.Electron.pt > 38) & (events.Electron.pfRelIso03_all < 0.25)) |\n",
    "             (events.Electron.pt > 120))\n",
    "            & (np.abs(events.Electron.eta) < 2.4)\n",
    "            & ((np.abs(events.Electron.eta) < 1.44) | (np.abs(events.Electron.eta) > 1.57))\n",
    "            & (events.Electron.cutBased >= events.Electron.LOOSE)\n",
    "        )\n",
    "        n_loose_electrons = ak.sum(loose_electrons, axis=1)\n",
    "\n",
    "        good_electrons = (\n",
    "            (events.Electron.pt > 38)\n",
    "            & (np.abs(events.Electron.eta) < 2.4)\n",
    "            & ((np.abs(events.Electron.eta) < 1.44) | (np.abs(events.Electron.eta) > 1.57))\n",
    "            & (np.abs(events.Electron.dz) < 0.1)\n",
    "            & (np.abs(events.Electron.dxy) < 0.05)\n",
    "            & (events.Electron.sip3d <= 4.0)\n",
    "            & (events.Electron.mvaFall17V2noIso_WP90)\n",
    "        )\n",
    "        n_good_electrons = ak.sum(good_electrons, axis=1)\n",
    "\n",
    "        # get candidate lepton\n",
    "        goodleptons = ak.concatenate([events.Muon[good_muons], events.Electron[good_electrons]], axis=1)    # concat muons and electrons\n",
    "        goodleptons = goodleptons[ak.argsort(goodleptons.pt, ascending=False)]      # sort by pt\n",
    "        candidatelep = ak.firsts(goodleptons)   # pick highest pt\n",
    "\n",
    "        candidatelep_p4 = build_p4(candidatelep)    # build p4 for candidate lepton\n",
    "        lep_reliso = candidatelep.pfRelIso04_all if hasattr(candidatelep, \"pfRelIso04_all\") else candidatelep.pfRelIso03_all    # reliso for candidate lepton\n",
    "        lep_miso = candidatelep.miniPFRelIso_all    # miniso for candidate lepton\n",
    "        mu_mvaId = candidatelep.mvaId if hasattr(candidatelep, \"mvaId\") else np.zeros(nevents)      # MVA-ID for candidate lepton\n",
    "\n",
    "        # JETS\n",
    "        goodjets = events.Jet[\n",
    "            (events.Jet.pt > 30)\n",
    "            & (abs(events.Jet.eta) < 2.5)\n",
    "            & events.Jet.isTight\n",
    "            & (events.Jet.puId > 0)\n",
    "        ]\n",
    "        # reject EE noisy jets for 2017\n",
    "        if self._year == '2017':\n",
    "            goodjets = goodjets[\n",
    "                (goodjets.pt > 50)\n",
    "                | (abs(goodjets.eta) < 2.65)\n",
    "                | (abs(goodjets.eta) > 3.139)\n",
    "            ]\n",
    "        ht = ak.sum(goodjets.pt, axis=1)\n",
    "\n",
    "        # FATJETS\n",
    "        fatjets = events.FatJet\n",
    "\n",
    "        good_fatjets = (\n",
    "            (fatjets.pt > 200)\n",
    "            & (abs(fatjets.eta) < 2.5)\n",
    "            & fatjets.isTight\n",
    "        )\n",
    "        n_fatjets = ak.sum(good_fatjets, axis=1)\n",
    "\n",
    "        good_fatjets = fatjets[good_fatjets]        # select good fatjets\n",
    "        good_fatjets = good_fatjets[ak.argsort(good_fatjets.pt, ascending=False)]       # sort them by pt\n",
    "        leadingfj = ak.firsts(good_fatjets)     # pick leading pt\n",
    "        secondfj = ak.pad_none(good_fatjets, 2, axis=1)[:, 1]       # pick second leading pt\n",
    "\n",
    "        # for hadronic channels: candidatefj is the leading pt one\n",
    "        fj_idx_had = 0\n",
    "        candidatefj_had = leadingfj\n",
    "\n",
    "        # for leptonic channel: first clean jets and leptons by removing overlap, then pick candidate_fj closest to the lepton\n",
    "        lep_in_fj_overlap_bool = good_fatjets.delta_r(candidatelep_p4) > 0.1\n",
    "        good_fatjets = good_fatjets[lep_in_fj_overlap_bool]\n",
    "        fj_idx_lep = ak.argmin(good_fatjets.delta_r(candidatelep_p4), axis=1, keepdims=True)\n",
    "        candidatefj_lep = ak.firsts(good_fatjets[fj_idx_lep])\n",
    "\n",
    "        # # TODO: get PFcands of the candidate jet\n",
    "        # msk = (events[\"FatJetPFCands\"].jetIdx == ak.firsts(fj_idx_lep))\n",
    "        # jet_ak_pfcands = events.FatJetPFCands[msk]\n",
    "        # jet_pfcands = events.PFCands[jet_ak_pfcands.pFCandsIdx]\n",
    "\n",
    "        # MET\n",
    "        met = events.MET\n",
    "        mt_lep_met = np.sqrt(\n",
    "            2. * candidatelep_p4.pt * met.pt * (ak.ones_like(met.pt) - np.cos(candidatelep_p4.delta_phi(met)))\n",
    "        )\n",
    "\n",
    "        # for leptonic channel: pick candidate_fj closest to the MET\n",
    "        # candidatefj_lep = ak.firsts(good_fatjets[ak.argmin(good_fatjets.delta_phi(met), axis=1, keepdims=True)])      # get candidatefj for leptonic channel\n",
    "\n",
    "        # lepton and fatjet mass\n",
    "        lep_fj_m = (candidatefj_lep - candidatelep_p4).mass  # mass of fatjet without lepton\n",
    "\n",
    "        # b-jets\n",
    "        # in event, pick highest b score in opposite direction from signal (we will make cut here to avoid tt background events producing bjets)\n",
    "        dphi_jet_lepfj = abs(goodjets.delta_phi(candidatefj_lep))\n",
    "        bjets_away_lepfj = goodjets[dphi_jet_lepfj > np.pi / 2]\n",
    "\n",
    "        dphi_jet_candidatefj_had = abs(goodjets.delta_phi(candidatefj_had))\n",
    "        bjets_away_candidatefj_had = goodjets[dphi_jet_candidatefj_had > np.pi / 2]\n",
    "\n",
    "        # deltaR\n",
    "        lep_fj_dr = candidatefj_lep.delta_r(candidatelep_p4)\n",
    "\n",
    "        \"\"\"\n",
    "        HEM issue: Hadronic calorimeter Endcaps Minus (HEM) issue.\n",
    "        The endcaps of the hadron calorimeter failed to cover the phase space at -3 < eta < -1.3 and -1.57 < phi < -0.87 during the 2018 data C and D.\n",
    "        The transverse momentum of the jets in this region is typically under-measured, this resuls in over-measured MET. It could also result on new electrons.\n",
    "        We must veto the jets and met in this region.\n",
    "        Should we veto on AK8 jets or electrons too?\n",
    "        Let's add this as a cut to check first.\n",
    "        \"\"\"\n",
    "        if self._year == \"2018\":\n",
    "            hem_cleaning = (\n",
    "                events.run >= 319077 &\n",
    "                ak.any((\n",
    "                    (events.Jet.pt > 30.)\n",
    "                    & (events.Jet.eta > -3.2)\n",
    "                    & (events.Jet.eta < -1.3)\n",
    "                    & (events.Jet.phi > -1.57)\n",
    "                    & (events.Jet.phi < -0.87)\n",
    "                ), -1)\n",
    "                | (\n",
    "                    (met.phi > -1.62)\n",
    "                    & (met.pt < 470.)\n",
    "                    & (met.phi < -0.62)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # event selections for semi-leptonic channels\n",
    "        self.add_selection(\n",
    "            name='fatjetKin',\n",
    "            sel=candidatefj_lep.pt > 200,\n",
    "            channel=['mu', 'ele']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name='ht',\n",
    "            sel=(ht > 200),\n",
    "            channel=['mu', 'ele']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name='mt',\n",
    "            sel=(mt_lep_met < 100),\n",
    "            channel=['mu', 'ele']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name='antibjettag',\n",
    "            sel=(ak.max(bjets_away_candidatefj_had.btagDeepFlavB, axis=1) < self._btagWPs[\"M\"]),\n",
    "            channel=['mu', 'ele']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name='leptonInJet',\n",
    "            sel=(lep_fj_dr < 0.8),\n",
    "            channel=['mu', 'ele']\n",
    "        )\n",
    "\n",
    "        # event selection for muon channel\n",
    "        self.add_selection(\n",
    "            name='leptonKin',\n",
    "            sel=(candidatelep.pt > 30),\n",
    "            channel=['mu']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name='oneLepton',\n",
    "            sel=(n_good_muons == 1) & (n_good_electrons == 0) & (n_loose_electrons == 0) & ~ak.any(loose_muons & ~good_muons, 1),\n",
    "            channel=['mu']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name='notaus',\n",
    "            sel=(n_loose_taus_mu == 0),\n",
    "            channel=['mu']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            'leptonIsolation',\n",
    "            sel=(((candidatelep.pt > 30) & (candidatelep.pt < 55) & (lep_reliso < 0.25)) | ((candidatelep.pt >= 55) & (candidatelep.miniPFRelIso_all < 0.2))),\n",
    "            channel=['mu']\n",
    "        )\n",
    "        # event selections for electron channel\n",
    "        self.add_selection(\n",
    "            name='leptonKin',\n",
    "            sel=(candidatelep.pt > 40),\n",
    "            channel=['ele']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name='oneLepton',\n",
    "            sel=(n_good_muons == 0) & (n_loose_muons == 0) & (n_good_electrons == 1) & ~ak.any(loose_electrons & ~good_electrons, 1),\n",
    "            channel=['ele']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name='notaus',\n",
    "            sel=(n_loose_taus_ele == 0),\n",
    "            channel=['ele']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            'leptonIsolation',\n",
    "            sel=(((candidatelep.pt > 30) & (candidatelep.pt < 120) & (lep_reliso < 0.3)) | ((candidatelep.pt >= 120) & (candidatelep.miniPFRelIso_all < 0.2))),\n",
    "            channel=['ele']\n",
    "        )\n",
    "\n",
    "        # event selections for hadronic channel\n",
    "        self.add_selection(\n",
    "            name='oneFatjet',\n",
    "            sel=(n_fatjets >= 1) &\n",
    "                (n_good_muons == 0) & (n_loose_muons == 0) &\n",
    "                (n_good_electrons == 0) & (n_loose_electrons == 0),\n",
    "            channel=['had']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name='fatjetKin',\n",
    "            sel=candidatefj_had.pt > 300,\n",
    "            channel=['had']\n",
    "        )\n",
    " \n",
    "        self.add_selection(\n",
    "            name='met',\n",
    "            sel=(met.pt < 200),\n",
    "            channel=['had']\n",
    "        )\n",
    "        self.add_selection(\n",
    "            name='antibjettag',\n",
    "            sel=(ak.max(bjets_away_candidatefj_had.btagDeepFlavB, axis=1) < self._btagWPs[\"M\"]),\n",
    "            channel=['had']\n",
    "        )\n",
    "\n",
    "        # fill tuple variables\n",
    "        variables = {\n",
    "            \"lep\": {\n",
    "                \"fj_pt\": candidatefj_lep.pt,\n",
    "                \"fj_bjets_ophem\": ak.max(bjets_away_lepfj.btagDeepFlavB, axis=1),\n",
    "                \"lep_pt\": candidatelep.pt,\n",
    "                \"lep_isolation\": lep_reliso,\n",
    "                \"lep_misolation\": lep_miso,\n",
    "                \"lep_fj_m\": lep_fj_m,\n",
    "                \"lep_fj_dr\": lep_fj_dr,\n",
    "                \"lep_met_mt\": mt_lep_met,\n",
    "            },\n",
    "            \"ele\": {},\n",
    "            \"mu\": {\n",
    "                \"lep_mvaId\": mu_mvaId,\n",
    "            },\n",
    "            \"had\": {\n",
    "                \"fj_pt\": candidatefj_had.pt,\n",
    "                \"fj_bjets_ophem\": ak.max(bjets_away_candidatefj_had.btagDeepFlavB, axis=1),\n",
    "                \"fj_pnh4q\": candidatefj_had.particleNet_H4qvsQCD,\n",
    "                \"fj_sl_pt\":  secondfj.pt,\n",
    "                \"fj_sl_pnh4q\": secondfj.particleNet_H4qvsQCD,\n",
    "            },\n",
    "            \"common\": {\n",
    "                \"met\": met.pt,\n",
    "                \"ht\": ht,\n",
    "            },\n",
    "        }\n",
    "\n",
    "        # gen matching\n",
    "        if (('HToWW' or 'HWW') in dataset) and isMC:\n",
    "            match_HWW_had = match_HWW(events.GenPart, candidatefj_had)\n",
    "            match_HWW_lep = match_HWW(events.GenPart, candidatefj_lep)\n",
    "\n",
    "            variables[\"lep\"][\"gen_Hpt\"] = ak.firsts(match_HWW_lep[\"matchedH\"].pt)\n",
    "            variables[\"lep\"][\"gen_Hnprongs\"] = match_HWW_lep[\"hWW_nprongs\"]\n",
    "            variables[\"lep\"][\"gen_iswlepton\"] = match_HWW_lep[\"iswlepton\"]\n",
    "            variables[\"lep\"][\"gen_iswstarlepton\"] = match_HWW_lep[\"iswstarlepton\"]\n",
    "            variables[\"had\"][\"gen_Hpt\"] = ak.firsts(match_HWW_had[\"matchedH\"].pt)\n",
    "            variables[\"had\"][\"gen_Hnprongs\"] = match_HWW_had[\"hWW_nprongs\"]\n",
    "\n",
    "        if ('DY' in dataset) and isMC:\n",
    "            Z = getParticles(events.GenPart, lowid=23, highid=23, flags=['fromHardProcess', 'isLastCopy'])\n",
    "            Z = ak.firsts(Z)\n",
    "            lep_Z_dr = Z.delta_r(candidatelep_p4)   # get dr between Z and lepton\n",
    "\n",
    "        # if trigger is not applied then save the trigger variables\n",
    "        if not self.apply_trigger:\n",
    "            variables[\"lep\"][\"cut_trigger_iso\"] = trigger_iso[ch]\n",
    "            variables[\"lep\"][\"cut_trigger_noniso\"] = trigger_noiso[ch]\n",
    "            variables[\"had\"][\"cut_trigger\"] = trigger_noiso[ch]\n",
    "\n",
    "        # let's save the hem veto as a cut for now\n",
    "        if self._year == \"2018\":\n",
    "            variables[\"common\"][\"hem_cleaning\"] = hem_cleaning\n",
    "\n",
    "        # TODO: run tagger on the events, add column that has tagger score\n",
    "\n",
    "        if isMC:\n",
    "            weights = Weights(nevents, storeIndividual=True)\n",
    "            weights.add('genweight', events.genWeight)\n",
    "\n",
    "            # self._btagSF.addBtagWeight(bjets_away_lepfj, weights, \"lep\")\n",
    "            # self._btagSF.addBtagWeight(bjets_away_candidatefj_had, weights, \"had\")\n",
    "\n",
    "            add_VJets_kFactors(weights, events.GenPart, dataset)\n",
    "\n",
    "            # store the final common weight\n",
    "            variables[\"common\"][\"weight\"] = weights.partial_weight([\"genweight\", \"L1Prefiring\", \"pileup\"])\n",
    "\n",
    "            weights_per_ch = {\"ele\": [], \"mu\": [], \"had\": []}\n",
    "            for key in weights._weights.keys():\n",
    "                # ignore btagSFlight/bc for now\n",
    "                if \"btagSFlight\" in key or \"btagSFbc\" in key:\n",
    "                    continue\n",
    "\n",
    "                if \"muon\" in key:\n",
    "                    varkey = \"mu\"\n",
    "                elif \"electron\" in key:\n",
    "                    varkey = \"ele\"\n",
    "                elif \"had\" in key:\n",
    "                    varkey = \"had\"\n",
    "                elif \"lep\" in key:\n",
    "                    varkey = \"lep\"\n",
    "                else:\n",
    "                    varkey = \"common\"\n",
    "                # store the individual weights (ONLY for now until we debug)\n",
    "                variables[varkey][f\"weight_{key}\"] = weights.partial_weight([key])\n",
    "                if varkey in weights_per_ch.keys():\n",
    "                    weights_per_ch[varkey].append(key)\n",
    "\n",
    "            # store the per channel weight\n",
    "            for ch in weights_per_ch.keys():\n",
    "                if len(weights_per_ch[ch]) > 0:\n",
    "                    variables[ch][f\"weight_{ch}\"] = weights.partial_weight(weights_per_ch[ch])\n",
    "\n",
    "            # NOTE: to add variations:\n",
    "            # for var in weights.variations:\n",
    "            #     variables[\"common\"][f\"weight_{key}\"] = weights.weight(key)\n",
    "\n",
    "        # initialize pandas dataframe\n",
    "        output = {}\n",
    "\n",
    "        for ch in self._channels:\n",
    "            fill_output = True\n",
    "            # for data, only fill output for the dataset needed\n",
    "            if not isMC and self.dataset_per_ch[ch] not in dataset:\n",
    "                fill_output = False\n",
    "            # only fill output for that channel if the selections yield any events\n",
    "            if np.sum(self.selections[ch].all(*self.selections[ch].names)) <= 0:\n",
    "                fill_output = False\n",
    "\n",
    "            if fill_output:\n",
    "                keys = [\"common\", ch]\n",
    "                if ch == \"ele\" or ch == \"mu\":\n",
    "                    keys += [\"lep\"]\n",
    "\n",
    "        #         out = {}\n",
    "        #         for key in keys:\n",
    "        #             for var, item in variables[key].items():\n",
    "        #                 # pad all the variables that are not a cut with -1\n",
    "        #                 pad_item = item if (\"cut\" in var or \"weight\" in var) else pad_val(item, -1)\n",
    "        #                 # fill out dictionary\n",
    "        #                 out[var] = item\n",
    "        #\n",
    "        #         # fill the output dictionary after selections\n",
    "        #         output[ch] = {\n",
    "        #             key: value[self.selections[ch].all(*self.selections[ch].names)] for (key, value) in out.items()\n",
    "        #         }\n",
    "        #     else:\n",
    "        #         output[ch] = {}\n",
    "        #\n",
    "        # # TODO: adding tagger stuff\n",
    "        # for ch in self._channels:\n",
    "        #     print('channel', ch)\n",
    "            print(\"pre-inference\")\n",
    "\n",
    "            pnet_vars = runInferenceTriton(\n",
    "                self.tagger_resources_path, events[self.selections[ch].all(*self.selections[ch].names)], fj_idx_lep=fj_idx_lep[self.selections[ch].all(*self.selections[ch].names)]\n",
    "            )\n",
    "\n",
    "            print(\"post-inference\")\n",
    "            output[ch] = {\n",
    "                **output[ch],\n",
    "                **{key: value for (key, value) in pnet_vars.items()},\n",
    "            }\n",
    "\n",
    "            # convert arrays to pandas\n",
    "            if not isinstance(output[ch], pd.DataFrame):\n",
    "                output[ch] = self.ak_to_pandas(output[ch])\n",
    "\n",
    "        # now save pandas dataframes\n",
    "        fname = events.behavior[\"__events_factory__\"]._partition_key.replace(\"/\", \"_\")\n",
    "        fname = 'condor_' + fname\n",
    "\n",
    "        for ch in self._channels:  # creating directories for each channel\n",
    "            if not os.path.exists(self._output_location + ch):\n",
    "                os.makedirs(self._output_location + ch)\n",
    "            if not os.path.exists(self._output_location + ch + '/parquet'):\n",
    "                os.makedirs(self._output_location + ch + '/parquet')\n",
    "\n",
    "            self.save_dfs_parquet(fname, output[ch], ch)\n",
    "\n",
    "        # return dictionary with cutflows\n",
    "        return {\n",
    "            dataset: {'mc': isMC,\n",
    "                      self._year: {'sumgenweight': sumgenweight,\n",
    "                                   'cutflows': self.cutflows}\n",
    "                      }\n",
    "        }\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        return accumulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = HwwProcessor(year='2017', channels=['ele', 'mu', 'had'], output_location='./outfiles' + '/1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96073a04cd244450907dcf27e1a65ada",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">attempt\n",
       "</pre>\n"
      ],
      "text/plain": [
       "attempt\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">pre-inference\n",
       "</pre>\n"
      ],
      "text/plain": [
       "pre-inference\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Running inference for candidate Jet\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Running inference for candidate Jet\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">size of input = 9\n",
       "</pre>\n"
      ],
      "text/plain": [
       "size of input = 9\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import uproot\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema, BaseSchema\n",
    "\n",
    "fileset = {'DYJetsToLL_Pt-250To400': ['nano_mc2017_1-1.root']}\n",
    "\n",
    "import uproot\n",
    "uproot.open.defaults['xrootd_handler'] = uproot.source.xrootd.MultithreadedXRootDSource\n",
    "\n",
    "from coffea.processor import IterativeExecutor,Runner,DaskExecutor\n",
    "\n",
    "# define executor (dask)\n",
    "# https://coffeateam.github.io/coffea/api/coffea.processor.DaskExecutor.html\n",
    "# executor = DaskExecutor(compression=1, status=True, client=client, treereduction=2)\n",
    "executor = processor.IterativeExecutor(status=True)\n",
    "\n",
    "# define the runner (Same as before)\n",
    "run = Runner(executor=executor,savemetrics=True,chunksize=10_000,schema=PFNanoAODSchema)\n",
    "\n",
    "# run\n",
    "out,metrics = run(fileset,'Events',processor_instance=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-1.pkl         0-1_ele.parquet 0-1_had.parquet 0-1_mu.parquet\r\n"
     ]
    }
   ],
   "source": [
    "! ls outfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with open(\"outfiles/0-1.pkl\", 'rb') as f:\n",
    "    metadata = pkl.load(f)\n",
    "    \n",
    "for key in metadata.keys():\n",
    "    sample = key\n",
    "\n",
    "data = {}\n",
    "\n",
    "data = pq.read_table(\"outfiles/0-1_ele.parquet\").to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>met</th>\n",
       "      <th>ht</th>\n",
       "      <th>weight</th>\n",
       "      <th>weight_genweight</th>\n",
       "      <th>weight_L1Prefiring</th>\n",
       "      <th>weight_pileup</th>\n",
       "      <th>weight_vjets_nominal</th>\n",
       "      <th>weight_d1K_NLO</th>\n",
       "      <th>weight_d2K_NLO</th>\n",
       "      <th>weight_d3K_NLO</th>\n",
       "      <th>...</th>\n",
       "      <th>lep_misolation</th>\n",
       "      <th>lep_fj_m</th>\n",
       "      <th>lep_fj_dr</th>\n",
       "      <th>lep_met_mt</th>\n",
       "      <th>fj_ttbar_bmerged</th>\n",
       "      <th>fj_ttbar_bsplit</th>\n",
       "      <th>fj_wjets_label</th>\n",
       "      <th>fj_isHVV_elenuqq</th>\n",
       "      <th>fj_isHVV_munuqq</th>\n",
       "      <th>fj_isHVV_taunuqq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.612907</td>\n",
       "      <td>447.37500</td>\n",
       "      <td>13.601362</td>\n",
       "      <td>20.189165</td>\n",
       "      <td>0.685567</td>\n",
       "      <td>0.982685</td>\n",
       "      <td>1.535414</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>62.179680</td>\n",
       "      <td>0.299843</td>\n",
       "      <td>71.886528</td>\n",
       "      <td>0.001474</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.978679</td>\n",
       "      <td>0.017502</td>\n",
       "      <td>4.110250e-06</td>\n",
       "      <td>0.000888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123.711029</td>\n",
       "      <td>371.43750</td>\n",
       "      <td>14.810668</td>\n",
       "      <td>20.189165</td>\n",
       "      <td>0.719904</td>\n",
       "      <td>1.019017</td>\n",
       "      <td>1.472875</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.030242</td>\n",
       "      <td>44.588131</td>\n",
       "      <td>0.201321</td>\n",
       "      <td>0.325316</td>\n",
       "      <td>0.014401</td>\n",
       "      <td>0.013754</td>\n",
       "      <td>0.532728</td>\n",
       "      <td>0.427444</td>\n",
       "      <td>4.547210e-05</td>\n",
       "      <td>0.011628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.491932</td>\n",
       "      <td>570.25000</td>\n",
       "      <td>15.026106</td>\n",
       "      <td>20.189165</td>\n",
       "      <td>0.795334</td>\n",
       "      <td>0.935790</td>\n",
       "      <td>1.511517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>44.666122</td>\n",
       "      <td>0.252124</td>\n",
       "      <td>78.710587</td>\n",
       "      <td>0.000713</td>\n",
       "      <td>0.005861</td>\n",
       "      <td>0.887609</td>\n",
       "      <td>0.103819</td>\n",
       "      <td>8.714887e-06</td>\n",
       "      <td>0.001990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>24.133675</td>\n",
       "      <td>728.34375</td>\n",
       "      <td>12.561553</td>\n",
       "      <td>20.189165</td>\n",
       "      <td>0.648352</td>\n",
       "      <td>0.959653</td>\n",
       "      <td>1.645740</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>16.657335</td>\n",
       "      <td>0.100688</td>\n",
       "      <td>49.839718</td>\n",
       "      <td>0.109255</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.376436</td>\n",
       "      <td>0.506892</td>\n",
       "      <td>4.687235e-07</td>\n",
       "      <td>0.006718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.491848</td>\n",
       "      <td>500.28125</td>\n",
       "      <td>-11.335053</td>\n",
       "      <td>-20.189165</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.998120</td>\n",
       "      <td>1.511517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>47.522362</td>\n",
       "      <td>0.278216</td>\n",
       "      <td>22.934059</td>\n",
       "      <td>0.000494</td>\n",
       "      <td>0.000418</td>\n",
       "      <td>0.973606</td>\n",
       "      <td>0.025310</td>\n",
       "      <td>1.264374e-08</td>\n",
       "      <td>0.000173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>290.403473</td>\n",
       "      <td>855.59375</td>\n",
       "      <td>21.445208</td>\n",
       "      <td>20.189165</td>\n",
       "      <td>0.991687</td>\n",
       "      <td>1.071118</td>\n",
       "      <td>1.418026</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004104</td>\n",
       "      <td>75.163155</td>\n",
       "      <td>0.422865</td>\n",
       "      <td>15.823492</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>0.026641</td>\n",
       "      <td>0.778317</td>\n",
       "      <td>0.163262</td>\n",
       "      <td>3.701225e-04</td>\n",
       "      <td>0.028321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>140.157074</td>\n",
       "      <td>597.93750</td>\n",
       "      <td>-9.963320</td>\n",
       "      <td>-20.189165</td>\n",
       "      <td>0.584778</td>\n",
       "      <td>0.843907</td>\n",
       "      <td>1.427484</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.009335</td>\n",
       "      <td>36.594910</td>\n",
       "      <td>0.367235</td>\n",
       "      <td>40.965614</td>\n",
       "      <td>0.001401</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.934171</td>\n",
       "      <td>0.064021</td>\n",
       "      <td>1.041912e-09</td>\n",
       "      <td>0.000287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>78.137581</td>\n",
       "      <td>445.62500</td>\n",
       "      <td>-19.746705</td>\n",
       "      <td>-20.189165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978084</td>\n",
       "      <td>1.511517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.007895</td>\n",
       "      <td>39.326492</td>\n",
       "      <td>0.110929</td>\n",
       "      <td>69.491348</td>\n",
       "      <td>0.018703</td>\n",
       "      <td>0.011054</td>\n",
       "      <td>0.290866</td>\n",
       "      <td>0.644478</td>\n",
       "      <td>4.360638e-04</td>\n",
       "      <td>0.034463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>44.058693</td>\n",
       "      <td>767.68750</td>\n",
       "      <td>-10.860132</td>\n",
       "      <td>-20.189165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.537919</td>\n",
       "      <td>1.423958</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>39.087818</td>\n",
       "      <td>0.111826</td>\n",
       "      <td>19.641182</td>\n",
       "      <td>0.000699</td>\n",
       "      <td>0.002293</td>\n",
       "      <td>0.908935</td>\n",
       "      <td>0.087035</td>\n",
       "      <td>5.690692e-06</td>\n",
       "      <td>0.001032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>13.283002</td>\n",
       "      <td>722.18750</td>\n",
       "      <td>21.624975</td>\n",
       "      <td>20.189165</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.071118</td>\n",
       "      <td>1.511517</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>51.264633</td>\n",
       "      <td>0.563984</td>\n",
       "      <td>54.991936</td>\n",
       "      <td>0.001418</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.917428</td>\n",
       "      <td>0.080676</td>\n",
       "      <td>7.487595e-09</td>\n",
       "      <td>0.000208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>736 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            met         ht     weight  weight_genweight  weight_L1Prefiring  \\\n",
       "0     17.612907  447.37500  13.601362         20.189165            0.685567   \n",
       "1    123.711029  371.43750  14.810668         20.189165            0.719904   \n",
       "2     44.491932  570.25000  15.026106         20.189165            0.795334   \n",
       "3     24.133675  728.34375  12.561553         20.189165            0.648352   \n",
       "4     21.491848  500.28125 -11.335053        -20.189165            0.562500   \n",
       "..          ...        ...        ...               ...                 ...   \n",
       "731  290.403473  855.59375  21.445208         20.189165            0.991687   \n",
       "732  140.157074  597.93750  -9.963320        -20.189165            0.584778   \n",
       "733   78.137581  445.62500 -19.746705        -20.189165            1.000000   \n",
       "734   44.058693  767.68750 -10.860132        -20.189165            1.000000   \n",
       "735   13.283002  722.18750  21.624975         20.189165            1.000000   \n",
       "\n",
       "     weight_pileup  weight_vjets_nominal  weight_d1K_NLO  weight_d2K_NLO  \\\n",
       "0         0.982685              1.535414             1.0             1.0   \n",
       "1         1.019017              1.472875             1.0             1.0   \n",
       "2         0.935790              1.511517             1.0             1.0   \n",
       "3         0.959653              1.645740             1.0             1.0   \n",
       "4         0.998120              1.511517             1.0             1.0   \n",
       "..             ...                   ...             ...             ...   \n",
       "731       1.071118              1.418026             1.0             1.0   \n",
       "732       0.843907              1.427484             1.0             1.0   \n",
       "733       0.978084              1.511517             1.0             1.0   \n",
       "734       0.537919              1.423958             1.0             1.0   \n",
       "735       1.071118              1.511517             1.0             1.0   \n",
       "\n",
       "     weight_d3K_NLO  ...  lep_misolation   lep_fj_m  lep_fj_dr  lep_met_mt  \\\n",
       "0               1.0  ...        0.000000  62.179680   0.299843   71.886528   \n",
       "1               1.0  ...        0.030242  44.588131   0.201321    0.325316   \n",
       "2               1.0  ...        0.000000  44.666122   0.252124   78.710587   \n",
       "3               1.0  ...        0.000000  16.657335   0.100688   49.839718   \n",
       "4               1.0  ...        0.000000  47.522362   0.278216   22.934059   \n",
       "..              ...  ...             ...        ...        ...         ...   \n",
       "731             1.0  ...        0.004104  75.163155   0.422865   15.823492   \n",
       "732             1.0  ...        0.009335  36.594910   0.367235   40.965614   \n",
       "733             1.0  ...        0.007895  39.326492   0.110929   69.491348   \n",
       "734             1.0  ...        0.000000  39.087818   0.111826   19.641182   \n",
       "735             1.0  ...        0.000000  51.264633   0.563984   54.991936   \n",
       "\n",
       "     fj_ttbar_bmerged  fj_ttbar_bsplit  fj_wjets_label  fj_isHVV_elenuqq  \\\n",
       "0            0.001474         0.001452        0.978679          0.017502   \n",
       "1            0.014401         0.013754        0.532728          0.427444   \n",
       "2            0.000713         0.005861        0.887609          0.103819   \n",
       "3            0.109255         0.000699        0.376436          0.506892   \n",
       "4            0.000494         0.000418        0.973606          0.025310   \n",
       "..                ...              ...             ...               ...   \n",
       "731          0.003089         0.026641        0.778317          0.163262   \n",
       "732          0.001401         0.000121        0.934171          0.064021   \n",
       "733          0.018703         0.011054        0.290866          0.644478   \n",
       "734          0.000699         0.002293        0.908935          0.087035   \n",
       "735          0.001418         0.000271        0.917428          0.080676   \n",
       "\n",
       "     fj_isHVV_munuqq  fj_isHVV_taunuqq  \n",
       "0       4.110250e-06          0.000888  \n",
       "1       4.547210e-05          0.011628  \n",
       "2       8.714887e-06          0.001990  \n",
       "3       4.687235e-07          0.006718  \n",
       "4       1.264374e-08          0.000173  \n",
       "..               ...               ...  \n",
       "731     3.701225e-04          0.028321  \n",
       "732     1.041912e-09          0.000287  \n",
       "733     4.360638e-04          0.034463  \n",
       "734     5.690692e-06          0.001032  \n",
       "735     7.487595e-09          0.000208  \n",
       "\n",
       "[736 rows x 32 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tagger_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.017830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.762091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.116105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.042151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.025972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>731</th>\n",
       "      <td>0.202045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732</th>\n",
       "      <td>0.068421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>733</th>\n",
       "      <td>2.010081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>0.095441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>735</th>\n",
       "      <td>0.087775</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>736 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tagger_score\n",
       "0        0.017830\n",
       "1        0.762091\n",
       "2        0.116105\n",
       "3        1.042151\n",
       "4        0.025972\n",
       "..            ...\n",
       "731      0.202045\n",
       "732      0.068421\n",
       "733      2.010081\n",
       "734      0.095441\n",
       "735      0.087775\n",
       "\n",
       "[736 rows x 1 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(data['fj_isHVV_elenuqq'] / (data['fj_ttbar_bmerged'] + data['fj_ttbar_bsplit'] + data['fj_wjets_label']), columns=['tagger_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
