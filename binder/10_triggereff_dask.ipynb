{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81cc03b6-51cc-45e1-b4c3-b28efbcb7f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "\n",
    "# we suppress ROOT warnings where our input ROOT tree has duplicate branches - these are handled correctly.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Found duplicate branch \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46a59769-ac91-4586-8e85-1b4294e021d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParticles(genparticles,lowid=22,highid=25,flags=['fromHardProcess', 'isLastCopy']):\n",
    "    \"\"\"\n",
    "    returns the particle objects that satisfy a low id, \n",
    "    high id condition and have certain flags\n",
    "    \"\"\"\n",
    "    absid = abs(genparticles.pdgId)\n",
    "    return genparticles[\n",
    "        ((absid >= lowid) & (absid <= highid))\n",
    "        & genparticles.hasFlags(flags)\n",
    "    ]\n",
    "\n",
    "def match_HWWlepqq(genparticles,candidatefj):\n",
    "    \"\"\"\n",
    "    return the number of matched objects (hWW*),daughters, \n",
    "    and gen flavor (enuqq, munuqq, taunuqq) \n",
    "    \"\"\"\n",
    "    W_PDGID = 24\n",
    "    HIGGS_PDGID = 25\n",
    "    higgs = getParticles(genparticles,HIGGS_PDGID)\n",
    "    is_hWW = ak.all(abs(higgs.children.pdgId)==W_PDGID,axis=2)\n",
    "\n",
    "    higgs = higgs[is_hWW]\n",
    "    higgs_wstar = higgs.children[ak.argmin(higgs.children.mass,axis=2,keepdims=True)]\n",
    "    higgs_w = higgs.children[ak.argmax(higgs.children.mass,axis=2,keepdims=True)]\n",
    "    \n",
    "    prompt_electron = getParticles(genparticles,11,11,['isPrompt','isLastCopy'])\n",
    "    prompt_muon = getParticles(genparticles,13,13,['isPrompt', 'isLastCopy'])\n",
    "    prompt_tau = getParticles(genparticles,15,15,['isPrompt', 'isLastCopy'])\n",
    "    prompt_q = getParticles(genparticles,0,5,['fromHardProcess', 'isLastCopy'])\n",
    "    prompt_q = prompt_q[abs(prompt_q.distinctParent.pdgId) == W_PDGID]\n",
    "    \n",
    "    dr_fj_quarks = candidatefj.delta_r(prompt_q)\n",
    "    dr_fj_electrons = candidatefj.delta_r(prompt_electron)\n",
    "    dr_fj_muons = candidatefj.delta_r(prompt_muon)\n",
    "    dr_fj_taus = candidatefj.delta_r(prompt_tau)\n",
    "    dr_daughters = ak.concatenate([dr_fj_quarks,dr_fj_electrons,dr_fj_muons,dr_fj_taus],axis=1)\n",
    "    hWWlepqq_nprongs = ak.sum(dr_daughters<0.8,axis=1)\n",
    "    \n",
    "    n_electrons = ak.sum(prompt_electron.pt>0,axis=1)\n",
    "    n_muons = ak.sum(prompt_muon.pt>0,axis=1)\n",
    "    n_taus = ak.sum(prompt_tau.pt>0,axis=1)\n",
    "    n_quarks = ak.sum(prompt_q.pt>0,axis=1)\n",
    "\n",
    "    # 4(elenuqq),6(munuqq),8(taunuqq)\n",
    "    hWWlepqq_flavor = (n_quarks==2)*1 + (n_electrons==1)*3 + (n_muons==1)*5 + (n_taus==1)*7\n",
    "    \n",
    "    matchedH = candidatefj.nearest(higgs, axis=1, threshold=0.8)\n",
    "    matchedW = candidatefj.nearest(higgs_w, axis=1, threshold=0.8)\n",
    "    matchedWstar = candidatefj.nearest(higgs_wstar, axis=1, threshold=0.8) \n",
    "\n",
    "    # 1 (H only), 4(W), 6(W star), 9(H, W and Wstar)\n",
    "    hWWlepqq_matched = (\n",
    "        (ak.sum(matchedH.pt > 0, axis=1)==1) * 1 \n",
    "        + (ak.sum(ak.flatten(matchedW.pt > 0, axis=2), axis=1)==1) * 3 \n",
    "        + (ak.sum(ak.flatten(matchedWstar.pt > 0, axis=2), axis=1)==1) * 5\n",
    "    )\n",
    "    \n",
    "    # leptons matched\n",
    "    dr_leptons = ak.concatenate([dr_fj_electrons,dr_fj_muons], axis=1)\n",
    "    matched_leptons = dr_leptons < 0.8\n",
    "    \n",
    "    leptons = ak.concatenate([prompt_electron, prompt_muon], axis=1)\n",
    "    leptons = leptons[matched_leptons]\n",
    "    \n",
    "    # leptons coming from W or W*\n",
    "    leptons_mass = ak.firsts(leptons.distinctParent.mass)\n",
    "    higgs_w_mass = ak.firsts(ak.flatten(higgs_w.mass))[ak.firsts(leptons.pt > 0)]\n",
    "    higgs_wstar_mass = ak.firsts(ak.flatten(higgs_wstar.mass))[ak.firsts(leptons.pt > 0)]\n",
    "\n",
    "    iswlepton = (leptons_mass == higgs_w_mass)\n",
    "    iswstarlepton = (leptons_mass == higgs_wstar_mass)\n",
    "    \n",
    "    # let's return only:\n",
    "    # - matchedH (the higgs boson that is matched to the jet)\n",
    "    # - (iswlepton,iswstarlepton)\n",
    "    return matchedH,iswlepton,iswstarlepton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5255de6a-60e5-4914-87cd-88106ec79a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from coffea.processor import ProcessorABC, column_accumulator\n",
    "from coffea.nanoevents.methods import candidate, vector\n",
    "from coffea.analysis_tools import Weights, PackedSelection\n",
    "    \n",
    "class TriggerEfficienciesProcessor(ProcessorABC):\n",
    "    \"\"\" Accumulates histograms from all input events: 1) before triggers, and 2) after triggers \"\"\"\n",
    "    def __init__(self, year=2017):\n",
    "        super(TriggerEfficienciesProcessor, self).__init__()\n",
    "        self._year = year\n",
    "        self._trigger_dict = {\n",
    "            2017:{\n",
    "                \"enominal\": [\n",
    "                    \"Ele35_WPTight_Gsf\", # electron trigger with isolation\n",
    "                    \"Ele115_CaloIdVT_GsfTrkIdT\", # electron trigger without isolation\n",
    "                ],\n",
    "                \"ejet\": [\n",
    "                    \"Ele50_CaloIdVT_GsfTrkIdT_PFJet165\"\n",
    "                ],\n",
    "                \"evvl\": [\n",
    "                    \"Ele15_IsoVVVL_PFHT600\"\n",
    "                ],\n",
    "                \"evvlmet\": [\n",
    "                    \"Ele15_IsoVVVL_PFHT450_PFMET50\"\n",
    "                ],\n",
    "                \"jet\": [\n",
    "                    \"PFHT1050\",\n",
    "                    \"AK8PFJet400_TrimMass30\",\n",
    "                    \"AK8PFJet420_TrimMass30\",\n",
    "                    \"AK8PFHT800_TrimMass50\",\n",
    "                    \"PFJet500\",\n",
    "                    \"AK8PFJet500\",\n",
    "                ],\n",
    "                \"munominal\": [\n",
    "                    \"IsoMu27\", # muon trigger with isolation\n",
    "                    \"Mu50\", # muon trigger without isolation\n",
    "                ],\n",
    "                \"muvvlmet\": [\n",
    "                    \"Mu15_IsoVVVL_PFHT450_PFMET50\"\n",
    "                ],\n",
    "                \"muvvl\": [\n",
    "                    \"Mu15_IsoVVVL_PFHT600\"\n",
    "                ],\n",
    "            }\n",
    "        }[self._year]\n",
    "        self._triggers = {\"ele\": [\"enominal\",\"ejet\",\"evvl\",\"evvlmet\",\"jet\"],\n",
    "                          \"mu\": [\"munominal\",\"muvvl\",\"muvvlmet\",\"jet\"],\n",
    "                          \"had\": [\"jet\"],\n",
    "                         }\n",
    "        \n",
    "        self._channels = [\"ele\",\"mu\"]\n",
    "    \n",
    "    def pad_val(\n",
    "        self, arr: ak.Array, target: int, value: float, axis: int = 0, to_numpy: bool = True\n",
    "    ):\n",
    "        \"\"\"pads awkward array up to `target` index along axis `axis` with value `value`, optionally converts to numpy array\"\"\"\n",
    "        ret = ak.fill_none(ak.pad_none(arr, target, axis=axis, clip=True), value)\n",
    "        return ret.to_numpy() if to_numpy else ret\n",
    "    \n",
    "    def process(self, events):\n",
    "        \"\"\" Returns pre- (den) and post- (num) trigger histograms from input NanoAOD events \"\"\"\n",
    "        dataset = events.metadata['dataset']\n",
    "        n_events = len(events)\n",
    "        isRealData = not hasattr(events, \"genWeight\")\n",
    "        \n",
    "        def pad_val_nevents(arr: ak.Array):\n",
    "            \"\"\"pad values with the length equal to the number of events\"\"\"\n",
    "            return self.pad_val(arr, n_events, -1)\n",
    "    \n",
    "        # skimmed events for different channels\n",
    "        out = {}\n",
    "        for channel in self._channels:\n",
    "            out[channel] = {}\n",
    "\n",
    "        \"\"\" Save OR of triggers as booleans \"\"\"\n",
    "        for channel in self._channels:\n",
    "            HLT_triggers = {}\n",
    "            for t in self._triggers[channel]:\n",
    "                HLT_triggers[\"HLT_\"+t] = np.any(\n",
    "                        np.array(\n",
    "                            [\n",
    "                                events.HLT[trigger] for trigger in self._trigger_dict[t] if trigger in events.HLT.fields\n",
    "                            ]\n",
    "                        ),\n",
    "                        axis=0,\n",
    "                )                \n",
    "            out[channel] = {**out[channel], **HLT_triggers}\n",
    "\n",
    "        \"\"\" Baseline selection \"\"\"\n",
    "        goodmuon = (\n",
    "            (events.Muon.pt > 25)\n",
    "            & (abs(events.Muon.eta) < 2.4)\n",
    "            & events.Muon.mediumId\n",
    "        )\n",
    "        nmuons = ak.sum(goodmuon, axis=1)\n",
    "        goodelectron = (\n",
    "            (events.Electron.pt > 25)\n",
    "            & (abs(events.Electron.eta) < 2.5)\n",
    "            & (events.Electron.mvaFall17V2noIso_WP90)\n",
    "        )\n",
    "        nelectrons = ak.sum(goodelectron, axis=1)\n",
    "        goodtau = (\n",
    "            (events.Tau.pt > 20)\n",
    "            & (abs(events.Tau.eta) < 2.3)\n",
    "            & (events.Tau.idAntiEle >= 8)\n",
    "            & (events.Tau.idAntiMu >= 1)\n",
    "        )\n",
    "        ntaus = ak.sum(goodtau, axis=1)\n",
    "        \n",
    "        # leading lepton\n",
    "        goodleptons = ak.concatenate([events.Muon[goodmuon], events.Electron[goodelectron]], axis=1)\n",
    "        candidatelep = ak.firsts(goodleptons[ak.argsort(goodleptons.pt)])\n",
    "        \n",
    "        # fatjet closest to MET\n",
    "        fatjets = events.FatJet\n",
    "        candidatefj = fatjets[\n",
    "            (fatjets.pt > 200) &\n",
    "            (abs(fatjets.eta) < 2.4)\n",
    "        ]\n",
    "        met = events.MET\n",
    "        dphi_met_fj = abs(candidatefj.delta_phi(met))\n",
    "        candidatefj = ak.firsts(candidatefj[ak.argmin(dphi_met_fj,axis=1,keepdims=True)])\n",
    "        dr_lep_fj = candidatefj.delta_r(candidatelep)\n",
    "\n",
    "        # jets\n",
    "        jets = events.Jet\n",
    "        candidatejet = jets[\n",
    "            (jets.pt > 30) \n",
    "            & (abs(jets.eta) < 2.5) \n",
    "            & jets.isTight\n",
    "        ]\n",
    "        \n",
    "        # define isolation\n",
    "        mu_iso = ak.where(candidatelep.pt >= 55., candidatelep.miniPFRelIso_all, candidatelep.pfRelIso03_all)\n",
    "        ele_iso = ak.where(candidatelep.pt >= 120., candidatelep.pfRelIso03_all, candidatelep.pfRelIso03_all)\n",
    "        \n",
    "        # define selections for different channels\n",
    "        for channel in self._channels:\n",
    "            selection = PackedSelection()\n",
    "            selection.add('fjkin', candidatefj.pt > 200)\n",
    "            if channel==\"mu\":\n",
    "                selection.add('onemuon', (nmuons == 1) & (nelectrons == 0) & (ntaus == 0))\n",
    "                selection.add('muonkin', (candidatelep.pt > 27.) & abs(candidatelep.eta < 2.4))\n",
    "            elif channel==\"ele\":\n",
    "                selection.add('oneelectron', (nelectrons == 1) & (nmuons == 0) & (ntaus == 0))\n",
    "                selection.add('electronkin', (candidatelep.pt > 30.) & abs(candidatelep.eta < 2.4))\n",
    "\n",
    "            \"\"\" Define other variables to save \"\"\"\n",
    "            out[channel][\"fj_pt\"] = pad_val_nevents(candidatefj.pt)\n",
    "            out[channel][\"fj_msoftdrop\"] = pad_val_nevents(candidatefj.msoftdrop)\n",
    "            out[channel][\"lep_pt\"] = pad_val_nevents(candidatelep.pt)\n",
    "            if channel==\"mu\":\n",
    "                out[channel][\"lep_isolation\"] = pad_val_nevents(mu_iso)\n",
    "            elif channel==\"ele\":\n",
    "                out[channel][\"lep_isolation\"] = pad_val_nevents(ele_iso)\n",
    "            out[channel][\"fj_lep_mass\"] = pad_val_nevents((candidatefj - candidatelep).mass)\n",
    "            out[channel][\"fj_lep_dR\"] = pad_val_nevents(dr_lep_fj)\n",
    "            out[channel][\"ht\"] = pad_val_nevents(ak.sum(candidatejet.pt, axis=1))\n",
    "            \n",
    "            if \"HToWW\" in dataset:\n",
    "                matchedH,iswlepton,iswstarlepton = match_HWWlepqq(events.GenPart,candidatefj)\n",
    "                matchedH_pt = ak.firsts(matchedH.pt)\n",
    "            else:\n",
    "                matchedH_pt = ak.zeros_like(candidatefj.pt)\n",
    "                iswlepton = ak.ones_like(candidatefj.pt, dtype=bool)\n",
    "                iswstarlepton = ak.ones_like(candidatefj.pt, dtype=bool)\n",
    "            out[channel][\"higgspt\"] = pad_val_nevents(matchedH_pt)\n",
    "            out[channel][\"iswlepton\"] = pad_val_nevents(iswlepton)\n",
    "            out[channel][\"iswstarlepton\"] = pad_val_nevents(iswstarlepton)\n",
    "\n",
    "            # use column accumulators\n",
    "            out[channel] = {\n",
    "                key: column_accumulator(value[selection.all(*selection.names)])\n",
    "                for (key, value) in out[channel].items()\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            self._year: {\n",
    "                dataset: {\"nevents\": n_events, \"skimmed_events\": out}\n",
    "            }\n",
    "        }\n",
    "\n",
    "                  \n",
    "    def postprocess(self, accumulator):\n",
    "        for year, datasets in accumulator.items():\n",
    "            for dataset, output in datasets.items():\n",
    "                for channel in output[\"skimmed_events\"].keys():\n",
    "                    output[\"skimmed_events\"][channel] = {\n",
    "                        key: value.value for (key, value) in output[\"skimmed_events\"][channel].items()\n",
    "                    }\n",
    "                \n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cd67de5-861d-4c47-aa1a-1ab951869184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the client\n",
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(\"tls://localhost:8786\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc15a3d-c500-4767-acc8-47d803172e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########                                ] | 22% Completed |  5min 30.3s"
     ]
    }
   ],
   "source": [
    "# define the full fileset\n",
    "# download this file in a new terminal in coffea casa: \n",
    "# wget https://raw.githubusercontent.com/cmantill/boostedhiggs/main/fileset/fileset_2017_UL_NANO.json\n",
    "# the dataset that we will run on is called: GluGluHToWWToLNuQQ_M125_TuneCP5_PSweight_13TeV-powheg2-jhugen727-pythia8\n",
    "import json\n",
    "import uproot\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema, BaseSchema\n",
    "dataset_name = \"GluGluHToWWToLNuQQ_M125_TuneCP5_PSweight_13TeV-powheg2-jhugen727-pythia8\"\n",
    "with open(\"/home/cms-jovyan/fileset_2017_UL_NANO.json\", 'r') as f:\n",
    "    files = json.load(f)[dataset_name]\n",
    "    \n",
    "fileset = {}\n",
    "# need to define the fileset but call them with xcache\n",
    "fileset[dataset_name] = [\"root://xcache/\"+ f for f in files]\n",
    "\n",
    "import uproot\n",
    "uproot.open.defaults['xrootd_handler'] = uproot.source.xrootd.MultithreadedXRootDSource\n",
    "\n",
    "from coffea.processor import IterativeExecutor,Runner,DaskExecutor\n",
    "\n",
    "# define processor\n",
    "p = TriggerEfficienciesProcessor()\n",
    "\n",
    "# define executor (dask)\n",
    "# https://coffeateam.github.io/coffea/api/coffea.processor.DaskExecutor.html\n",
    "executor = DaskExecutor(compression=1, status=True, client=client, treereduction=2)\n",
    "\n",
    "# define the runner (Same as before)\n",
    "run = Runner(executor=executor,savemetrics=True,chunksize=10000,schema=NanoAODSchema)\n",
    "\n",
    "# run\n",
    "out,metrics = run(fileset,'Events',processor_instance=p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91dde78-61d3-419c-ab15-b0153d8dcb2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fileset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e816960c-1735-4b25-a6a6-3fdcd1bc8554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'GluGluHToWWToLNuQQ_M125_TuneCP5_PSweight_13TeV-powheg2-jhugen727-pythia8': {'jet': Hist(\n",
      "  StrCategory(['e', 'mu'], growth=True, name='channel', label='channel'),\n",
      "  StrCategory(['numerator', 'denominator'], name='region', label='region'),\n",
      "  Regular(30, 200, 700, name='jetpt', label='Jet $p_T$ [GeV]'),\n",
      "  Regular(30, -2.4, 2.4, name='jeteta', label='Jet eta'),\n",
      "  storage=Double()) # Sum: 104831.0 (105045.0 with flow), 'lepton': Hist(\n",
      "  StrCategory(['e', 'mu'], growth=True, name='channel', label='channel'),\n",
      "  StrCategory(['numerator', 'denominator'], name='region', label='region'),\n",
      "  Regular(30, 10, 200, name='leppt', label='Lepton $p_T$ [GeV]'),\n",
      "  storage=Double()) # Sum: 99864.0 (105045.0 with flow), 'higgs': Hist(\n",
      "  StrCategory(['e', 'mu'], growth=True, name='channel', label='channel'),\n",
      "  StrCategory(['numerator', 'denominator'], name='region', label='region'),\n",
      "  Regular(50, 10, 800, name='higgspt', label='matched H $p_T$ [GeV]'),\n",
      "  storage=Double()) # Sum: 52888.0 (105045.0 with flow)}}\n"
     ]
    }
   ],
   "source": [
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31588e7c-92d2-44dd-89a4-0975e36ac8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3122f7-49c1-4116-9ef2-7a1c7f2f037f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77384418-4b41-46c6-8f4c-436bea45cd13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22db6738-45b7-4d2d-839f-a92c3e06e278",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd487696-36f0-4fe5-a65b-5a834fd71bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92813fc9-ab28-4c4c-9efb-75ab55b1c537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fc1b581-93f2-4751-96b9-2f660533e849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853eefb1-f8bb-4d7b-a879-ff73a0414a9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d472afe-4b0e-432a-a2ed-5e91bcd4f396",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
