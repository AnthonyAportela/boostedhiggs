{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import uproot\n",
    "from coffea import nanoevents, processor\n",
    "from coffea.nanoevents import BaseSchema, NanoAODSchema, NanoEventsFactory\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import pickle as pkl\n",
    "import shutil\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from coffea import processor\n",
    "from coffea.analysis_tools import PackedSelection, Weights\n",
    "from coffea.nanoevents.methods import candidate, vector\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Found duplicate branch \")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "np.seterr(invalid=\"ignore\")\n",
    "\n",
    "### awkward 1.10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_p4(cand):\n",
    "    return ak.zip(\n",
    "        {\n",
    "            \"pt\": cand.pt,\n",
    "            \"eta\": cand.eta,\n",
    "            \"phi\": cand.phi,\n",
    "            \"mass\": cand.mass,\n",
    "            \"charge\": cand.charge,\n",
    "        },\n",
    "        with_name=\"PtEtaPhiMCandidate\",\n",
    "        behavior=candidate.behavior,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mHWW\u001b[m\u001b[m       dy2.root  hww1.root hww3.root qcd2.root\r\n",
      "dy1.root  dy3.root  hww2.root qcd1.root qcd3.root\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../rootfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### schema\n",
    "nanoevents.PFNanoAODSchema.mixins[\"PFCands\"] = \"PFCand\"\n",
    "nanoevents.PFNanoAODSchema.mixins[\"SV\"] = \"PFCand\"\n",
    "\n",
    "# load a root file into coffea-friendly NanoAOD structure\n",
    "import uproot\n",
    "f = uproot.open(f\"../rootfiles/hww1.root\")\n",
    "num = f['Events'].num_entries   ### checks number of events per file \n",
    "print(f'number of events per file is {num}')\n",
    "\n",
    "events = nanoevents.NanoEventsFactory.from_root(f, \"Events\", schemaclass=nanoevents.PFNanoAODSchema).events()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get candidate jet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### make selections\n",
    "nevents = len(events)\n",
    "\n",
    "# define muon objects\n",
    "loose_muons = (\n",
    "    (((events.Muon.pt > 30) & (events.Muon.pfRelIso04_all < 0.25)) |\n",
    "     (events.Muon.pt > 55))\n",
    "    & (np.abs(events.Muon.eta) < 2.4)\n",
    "    & (events.Muon.looseId)\n",
    ")\n",
    "n_loose_muons = ak.sum(loose_muons, axis=1)\n",
    "\n",
    "good_muons = (\n",
    "    (events.Muon.pt > 28)\n",
    "    & (np.abs(events.Muon.eta) < 2.4)\n",
    "    & (np.abs(events.Muon.dz) < 0.1)\n",
    "    & (np.abs(events.Muon.dxy) < 0.05)\n",
    "    & (events.Muon.sip3d <= 4.0)\n",
    "    & events.Muon.mediumId\n",
    ")\n",
    "n_good_muons = ak.sum(good_muons, axis=1)\n",
    "\n",
    "# define electron objects\n",
    "loose_electrons = (\n",
    "    (((events.Electron.pt > 38) & (events.Electron.pfRelIso03_all < 0.25)) |\n",
    "     (events.Electron.pt > 120))\n",
    "    & ((np.abs(events.Electron.eta) < 1.44) | (np.abs(events.Electron.eta) > 1.57))\n",
    "    & (events.Electron.cutBased >= events.Electron.LOOSE)\n",
    ")\n",
    "n_loose_electrons = ak.sum(loose_electrons, axis=1)\n",
    "\n",
    "good_electrons = (\n",
    "    (events.Electron.pt > 38)\n",
    "    & ((np.abs(events.Electron.eta) < 1.44) | (np.abs(events.Electron.eta) > 1.57))\n",
    "    & (np.abs(events.Electron.dz) < 0.1)\n",
    "    & (np.abs(events.Electron.dxy) < 0.05)\n",
    "    & (events.Electron.sip3d <= 4.0)\n",
    "    & (events.Electron.mvaFall17V2noIso_WP90)\n",
    ")\n",
    "n_good_electrons = ak.sum(good_electrons, axis=1)\n",
    "\n",
    "# leading lepton\n",
    "goodleptons = ak.concatenate([events.Muon[good_muons], events.Electron[good_electrons]], axis=1)\n",
    "goodleptons = goodleptons[ak.argsort(goodleptons.pt, ascending=False)]\n",
    "candidatelep = ak.firsts(goodleptons)\n",
    "\n",
    "# candidate leptons\n",
    "candidatelep_p4 = build_p4(candidatelep)\n",
    "\n",
    "# MET\n",
    "met = events.MET\n",
    "mt_lep_met = np.sqrt(\n",
    "    2. * candidatelep_p4.pt * met.pt * (ak.ones_like(met.pt) - np.cos(candidatelep_p4.delta_phi(met)))\n",
    ")\n",
    "\n",
    "# JETS\n",
    "goodjets = events.Jet[\n",
    "    (events.Jet.pt > 30)\n",
    "    & (abs(events.Jet.eta) < 2.5)\n",
    "    & events.Jet.isTight\n",
    "    & (events.Jet.puId > 0)\n",
    "]\n",
    "ht = ak.sum(goodjets.pt, axis=1)\n",
    "\n",
    "# FATJETS\n",
    "fatjets = events.FatJet\n",
    "\n",
    "good_fatjets = (\n",
    "    (fatjets.pt > 200)\n",
    "    & (abs(fatjets.eta) < 2.5)\n",
    "    & fatjets.isTight\n",
    ")\n",
    "n_fatjets = ak.sum(good_fatjets, axis=1)\n",
    "\n",
    "good_fatjets = fatjets[good_fatjets]\n",
    "good_fatjets = good_fatjets[ak.argsort(good_fatjets.pt, ascending=False)]\n",
    "\n",
    "# for lep channel: first clean jets and leptons by removing overlap, then pick candidate_fj closest to the lepton\n",
    "lep_in_fj_overlap_bool = good_fatjets.delta_r(candidatelep_p4) > 0.1\n",
    "good_fatjets = good_fatjets[lep_in_fj_overlap_bool]\n",
    "fj_idx_lep = ak.argmin(good_fatjets.delta_r(candidatelep_p4), axis=1, keepdims=True)\n",
    "candidatefj = ak.firsts(good_fatjets[fj_idx_lep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [None, None, None, ... None, None, None] type='98400 * ?float32[parameter...'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidatefj.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build PFcands and SVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import modules to build the tagger inputs\n",
    "import sys\n",
    "sys.path.append(\"../boostedhiggs/\")\n",
    "from get_tagger_inputs import get_pfcands_features, get_svs_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_resources_path = \"../boostedhiggs/tagger_resources/\"\n",
    "\n",
    "# use this model\n",
    "model_name = \"particlenet_hww_inclv2_pre2\"\n",
    "\n",
    "# the different models we can use\n",
    "pversion, out_name = {\n",
    "    \"05_10_ak8_ttbarwjets\": [\"PN_UCSD\", \"softmax__0\"],\n",
    "    \"particlenet_hww_inclv2_pre2\": [\"ParticleNet\", \"output__0\"],\n",
    "    \"particlenet_hww_inclv2_pre2_noreg\": [\"PN_v2_noreg\", \"softmax__0\"],\n",
    "    \"ak8_MD_vminclv2ParT_manual_fixwrap\": [\"ParT_noreg\", \"softmax\"],\n",
    "    \"ak8_MD_vminclv2ParT_manual_fixwrap_all_nodes\": [\"ParT\", \"softmax\"],\n",
    "}[model_name]   \n",
    "\n",
    "with open(f\"{tagger_resources_path}/triton_config_{model_name}.json\") as f:\n",
    "    triton_config = json.load(f)\n",
    "\n",
    "with open(f\"{tagger_resources_path}/{triton_config['model_name']}.json\") as f:\n",
    "    tagger_vars = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get pfcands\n",
    "fatjet_label = \"FatJet\"\n",
    "pfcands_label = \"FatJetPFCands\"\n",
    "    \n",
    "pfcands_features = get_pfcands_features(tagger_vars, events, fj_idx_lep, fatjet_label, pfcands_label)\n",
    "\n",
    "# get svs\n",
    "svs_label = \"FatJetSVs\"\n",
    "    \n",
    "svs_features = get_svs_features(tagger_vars, events, fj_idx_lep, fatjet_label, svs_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is our input to the tagger\n",
    "feature_dict = {\n",
    "    **pfcands_features,\n",
    "    **svs_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/lgray/hgg-coffea/blob/triton-bdts/src/hgg_coffea/tools/chained_quantile.py\n",
    "class wrapped_triton:\n",
    "    def __init__(self, model_url: str, batch_size: int, out_name: str = \"softmax__0\") -> None:\n",
    "        fullprotocol, location = model_url.split(\"://\")\n",
    "        _, protocol = fullprotocol.split(\"+\")\n",
    "        address, model, version = location.split(\"/\")\n",
    "\n",
    "        self._protocol = protocol\n",
    "        self._address = address\n",
    "        self._model = model\n",
    "        self._version = version\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._out_name = out_name\n",
    "\n",
    "    def __call__(self, input_dict: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        if self._protocol == \"grpc\":\n",
    "            client = triton_grpc.InferenceServerClient(url=self._address, verbose=False)\n",
    "            triton_protocol = triton_grpc\n",
    "        elif self._protocol == \"http\":\n",
    "            client = triton_http.InferenceServerClient(\n",
    "                url=self._address,\n",
    "                verbose=False,\n",
    "                concurrency=12,\n",
    "            )\n",
    "            triton_protocol = triton_http\n",
    "        else:\n",
    "            raise ValueError(f\"{self._protocol} does not encode a valid protocol (grpc or http)\")\n",
    "\n",
    "        # manually split into batches for gpu inference\n",
    "        input_size = input_dict[list(input_dict.keys())[0]].shape[0]\n",
    "        # print(f\"size of input (number of events) = {input_size}\")\n",
    "\n",
    "        outs = [\n",
    "            self._do_inference(\n",
    "                {key: input_dict[key][batch : batch + self._batch_size] for key in input_dict},\n",
    "                triton_protocol,\n",
    "                client,\n",
    "            )\n",
    "            for batch in tqdm(range(0, input_dict[list(input_dict.keys())[0]].shape[0], self._batch_size))\n",
    "        ]\n",
    "\n",
    "        return np.concatenate(outs) if input_size > 0 else outs\n",
    "\n",
    "    def _do_inference(self, input_dict: Dict[str, np.ndarray], triton_protocol, client) -> np.ndarray:\n",
    "        # Infer\n",
    "        inputs = []\n",
    "\n",
    "        for key in input_dict:\n",
    "            input = triton_protocol.InferInput(key, input_dict[key].shape, \"FP32\")\n",
    "            input.set_data_from_numpy(input_dict[key])\n",
    "            inputs.append(input)\n",
    "\n",
    "        output = triton_protocol.InferRequestedOutput(self._out_name)\n",
    "\n",
    "        request = client.infer(\n",
    "            self._model,\n",
    "            model_version=self._version,\n",
    "            inputs=inputs,\n",
    "            outputs=[output],\n",
    "        )\n",
    "\n",
    "        return request.as_numpy(self._out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triton_model = wrapped_triton(triton_config[\"model_url\"], triton_config[\"batch_size\"], out_name=out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_inputs = []\n",
    "\n",
    "for input_name in tagger_vars[\"input_names\"]:\n",
    "    for key in tagger_vars[input_name][\"var_names\"]:\n",
    "        np.expand_dims(feature_dict[key], 1)\n",
    "\n",
    "if out_name == \"softmax\":\n",
    "    tagger_inputs = {\n",
    "        f\"{input_name}\": np.concatenate(\n",
    "            [np.expand_dims(feature_dict[key], 1) for key in tagger_vars[input_name][\"var_names\"]],\n",
    "            axis=1,\n",
    "        )\n",
    "        for i, input_name in enumerate(tagger_vars[\"input_names\"])\n",
    "    }\n",
    "else:\n",
    "    tagger_inputs = {\n",
    "        f\"{input_name}__{i}\": np.concatenate(\n",
    "            [np.expand_dims(feature_dict[key], 1) for key in tagger_vars[input_name][\"var_names\"]],\n",
    "            axis=1,\n",
    "        )\n",
    "        for i, input_name in enumerate(tagger_vars[\"input_names\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as triton_http\n",
    "from tqdm import tqdm\n",
    "\n",
    "# run inference for one fat jet\n",
    "tagger_outputs = triton_model(tagger_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "coffea-env",
   "language": "python",
   "name": "coffea-env"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
