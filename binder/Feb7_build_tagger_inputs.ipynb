{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5246768",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pickle as pkl\n",
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import uproot\n",
    "from coffea import nanoevents, processor\n",
    "from coffea.nanoevents import BaseSchema, NanoAODSchema, NanoEventsFactory\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "import pickle as pkl\n",
    "import shutil\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "from typing import List, Optional\n",
    "\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from coffea import processor\n",
    "from coffea.analysis_tools import PackedSelection, Weights\n",
    "from coffea.nanoevents.methods import candidate, vector\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", message=\"Found duplicate branch \")\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "np.seterr(invalid=\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e75dfa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hww1.root  hww2.root\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../rootfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f07f0fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of events per file is 98400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for GenFatJetCands_jetIdx => GenJetAK8\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for GenFatJetCands_pFCandsIdx => GenCands\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for GenFatJetSVs_jetIdx => GenJetAK8\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for GenFatJetSVs_sVIdx => SV\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for GenJetCands_jetIdx => GenJet\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for GenJetCands_pFCandsIdx => GenCands\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for GenJetSVs_jetIdx => GenJet\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for GenJetSVs_sVIdx => SV\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for JetPFCands_jetIdx => Jet\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for JetPFCands_pFCandsIdx => PFCands\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for JetSVs_jetIdx => Jet\n",
      "  RuntimeWarning,\n",
      "/uscms/home/fmokhtar/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/coffea/nanoevents/schemas/nanoaod.py:195: RuntimeWarning: Missing cross-reference index for JetSVs_sVIdx => SV\n",
      "  RuntimeWarning,\n"
     ]
    }
   ],
   "source": [
    "### schema\n",
    "nanoevents.PFNanoAODSchema.mixins[\"PFCands\"] = \"PFCand\"\n",
    "nanoevents.PFNanoAODSchema.mixins[\"SV\"] = \"PFCand\"\n",
    "\n",
    "# load a root file into coffea-friendly NanoAOD structure\n",
    "import uproot\n",
    "f = uproot.open(f\"../rootfiles/hww1.root\")\n",
    "num = f['Events'].num_entries   ### checks number of events per file \n",
    "print(f'number of events per file is {num}')\n",
    "\n",
    "events = nanoevents.NanoEventsFactory.from_root(f, \"Events\", schemaclass=nanoevents.PFNanoAODSchema).events()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d65cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_p4(cand):\n",
    "    return ak.zip(\n",
    "        {\n",
    "            \"pt\": cand.pt,\n",
    "            \"eta\": cand.eta,\n",
    "            \"phi\": cand.phi,\n",
    "            \"mass\": cand.mass,\n",
    "            \"charge\": cand.charge,\n",
    "        },\n",
    "        with_name=\"PtEtaPhiMCandidate\",\n",
    "        behavior=candidate.behavior,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7948cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "### make selections\n",
    "nevents = len(events)\n",
    "\n",
    "# define muon objects\n",
    "loose_muons = (\n",
    "    (((events.Muon.pt > 30) & (events.Muon.pfRelIso04_all < 0.25)) |\n",
    "     (events.Muon.pt > 55))\n",
    "    & (np.abs(events.Muon.eta) < 2.4)\n",
    "    & (events.Muon.looseId)\n",
    ")\n",
    "n_loose_muons = ak.sum(loose_muons, axis=1)\n",
    "\n",
    "good_muons = (\n",
    "    (events.Muon.pt > 28)\n",
    "    & (np.abs(events.Muon.eta) < 2.4)\n",
    "    & (np.abs(events.Muon.dz) < 0.1)\n",
    "    & (np.abs(events.Muon.dxy) < 0.05)\n",
    "    & (events.Muon.sip3d <= 4.0)\n",
    "    & events.Muon.mediumId\n",
    ")\n",
    "n_good_muons = ak.sum(good_muons, axis=1)\n",
    "\n",
    "# define electron objects\n",
    "loose_electrons = (\n",
    "    (((events.Electron.pt > 38) & (events.Electron.pfRelIso03_all < 0.25)) |\n",
    "     (events.Electron.pt > 120))\n",
    "    & ((np.abs(events.Electron.eta) < 1.44) | (np.abs(events.Electron.eta) > 1.57))\n",
    "    & (events.Electron.cutBased >= events.Electron.LOOSE)\n",
    ")\n",
    "n_loose_electrons = ak.sum(loose_electrons, axis=1)\n",
    "\n",
    "good_electrons = (\n",
    "    (events.Electron.pt > 38)\n",
    "    & ((np.abs(events.Electron.eta) < 1.44) | (np.abs(events.Electron.eta) > 1.57))\n",
    "    & (np.abs(events.Electron.dz) < 0.1)\n",
    "    & (np.abs(events.Electron.dxy) < 0.05)\n",
    "    & (events.Electron.sip3d <= 4.0)\n",
    "    & (events.Electron.mvaFall17V2noIso_WP90)\n",
    ")\n",
    "n_good_electrons = ak.sum(good_electrons, axis=1)\n",
    "\n",
    "# leading lepton\n",
    "goodleptons = ak.concatenate([events.Muon[good_muons], events.Electron[good_electrons]], axis=1)\n",
    "goodleptons = goodleptons[ak.argsort(goodleptons.pt, ascending=False)]\n",
    "candidatelep = ak.firsts(goodleptons)\n",
    "\n",
    "# candidate leptons\n",
    "candidatelep_p4 = build_p4(candidatelep)\n",
    "\n",
    "# MET\n",
    "met = events.MET\n",
    "mt_lep_met = np.sqrt(\n",
    "    2. * candidatelep_p4.pt * met.pt * (ak.ones_like(met.pt) - np.cos(candidatelep_p4.delta_phi(met)))\n",
    ")\n",
    "\n",
    "# JETS\n",
    "goodjets = events.Jet[\n",
    "    (events.Jet.pt > 30)\n",
    "    & (abs(events.Jet.eta) < 2.5)\n",
    "    & events.Jet.isTight\n",
    "    & (events.Jet.puId > 0)\n",
    "]\n",
    "ht = ak.sum(goodjets.pt, axis=1)\n",
    "\n",
    "# FATJETS\n",
    "fatjets = events.FatJet\n",
    "\n",
    "good_fatjets = (\n",
    "    (fatjets.pt > 200)\n",
    "    & (abs(fatjets.eta) < 2.5)\n",
    "    & fatjets.isTight\n",
    ")\n",
    "n_fatjets = ak.sum(good_fatjets, axis=1)\n",
    "\n",
    "good_fatjets = fatjets[good_fatjets]\n",
    "good_fatjets = good_fatjets[ak.argsort(good_fatjets.pt, ascending=False)]\n",
    "\n",
    "# for lep channel: first clean jets and leptons by removing overlap, then pick candidate_fj closest to the lepton\n",
    "lep_in_fj_overlap_bool = good_fatjets.delta_r(candidatelep_p4) > 0.1\n",
    "good_fatjets = good_fatjets[lep_in_fj_overlap_bool]\n",
    "fj_idx_lep = ak.argmin(good_fatjets.delta_r(candidatelep_p4), axis=1, keepdims=True)\n",
    "candidatefj = ak.firsts(good_fatjets[fj_idx_lep])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5100ae3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Array [None, None, None, ... None, None, None] type='98400 * ?float32[parameter...'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "candidatefj.pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f031986d",
   "metadata": {},
   "source": [
    "## Build PFcands and SVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "776ca93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_resources_path = \"../boostedhiggs/tagger_resources/\"\n",
    "model_name = \"particlenet_hww_inclv2_pre2_noreg\"\n",
    "\n",
    "with open(f\"{tagger_resources_path}/triton_config_{model_name}.json\") as f:\n",
    "    triton_config = json.load(f)\n",
    "\n",
    "with open(f\"{tagger_resources_path}/{triton_config['model_name']}.json\") as f:\n",
    "    tagger_vars = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "330a56e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from coffea.nanoevents.methods.base import NanoEventsArray\n",
    "import numpy.ma as ma\n",
    "\n",
    "def get_pfcands_features(\n",
    "    tagger_vars: dict,\n",
    "    preselected_events: NanoEventsArray,\n",
    "    fj_idx_lep,\n",
    "    fatjet_label: str = \"FatJetAK15\",\n",
    "    pfcands_label: str = \"FatJetPFCands\",\n",
    "    normalize: bool = True,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts the pf_candidate features specified in the ``tagger_vars`` dict from the\n",
    "    ``preselected_events`` and returns them as a dict of numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "    jet = ak.firsts(preselected_events[fatjet_label][fj_idx_lep])\n",
    "\n",
    "    msk = preselected_events[pfcands_label].jetIdx == ak.firsts(fj_idx_lep)\n",
    "    jet_ak_pfcands = preselected_events[pfcands_label][msk]\n",
    "    jet_pfcands = preselected_events.PFCands[jet_ak_pfcands.pFCandsIdx]\n",
    "\n",
    "    # sort them by pt\n",
    "    pfcand_sort = ak.argsort(jet_pfcands.pt, ascending=False)\n",
    "    jet_pfcands = jet_pfcands[pfcand_sort]\n",
    "\n",
    "    # negative eta jets have -1 sign, positive eta jets have +1\n",
    "    eta_sign = ak.ones_like(jet_pfcands.eta)\n",
    "    eta_sign = eta_sign * (ak.values_astype(jet.eta > 0, int) * 2 - 1)\n",
    "    feature_dict[\"pfcand_etarel\"] = eta_sign * (jet_pfcands.eta - jet.eta)\n",
    "    feature_dict[\"pfcand_phirel\"] = jet.delta_phi(jet_pfcands)\n",
    "    feature_dict[\"pfcand_abseta\"] = np.abs(jet_pfcands.eta)\n",
    "\n",
    "    feature_dict[\"pfcand_pt_log_nopuppi\"] = np.log(jet_pfcands.pt)\n",
    "    feature_dict[\"pfcand_e_log_nopuppi\"] = np.log(jet_pfcands.energy)\n",
    "\n",
    "    pdgIds = jet_pfcands.pdgId\n",
    "    feature_dict[\"pfcand_isEl\"] = np.abs(pdgIds) == 11\n",
    "    feature_dict[\"pfcand_isMu\"] = np.abs(pdgIds) == 13\n",
    "    feature_dict[\"pfcand_isChargedHad\"] = np.abs(pdgIds) == 211\n",
    "    feature_dict[\"pfcand_isGamma\"] = np.abs(pdgIds) == 22\n",
    "    feature_dict[\"pfcand_isNeutralHad\"] = np.abs(pdgIds) == 130\n",
    "\n",
    "    feature_dict[\"pfcand_charge\"] = jet_pfcands.charge\n",
    "    feature_dict[\"pfcand_VTX_ass\"] = jet_pfcands.pvAssocQuality\n",
    "    feature_dict[\"pfcand_lostInnerHits\"] = jet_pfcands.lostInnerHits\n",
    "    feature_dict[\"pfcand_quality\"] = jet_pfcands.trkQuality\n",
    "\n",
    "    feature_dict[\"pfcand_normchi2\"] = np.floor(jet_pfcands.trkChi2)\n",
    "\n",
    "    if \"Cdz\" in jet_ak_pfcands.fields:\n",
    "        feature_dict[\"pfcand_dz\"] = jet_ak_pfcands[\"Cdz\"][pfcand_sort]\n",
    "        feature_dict[\"pfcand_dxy\"] = jet_ak_pfcands[\"Cdxy\"][pfcand_sort]\n",
    "        feature_dict[\"pfcand_dzsig\"] = jet_ak_pfcands[\"Cdzsig\"][pfcand_sort]\n",
    "        feature_dict[\"pfcand_dxysig\"] = jet_ak_pfcands[\"Cdxysig\"][pfcand_sort]\n",
    "    else:\n",
    "        # this is for old PFNano (<= v2.3)\n",
    "        feature_dict[\"pfcand_dz\"] = jet_pfcands.dz\n",
    "        feature_dict[\"pfcand_dxy\"] = jet_pfcands.d0\n",
    "        feature_dict[\"pfcand_dzsig\"] = jet_pfcands.dz / jet_pfcands.dzErr\n",
    "        feature_dict[\"pfcand_dxysig\"] = jet_pfcands.d0 / jet_pfcands.d0Err\n",
    "\n",
    "    feature_dict[\"pfcand_px\"] = jet_pfcands.px\n",
    "    feature_dict[\"pfcand_py\"] = jet_pfcands.py\n",
    "    feature_dict[\"pfcand_pz\"] = jet_pfcands.pz\n",
    "#     feature_dict[\"pfcand_energy\"] = jet_pfcands.E\n",
    "    feature_dict[\"pfcand_energy\"] = jet_pfcands.energy\n",
    "\n",
    "    # btag vars\n",
    "    for var in tagger_vars[\"pf_features\"][\"var_names\"]:\n",
    "        if \"btag\" in var:\n",
    "            feature_dict[var] = jet_ak_pfcands[var[len(\"pfcand_\") :]][pfcand_sort]\n",
    "\n",
    "    # pfcand mask\n",
    "    feature_dict[\"pfcand_mask\"] = (\n",
    "        ~(\n",
    "            ma.masked_invalid(\n",
    "                ak.pad_none(\n",
    "                    feature_dict[\"pfcand_abseta\"],\n",
    "                    tagger_vars[\"pf_features\"][\"var_length\"],\n",
    "                    axis=1,\n",
    "                    clip=True,\n",
    "                ).to_numpy()\n",
    "            ).mask\n",
    "        )\n",
    "    ).astype(np.float32)\n",
    "\n",
    "    # if no padding is needed, mask will = 1.0\n",
    "    if isinstance(feature_dict[\"pfcand_mask\"], np.float32):\n",
    "        feature_dict[\"pfcand_mask\"] = np.ones(\n",
    "            (\n",
    "                len(feature_dict[\"pfcand_abseta\"]),\n",
    "                tagger_vars[\"pf_features\"][\"var_length\"],\n",
    "            )\n",
    "        ).astype(np.float32)\n",
    "\n",
    "    repl_values_dict = {\n",
    "        \"pfcand_normchi2\": [-1, 999],\n",
    "        \"pfcand_dz\": [-1, 0],\n",
    "        \"pfcand_dzsig\": [1, 0],\n",
    "        \"pfcand_dxy\": [-1, 0],\n",
    "        \"pfcand_dxysig\": [1, 0],\n",
    "    }\n",
    "\n",
    "    # convert to numpy arrays and normalize features\n",
    "    if \"pf_vectors\" in tagger_vars.keys():\n",
    "        variables = set(tagger_vars[\"pf_features\"][\"var_names\"] + tagger_vars[\"pf_vectors\"][\"var_names\"])\n",
    "    else:\n",
    "        variables = tagger_vars[\"pf_features\"][\"var_names\"]\n",
    "\n",
    "    for var in variables:\n",
    "        a = (\n",
    "            ak.pad_none(\n",
    "                feature_dict[var],\n",
    "                tagger_vars[\"pf_features\"][\"var_length\"],\n",
    "                axis=1,\n",
    "                clip=True,\n",
    "            )\n",
    "            .to_numpy()\n",
    "            .filled(fill_value=0)\n",
    "        ).astype(np.float32)\n",
    "        a = np.nan_to_num(a)\n",
    "\n",
    "        # replace values to match PKU's\n",
    "        if var in repl_values_dict:\n",
    "            vals = repl_values_dict[var]\n",
    "            a[a == vals[0]] = vals[1]\n",
    "\n",
    "        if normalize:\n",
    "            if var in tagger_vars[\"pf_features\"][\"var_names\"]:\n",
    "                info = tagger_vars[\"pf_features\"][\"var_infos\"][var]\n",
    "            else:\n",
    "                info = tagger_vars[\"pf_vectors\"][\"var_infos\"][var]\n",
    "\n",
    "            a = (a - info[\"median\"]) * info[\"norm_factor\"]\n",
    "            a = np.clip(a, info.get(\"lower_bound\", -5), info.get(\"upper_bound\", 5))\n",
    "\n",
    "        feature_dict[var] = a\n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f31c1d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fatjet_label = \"FatJet\"\n",
    "pfcands_label = \"FatJetPFCands\"\n",
    "    \n",
    "pfcands_features = get_pfcands_features(tagger_vars, events, fj_idx_lep, fatjet_label, pfcands_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "97b9f004",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svs_features(\n",
    "    tagger_vars: dict,\n",
    "    preselected_events: NanoEventsArray,\n",
    "    fj_idx_lep,\n",
    "    fatjet_label: str = \"FatJetAK15\",\n",
    "    svs_label: str = \"JetSVsAK15\",\n",
    "    normalize: bool = True,\n",
    ") -> Dict[str, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Extracts the sv features specified in the ``tagger_vars`` dict from the\n",
    "    ``preselected_events`` and returns them as a dict of numpy arrays\n",
    "    \"\"\"\n",
    "\n",
    "    feature_dict = {}\n",
    "\n",
    "    jet = ak.firsts(preselected_events[fatjet_label][fj_idx_lep])\n",
    "    msk = preselected_events[svs_label].jetIdx == ak.firsts(fj_idx_lep)\n",
    "    jet_svs = preselected_events.SV[preselected_events[svs_label].sVIdx[(preselected_events[svs_label].sVIdx != -1) * (msk)]]\n",
    "\n",
    "    # sort by dxy significance\n",
    "    jet_svs = jet_svs[ak.argsort(jet_svs.dxySig, ascending=False)]\n",
    "\n",
    "    # negative eta jets have -1 sign, positive eta jets have +1\n",
    "    eta_sign = ak.values_astype(jet_svs.eta > 0, int) * 2 - 1\n",
    "    feature_dict[\"sv_etarel\"] = eta_sign * (jet_svs.eta - jet.eta)\n",
    "    feature_dict[\"sv_phirel\"] = jet_svs.delta_phi(jet)\n",
    "    feature_dict[\"sv_abseta\"] = np.abs(jet_svs.eta)\n",
    "    feature_dict[\"sv_mass\"] = jet_svs.mass\n",
    "    feature_dict[\"sv_pt_log\"] = np.log(jet_svs.pt)\n",
    "\n",
    "    feature_dict[\"sv_ntracks\"] = jet_svs.ntracks\n",
    "    feature_dict[\"sv_normchi2\"] = jet_svs.chi2\n",
    "    feature_dict[\"sv_dxy\"] = jet_svs.dxy\n",
    "    feature_dict[\"sv_dxysig\"] = jet_svs.dxySig\n",
    "    feature_dict[\"sv_d3d\"] = jet_svs.dlen\n",
    "    feature_dict[\"sv_d3dsig\"] = jet_svs.dlenSig\n",
    "    svpAngle = jet_svs.pAngle\n",
    "    feature_dict[\"sv_costhetasvpv\"] = -np.cos(svpAngle)\n",
    "\n",
    "    feature_dict[\"sv_px\"] = jet_svs.px\n",
    "    feature_dict[\"sv_py\"] = jet_svs.py\n",
    "    feature_dict[\"sv_pz\"] = jet_svs.pz\n",
    "    feature_dict[\"sv_energy\"] = jet_svs.energy\n",
    "\n",
    "    feature_dict[\"sv_mask\"] = (\n",
    "        ~(\n",
    "            ma.masked_invalid(\n",
    "                ak.pad_none(\n",
    "                    feature_dict[\"sv_etarel\"],\n",
    "                    tagger_vars[\"sv_features\"][\"var_length\"],\n",
    "                    axis=1,\n",
    "                    clip=True,\n",
    "                ).to_numpy()\n",
    "            ).mask\n",
    "        )\n",
    "    ).astype(np.float32)\n",
    "    if isinstance(feature_dict[\"sv_mask\"], np.float32):\n",
    "        feature_dict[\"sv_mask\"] = np.ones((len(feature_dict[\"sv_abseta\"]), tagger_vars[\"sv_features\"][\"var_length\"])).astype(\n",
    "            np.float32\n",
    "        )\n",
    "\n",
    "    # convert to numpy arrays and normalize features\n",
    "    if \"sv_vectors\" in tagger_vars.keys():\n",
    "        variables = set(tagger_vars[\"sv_features\"][\"var_names\"] + tagger_vars[\"sv_vectors\"][\"var_names\"])\n",
    "    else:\n",
    "        variables = tagger_vars[\"sv_features\"][\"var_names\"]\n",
    "\n",
    "    for var in variables:\n",
    "        a = (\n",
    "            ak.pad_none(\n",
    "                feature_dict[var],\n",
    "                tagger_vars[\"sv_features\"][\"var_length\"],\n",
    "                axis=1,\n",
    "                clip=True,\n",
    "            )\n",
    "            .to_numpy()\n",
    "            .filled(fill_value=0)\n",
    "        ).astype(np.float32)\n",
    "        a = np.nan_to_num(a)\n",
    "\n",
    "        if normalize:\n",
    "            if var in tagger_vars[\"sv_features\"][\"var_names\"]:\n",
    "                info = tagger_vars[\"sv_features\"][\"var_infos\"][var]\n",
    "            else:\n",
    "                info = tagger_vars[\"sv_vectors\"][\"var_infos\"][var]\n",
    "            a = (a - info[\"median\"]) * info[\"norm_factor\"]\n",
    "            a = np.clip(a, info.get(\"lower_bound\", -5), info.get(\"upper_bound\", 5))\n",
    "\n",
    "        feature_dict[var] = a\n",
    "\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf1b6e33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fatjet_label = \"FatJet\"\n",
    "svs_label = \"FatJetSVs\"\n",
    "    \n",
    "svs_features = get_svs_features(tagger_vars, events, fj_idx_lep, fatjet_label, svs_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8d3c5d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = {\n",
    "    **pfcands_features,\n",
    "    **svs_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c656e",
   "metadata": {},
   "source": [
    "## Triton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "52ae0762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adapted from https://github.com/lgray/hgg-coffea/blob/triton-bdts/src/hgg_coffea/tools/chained_quantile.py\n",
    "class wrapped_triton:\n",
    "    def __init__(self, model_url: str, batch_size: int, out_name: str = \"softmax__0\") -> None:\n",
    "        fullprotocol, location = model_url.split(\"://\")\n",
    "        _, protocol = fullprotocol.split(\"+\")\n",
    "        address, model, version = location.split(\"/\")\n",
    "\n",
    "        self._protocol = protocol\n",
    "        self._address = address\n",
    "        self._model = model\n",
    "        self._version = version\n",
    "\n",
    "        self._batch_size = batch_size\n",
    "        self._out_name = out_name\n",
    "\n",
    "    def __call__(self, input_dict: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "        if self._protocol == \"grpc\":\n",
    "            client = triton_grpc.InferenceServerClient(url=self._address, verbose=False)\n",
    "            triton_protocol = triton_grpc\n",
    "        elif self._protocol == \"http\":\n",
    "            client = triton_http.InferenceServerClient(\n",
    "                url=self._address,\n",
    "                verbose=False,\n",
    "                concurrency=12,\n",
    "            )\n",
    "            triton_protocol = triton_http\n",
    "        else:\n",
    "            raise ValueError(f\"{self._protocol} does not encode a valid protocol (grpc or http)\")\n",
    "\n",
    "        # manually split into batches for gpu inference\n",
    "        input_size = input_dict[list(input_dict.keys())[0]].shape[0]\n",
    "        # print(f\"size of input (number of events) = {input_size}\")\n",
    "\n",
    "        outs = [\n",
    "            self._do_inference(\n",
    "                {key: input_dict[key][batch : batch + self._batch_size] for key in input_dict},\n",
    "                triton_protocol,\n",
    "                client,\n",
    "            )\n",
    "            for batch in tqdm(range(0, input_dict[list(input_dict.keys())[0]].shape[0], self._batch_size))\n",
    "        ]\n",
    "\n",
    "        return np.concatenate(outs) if input_size > 0 else outs\n",
    "\n",
    "    def _do_inference(self, input_dict: Dict[str, np.ndarray], triton_protocol, client) -> np.ndarray:\n",
    "        # Infer\n",
    "        inputs = []\n",
    "\n",
    "        for key in input_dict:\n",
    "            input = triton_protocol.InferInput(key, input_dict[key].shape, \"FP32\")\n",
    "            input.set_data_from_numpy(input_dict[key])\n",
    "            inputs.append(input)\n",
    "\n",
    "        output = triton_protocol.InferRequestedOutput(self._out_name)\n",
    "\n",
    "        request = client.infer(\n",
    "            self._model,\n",
    "            model_version=self._version,\n",
    "            inputs=inputs,\n",
    "            outputs=[output],\n",
    "        )\n",
    "\n",
    "        return request.as_numpy(self._out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b48d0205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "bae4a60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pversion, out_name = {\n",
    "    \"05_10_ak8_ttbarwjets\": [\"PN_UCSD\", \"softmax__0\"],\n",
    "    \"particlenet_hww_inclv2_pre2\": [\"PN_v2\", \"output__0\"],\n",
    "    \"particlenet_hww_inclv2_pre2_noreg\": [\"PN_v2_noreg\", \"softmax__0\"],\n",
    "    \"ak8_MD_vminclv2ParT_manual_fixwrap\": [\"ParT\", \"softmax\"],\n",
    "}[model_name]\n",
    "\n",
    "triton_model = wrapped_triton(triton_config[\"model_url\"], triton_config[\"batch_size\"], out_name=out_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ca6efc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tagger_inputs = []\n",
    "\n",
    "for input_name in tagger_vars[\"input_names\"]:\n",
    "    for key in tagger_vars[input_name][\"var_names\"]:\n",
    "        np.expand_dims(feature_dict[key], 1)\n",
    "\n",
    "if out_name == \"softmax\":\n",
    "    tagger_inputs = {\n",
    "        f\"{input_name}\": np.concatenate(\n",
    "            [np.expand_dims(feature_dict[key], 1) for key in tagger_vars[input_name][\"var_names\"]],\n",
    "            axis=1,\n",
    "        )\n",
    "        for i, input_name in enumerate(tagger_vars[\"input_names\"])\n",
    "    }\n",
    "else:\n",
    "    tagger_inputs = {\n",
    "        f\"{input_name}__{i}\": np.concatenate(\n",
    "            [np.expand_dims(feature_dict[key], 1) for key in tagger_vars[input_name][\"var_names\"]],\n",
    "            axis=1,\n",
    "        )\n",
    "        for i, input_name in enumerate(tagger_vars[\"input_names\"])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a6ac53de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|                                                                                              | 0/385 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "ename": "InferenceServerException",
     "evalue": "[StatusCode.UNAVAILABLE] failed to connect to all addresses",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInferenceServerException\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21594/14426266.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtagger_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtagger_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtriton_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtagger_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_21594/3520718044.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, input_dict)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             )\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         ]\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21594/3520718044.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             )\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         ]\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_21594/3520718044.py\u001b[0m in \u001b[0;36m_do_inference\u001b[0;34m(self, input_dict, triton_protocol, client)\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0mmodel_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         )\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/tritonclient/grpc/__init__.py\u001b[0m in \u001b[0;36minfer\u001b[0;34m(self, model_name, inputs, model_version, outputs, request_id, sequence_id, sequence_start, sequence_end, priority, timeout, client_timeout, headers, compression_algorithm)\u001b[0m\n\u001b[1;32m   1320\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mgrpc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRpcError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrpc_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m             \u001b[0mraise_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m     def async_infer(self,\n",
      "\u001b[0;32m~/nobackup/miniconda3/envs/coffea-env/lib/python3.7/site-packages/tritonclient/grpc/__init__.py\u001b[0m in \u001b[0;36mraise_error_grpc\u001b[0;34m(rpc_error)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mraise_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mget_error_grpc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrpc_error\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mInferenceServerException\u001b[0m: [StatusCode.UNAVAILABLE] failed to connect to all addresses"
     ]
    }
   ],
   "source": [
    "import tritonclient.grpc as triton_grpc\n",
    "import tritonclient.http as triton_http\n",
    "from tqdm import tqdm\n",
    "\n",
    "# run inference for one fat jet\n",
    "tagger_outputs = triton_model(tagger_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e3cb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
